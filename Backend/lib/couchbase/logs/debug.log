[ns_server:info,2023-08-29T18:14:48.975Z,nonode@nohost:<0.146.0>:ns_server:init_logging:120]Started & configured logging
[ns_server:info,2023-08-29T18:14:48.987Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]Static config terms:
[{error_logger_mf_dir,"/opt/couchbase/var/lib/couchbase/logs"},
 {path_config_bindir,"/opt/couchbase/bin"},
 {path_config_etcdir,"/opt/couchbase/etc/couchbase"},
 {path_config_libdir,"/opt/couchbase/lib"},
 {path_config_datadir,"/opt/couchbase/var/lib/couchbase"},
 {path_config_tmpdir,"/opt/couchbase/var/lib/couchbase/tmp"},
 {path_config_secdir,"/opt/couchbase/etc/security"},
 {nodefile,"/opt/couchbase/var/lib/couchbase/couchbase-server.node"},
 {loglevel_default,debug},
 {loglevel_couchdb,info},
 {loglevel_ns_server,debug},
 {loglevel_error_logger,debug},
 {loglevel_user,debug},
 {loglevel_menelaus,debug},
 {loglevel_ns_doctor,debug},
 {loglevel_stats,debug},
 {loglevel_rebalance,debug},
 {loglevel_cluster,debug},
 {loglevel_views,debug},
 {loglevel_mapreduce_errors,debug},
 {loglevel_xdcr,debug},
 {loglevel_access,info},
 {loglevel_cbas,debug},
 {disk_sink_opts,[{rotation,[{compress,true},
                             {size,41943040},
                             {num_files,10},
                             {buffer_size_max,52428800}]}]},
 {disk_sink_opts_json_rpc,[{rotation,[{compress,true},
                                      {size,41943040},
                                      {num_files,2},
                                      {buffer_size_max,52428800}]}]},
 {net_kernel_verbosity,10}]
[ns_server:warn,2023-08-29T18:14:48.987Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter error_logger_mf_dir, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter path_config_bindir, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter path_config_etcdir, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter path_config_libdir, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter path_config_datadir, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter path_config_tmpdir, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter path_config_secdir, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter nodefile, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_default, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.988Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_couchdb, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_ns_server, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_error_logger, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_user, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_menelaus, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_ns_doctor, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_stats, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_rebalance, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_cluster, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_views, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_mapreduce_errors, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.989Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_xdcr, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.990Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_access, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.990Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter loglevel_cbas, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.990Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter disk_sink_opts, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.990Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter disk_sink_opts_json_rpc, which is given from command line
[ns_server:warn,2023-08-29T18:14:48.990Z,nonode@nohost:<0.146.0>:ns_server:log_pending:29]not overriding parameter net_kernel_verbosity, which is given from command line
[ns_server:info,2023-08-29T18:14:49.012Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip_start"
[ns_server:info,2023-08-29T18:14:49.013Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:read_address_config_from_path:83]Reading ip config from "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-08-29T18:14:49.014Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:init:179]ip config not found. Looks like we're brand new node
[ns_server:info,2023-08-29T18:14:49.023Z,nonode@nohost:dist_manager<0.197.0>:dist_manager:bringup:245]Attempting to bring up net_kernel with name 'ns_1@cb.local'
[error_logger:info,2023-08-29T18:14:49.057Z,nonode@nohost:ssl_dist_admin_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.201.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.058Z,nonode@nohost:ssl_dist_admin_sup<0.200.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.202.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.058Z,nonode@nohost:ssl_dist_sup<0.199.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.200.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.062Z,nonode@nohost:tls_dist_sup<0.203.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.204.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.067Z,nonode@nohost:tls_dist_server_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.206.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.067Z,nonode@nohost:tls_dist_server_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.207.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.067Z,nonode@nohost:tls_dist_server_sup<0.205.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.208.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.068Z,nonode@nohost:tls_dist_sup<0.203.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.205.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.068Z,nonode@nohost:ssl_dist_sup<0.199.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.203.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.068Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.199.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:49.071Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Starting cb_dist with config []
[error_logger:info,2023-08-29T18:14:49.074Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.209.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.077Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.210.0>},
              {id,cb_epmd},
              {mfargs,{cb_epmd,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.081Z,nonode@nohost:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.211.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:49.086Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2023-08-29T18:14:49.087Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2023-08-29T18:14:49.087Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Started listener: inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:49.147Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Started acceptor inet_tcp_dist: <0.214.0>
[error_logger:info,2023-08-29T18:14:49.148Z,ns_1@cb.local:net_sup<0.198.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.212.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@cb.local',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.148Z,ns_1@cb.local:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.198.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@cb.local',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:49.149Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:configure_net_kernel:298]Set net_kernel vebosity to 10 -> 0
[ns_server:info,2023-08-29T18:14:49.158Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_node:159]saving node name '"ns_1@cb.local"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2023-08-29T18:14:49.194Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:bringup:265]Attempted to save node name to disk: ok
[ns_server:debug,2023-08-29T18:14:49.200Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:wait_for_node:272]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2023-08-29T18:14:49.201Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:49.206Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:49.206Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792641.218432>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:49.206Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.218432>,
                                  inet_tcp_dist,<0.216.0>,
                                  #Ref<0.4274179209.2407792641.218434>}
[ns_server:debug,2023-08-29T18:14:49.268Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:wait_for_node:284]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2023-08-29T18:14:49.269Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_address_config:146]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2023-08-29T18:14:49.269Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_address_config:147]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:info,2023-08-29T18:14:49.308Z,ns_1@cb.local:dist_manager<0.197.0>:dist_manager:save_address_config:150]Persisted the address successfully
[error_logger:info,2023-08-29T18:14:49.317Z,ns_1@cb.local:root_sup<0.196.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.197.0>},
              {id,dist_manager},
              {mfargs,{dist_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.329Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.219.0>},
              {id,local_tasks},
              {mfargs,{local_tasks,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:49.337Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:log_os_info:start_link:19]OS type: {unix,linux} Version: {5,15,90}
Runtime info: [{otp_release,"24"},
               {erl_version,"12.3.2.4"},
               {erl_version_long,
                   "Erlang/OTP 24 [erts-12.3.2.4] [source-c057945] [64-bit] [smp:8:8] [ds:8:8:10] [async-threads:16] [jit]\n"},
               {system_arch_raw,"x86_64-pc-linux-gnu"},
               {system_arch,"x86_64-pc-linux-gnu"},
               {localtime,{{2023,8,29},{18,14,49}}},
               {memory,
                   [{total,43374632},
                    {processes,9662896},
                    {processes_used,9653656},
                    {system,33711736},
                    {atom,532681},
                    {atom_used,500010},
                    {binary,183928},
                    {code,10396566},
                    {ets,2618896}]},
               {loaded,
                   [ns_info,log_os_info,local_tasks,restartable,
                    ns_server_cluster_sup,ns_cluster,dist_util,ns_node_disco,
                    crypto,re,auth,tls_dist_server_sup,tls_dist_sup,
                    ale_error_logger_handler,ssl_dist_admin_sup,ssl_dist_sup,
                    inet_tls_dist,inet_tcp_dist,inet_tcp,gen_tcp,erl_epmd,
                    cb_epmd,gen_udp,inet_hosts,dist_manager,root_sup,
                    path_config,cb_dist,ns_server_stats,calendar,
                    ale_default_formatter,'ale_logger-metakv',
                    'ale_logger-rebalance','ale_logger-chronicle',
                    'ale_logger-menelaus','ale_logger-stats',
                    'ale_logger-json_rpc','ale_logger-access',
                    'ale_logger-ns_server','ale_logger-user',
                    'ale_logger-ns_doctor','ale_logger-cluster',
                    'ale_logger-xdcr',erl_bits,otp_internal,
                    cb_log_counter_sink,ns_log_sink,ale_disk_sink,misc,
                    couch_util,ns_server,timer,cpu_sup,filelib,memsup,disksup,
                    os_mon,unicode_util,string,io,release_handler,
                    alarm_handler,sasl,httpd_sup,httpc_handler_sup,
                    httpc_cookie,inets_trace,httpc_manager,httpc,
                    httpc_profile_sup,httpc_sup,inets_sup,inets_app,ssl,
                    lhttpc_manager,lhttpc_sup,lhttpc,
                    dtls_server_session_cache_sup,dtls_listener_sup,
                    dtls_server_sup,dtls_connection_sup,dtls_sup,
                    ssl_upgrade_server_session_cache_sup,
                    ssl_server_session_cache_sup,
                    tls_server_session_ticket_sup,ssl_listen_tracker_sup,
                    tls_server_sup,tls_connection_sup,tls_sup,
                    ssl_connection_sup,tls_client_ticket_store,
                    ssl_client_session_cache_db,ssl_config,ssl_manager,
                    ssl_pkix_db,ssl_pem_cache,ssl_admin_sup,ssl_sup,
                    logger_h_common,logger_std_h,ssl_logger,ssl_app,
                    'ale_logger-ale_logger','ale_logger-error_logger',
                    beam_opcodes,beam_dict,beam_asm,beam_z,beam_flatten,
                    beam_trim,beam_clean,beam_peep,beam_block,beam_utils,
                    beam_jump,beam_a,beam_validator,beam_ssa_codegen,
                    beam_ssa_pre_codegen,beam_ssa_throw,beam_ssa_dead,
                    beam_call_types,beam_types,beam_ssa_type,beam_ssa_bc_size,
                    beam_ssa_opt,beam_ssa_funs,beam_ssa_bsm,beam_ssa_recv,
                    beam_ssa_share,beam_ssa_bool,beam_ssa,beam_kernel_to_ssa,
                    v3_kernel,sys_core_bsm,sys_core_alias,erl_bifs,
                    cerl_clauses,sets,sys_core_fold,sys_core_inline,
                    cerl_trees,core_lib,cerl,v3_core,erl_expand_records,sofs,
                    erl_internal,ordsets,compile,dynamic_compile,ale_utils,
                    io_lib_pretty,io_lib_format,io_lib,ale_codegen,dict,ale,
                    ale_dynamic_sup,ale_sup,ale_app,ns_bootstrap,child_erlang,
                    raw_file_io,orddict,c,erl_signal_handler,
                    logger_handler_watcher,logger_sup,kernel_refc,
                    kernel_config,user_io,user_sup,supervisor_bridge,
                    standard_error,global_group,erl_distribution,maps,rand,
                    net_kernel,global,rpc,epp,inet_gethost_native,inet_parse,
                    inet,inet_udp,inet_config,inet_db,unicode,os,gb_trees,
                    gb_sets,binary,erl_anno,proplists,erl_scan,queue,
                    logger_olp,logger_proxy,error_handler,application,
                    logger_filters,kernel,application_master,code,
                    logger_server,heart,error_logger,file_server,code_server,
                    erl_eval,logger_backend,file,application_controller,
                    logger_config,file_io_server,gen_server,ets,
                    logger_simple_h,filename,logger,proc_lib,gen,lists,
                    gen_event,supervisor,erl_lint,erl_parse,persistent_term,
                    counters,atomics,erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {applications,
                   [{asn1,"The Erlang ASN1 compiler version 5.0.18","5.0.18"},
                    {crypto,"CRYPTO","5.0.6.3"},
                    {public_key,"Public key infrastructure","1.12.0.1"},
                    {os_mon,"CPO  CXC 138 46","2.7.1"},
                    {inets,"INETS  CXC 138 49","7.5.3.1"},
                    {kernel,"ERTS  CXC 138 10","8.3.2.1"},
                    {stdlib,"ERTS  CXC 138 10","3.17.2"},
                    {ale,"Another Logger for Erlang","0.0.0"},
                    {sasl,"SASL  CXC 138 11","4.1.2"},
                    {lhttpc,"Lightweight HTTP Client","1.3.0"},
                    {ssl,"Erlang/OTP SSL application","10.7.3.3"},
                    {ns_server,"Couchbase server","7.2.0-5325-enterprise"}]},
               {pre_loaded,
                   [persistent_term,counters,atomics,
                    erts_dirty_process_signal_handler,
                    erts_literal_area_collector,erl_tracer,erts_internal,
                    erlang,erl_prim_loader,prim_zip,prim_net,prim_socket,
                    socket_registry,zlib,prim_file,prim_inet,prim_eval,
                    prim_buffer,init,erl_init,erts_code_purger]},
               {process_count,150},
               {node,'ns_1@cb.local'},
               {nodes,[]},
               {registered,
                   [tls_server_session_ticket_sup_dist,tls_connection_sup,
                    kernel_safe_sup,logger_proxy,sasl_sup,memsup,
                    'sink-disk_error',ssl_pem_cache_dist,ale_dynamic_sup,
                    'sink-disk_reports',erl_prim_loader,
                    application_controller,tls_server_sup,ssl_admin_sup,
                    net_kernel,inet_db,'sink-disk_json_rpc',
                    ssl_server_session_cache_sup,httpc_profile_sup,
                    sasl_safe_sup,ssl_listen_tracker_sup_dist,dtls_server_sup,
                    rex,logger_handler_watcher,tls_dist_server_sup,tls_sup,
                    'sink-disk_default',root_sup,erl_epmd,kernel_refc,
                    httpc_sup,ssl_pem_cache,dtls_connection_sup,
                    'sink-disk_stats',ssl_manager_dist,'sink-disk_metakv',
                    lhttpc_manager,auth,httpc_handler_sup,erts_code_purger,
                    cpu_sup,logger_std_h_ssl_handler,dtls_listener_sup,ale,
                    kernel_sup,erl_signal_server,ssl_dist_admin_sup,
                    tls_server_session_ticket_sup,ssl_connection_sup,
                    lhttpc_sup,user,ssl_upgrade_server_session_cache_sup,
                    'sink-disk_xdcr',tls_client_ticket_store,ale_sup,
                    dtls_server_session_cache_sup,os_mon_sup,
                    'sink-disk_access_int',httpd_sup,disksup,init,cb_dist,
                    global_group,standard_error_sup,file_server_2,
                    'sink-cb_log_counter',ssl_dist_sup,
                    ssl_upgrade_server_session_cache_sup_dist,inets_sup,
                    global_name_server,ssl_sup,ns_server_cluster_sup,
                    standard_error,httpc_manager,logger_sup,'sink-disk_debug',
                    ssl_listen_tracker_sup,socket_registry,logger,
                    release_handler,'sink-ns_log',dtls_sup,'sink-disk_access',
                    code_server,ale_stats_events,local_tasks,net_sup,
                    tls_dist_sup,timer_server,tls_dist_connection_sup,
                    alarm_handler,dist_manager,ssl_manager]},
               {cookie,nocookie},
               {wordsize,8},
               {wall_clock,5}]
[ns_server:info,2023-08-29T18:14:49.358Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:log_os_info:start_link:21]Manifest:
["<manifest>",
 "  <remote name=\"blevesearch\" fetch=\"https://github.com/blevesearch/\" />",
 "  <remote name=\"couchbase\" fetch=\"https://github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbase-priv\" fetch=\"ssh://git@github.com/couchbase/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbasedeps\" fetch=\"https://github.com/couchbasedeps/\" review=\"review.couchbase.org\" />",
 "  <remote name=\"couchbaselabs\" fetch=\"https://github.com/couchbaselabs/\" review=\"review.couchbase.org\" />",
 "  ","  <default remote=\"couchbase\" revision=\"master\" />","  ",
 "  <project name=\"HdrHistogram_c\" path=\"third_party/HdrHistogram_c\" remote=\"couchbasedeps\" revision=\"caed837aa163421a637222157b3f6353b4ca831a\" groups=\"kv\" />",
 "  <project name=\"analytics-dcp-client\" path=\"analytics/java-dcp-client\" revision=\"d4bb7bf6444805681da7f564ee9f1ea71895c113\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"asterixdb\" path=\"analytics/asterixdb\" revision=\"e42e1b99e7f9a5a7437234be89438f37a18a06d2\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"backup\" remote=\"couchbase-priv\" revision=\"b5572a18a617c7445eff15bd9d07447ba8430378\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"bbolt\" path=\"godeps/src/go.etcd.io/bbolt\" remote=\"couchbasedeps\" revision=\"68cc10a767ea1c6b9e8dcb9847317ff192d6d974\" />",
 "  <project name=\"bitset\" path=\"godeps/src/github.com/willf/bitset\" remote=\"couchbasedeps\" revision=\"28a4168144bb8ac95454e1f51c84da1933681ad4\" />",
 "  <project name=\"blance\" path=\"godeps/src/github.com/couchbase/blance\" revision=\"b2ec44a33677b3c64c516ed9e9c49721e266a58a\" />",
 "  <project name=\"bolt\" path=\"godeps/src/github.com/boltdb/bolt\" remote=\"couchbasedeps\" revision=\"51f99c862475898df9773747d3accd05a7ca33c1\" />",
 "  <project name=\"build\" path=\"cbbuild\" revision=\"9b1df055e536d63896a595e6e38f320e45c5a21d\" groups=\"notdefault,build\">",
 "    <annotation name=\"RELEASE\" value=\"neo\" />",
 "    <annotation name=\"PRODUCT\" value=\"couchbase-server\" />",
 "    <annotation name=\"BLD_NUM\" value=\"5325\" />",
 "    <annotation name=\"VERSION\" value=\"7.2.0\" />",
 "    <annotation name=\"BSL_PRODUCT\" value=\"Couchbase Server\" />",
 "    <annotation name=\"BSL_VERSION\" value=\"7.2\" />",
 "    <annotation name=\"BSL_CHANGE_DATE\" value=\"April 1, 2026\" />",
 "  </project>",
 "  <project name=\"cbas\" path=\"goproj/src/github.com/couchbase/cbas\" remote=\"couchbase-priv\" revision=\"f7ea66f5afc13368e6b538c6b72e7472eaaabafe\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-core\" path=\"analytics\" remote=\"couchbase-priv\" revision=\"19c0e48f50ce6d57dda421cee1d7c2bceec13c59\" upstream=\"7.2.0\" dest-branch=\"7.2.0\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbas-ui\" revision=\"bca24852854206cbdc70095ddb15c9f07fa29196\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"cbauth\" path=\"goproj/src/github.com/couchbase/cbauth\" revision=\"efcd43440ed31b277503e76fdb3c7efc11154f59\" groups=\"backup\" />",
 "  <project name=\"cbbs\" remote=\"couchbase-priv\" revision=\"313647360e0468dfa2f870a4656d7a82bf83df0b\" groups=\"backup,notdefault,enterprise\" />",
 "  <project name=\"cbft\" revision=\"02a030d5ccd77934f8713329395ca7ccad9d6115\" />",
 "  <project name=\"cbftx\" remote=\"couchbase-priv\" revision=\"31cb03c796e9555bd5d323a704680625bab79a2f\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"cbgt\" revision=\"d685dcd6c513b149c7e1aeeb0b09a8349a79b76f\" />",
 "  <project name=\"cbsummary\" path=\"goproj/src/github.com/couchbase/cbsummary\" revision=\"ae0e924e54669e2c590dd70c6bdf742e8976e2d0\" />",
 "  <project name=\"chronicle\" path=\"ns_server/deps/chronicle\" revision=\"67d2c3a847d8feeedd2c76ab383b508f1c29549c\" />",
 "  <project name=\"clog\" path=\"godeps/src/github.com/couchbase/clog\" revision=\"f935d1fdfc36541b505cf86fea4822e4067f9c39\" />",
 "  <project name=\"cobra\" path=\"godeps/src/github.com/spf13/cobra\" remote=\"couchbasedeps\" revision=\"0f056af21f5f368e5b0646079d0094a2c64150f7\" />",
 "  <project name=\"context\" path=\"godeps/src/github.com/gorilla/context\" remote=\"couchbasedeps\" revision=\"215affda49addc4c8ef7e2534915df2c8c35c6cd\" />",
 "  <project name=\"couchbase-cli\" revision=\"73715f3cb60a054b985dd8a93a34323ef1f737c4\" groups=\"kv\" />",
 "  <project name=\"couchbase-jvm-core\" path=\"analytics/couchbase-jvm-core\" revision=\"b712718b5a9a3ea4699e537871644446494fe9e3\" upstream=\"refs/tags/1.7.24\" dest-branch=\"refs/tags/1.7.24\" groups=\"notdefault,enterprise,analytics\" />",
 "  <project name=\"couchdb\" revision=\"24833e246a9d94ddd1b82a34bfd94d47c51a6ea9\" />",
 "  <project name=\"couchdbx-app\" revision=\"64bdc899ba72d021a3c1dde1a1aa5b698f42ee06\" groups=\"notdefault,packaging\" />",
 "  <project name=\"couchstore\" revision=\"42e62095c614f704d004d10fe50d324ff4a0dc1f\" groups=\"kv\" />",
 "  <project name=\"crypto\" path=\"godeps/src/golang.org/x/crypto\" remote=\"couchbasedeps\" revision=\"630584e8d5aaa1472863b49679b2d5548d80dcba\" />",
 "  <project name=\"cuckoofilter\" path=\"godeps/src/github.com/seiflotfy/cuckoofilter\" remote=\"couchbasedeps\" revision=\"d04838794ab86926d32b124345777e55e6f43974\" />",
 "  <project name=\"docloader\" path=\"goproj/src/github.com/couchbase/docloader\" revision=\"2e6af0c097c8bb98a596cbc81cdf6e169ae5b3cc\" />",
 "  <project name=\"errors\" path=\"godeps/src/github.com/pkg/errors\" remote=\"couchbasedeps\" revision=\"30136e27e2ac8d167177e8a583aa4c3fea5be833\" />",
 "  <project name=\"eventing\" path=\"goproj/src/github.com/couchbase/eventing\" revision=\"96fb41ba9fb6e3d20954244b636343d43cb70333\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"eventing-ee\" path=\"goproj/src/github.com/couchbase/eventing-ee\" remote=\"couchbase-priv\" revision=\"6b4b1f0e33ab3445c2d782b787af83bececf1bf6\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"flatbuffers\" path=\"godeps/src/github.com/google/flatbuffers\" remote=\"couchbasedeps\" revision=\"1a8968225130caeddd16e227678e6f8af1926303\" />",
 "  <project name=\"forestdb\" revision=\"48c31dcb979ca7e152c2db570af49149c6d3e2a7\" groups=\"backup\" />",
 "  <project name=\"fwd\" path=\"godeps/src/github.com/philhofer/fwd\" remote=\"couchbasedeps\" revision=\"bb6d471dc95d4fe11e432687f8b70ff496cf3136\" />",
 "  <project name=\"geocouch\" revision=\"68f3b9d36630682d17ca5232770f1693b9b8fa18\" />",
 "  <project name=\"ghistogram\" path=\"godeps/src/github.com/couchbase/ghistogram\" revision=\"4ae3f06d0ac7b02081e33c1ec309daa22838d207\" />",
 "  <project name=\"go-bindata-assetfs\" path=\"godeps/src/github.com/elazarl/go-bindata-assetfs\" remote=\"couchbasedeps\" revision=\"57eb5e1fc594ad4b0b1dbea7b286d299e0cb43c2\" />",
 "  <project name=\"go-couchbase\" path=\"goproj/src/github.com/couchbase/go-couchbase\" revision=\"959eaf944140a6c660990f38b1db310ddd6d8e42\" groups=\"backup\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/andelf/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" />",
 "  <project name=\"go-curl\" path=\"godeps/src/github.com/couchbasedeps/go-curl\" remote=\"couchbasedeps\" revision=\"f0b2afc926ec79be5d7f30393b3485352781a705\" />",
 "  <project name=\"go-genproto\" path=\"godeps/src/google.golang.org/genproto\" remote=\"couchbasedeps\" revision=\"2b5a72b8730b0b16380010cfe5286c42108d88e7\" />",
 "  <project name=\"go-jsonpointer\" path=\"godeps/src/github.com/dustin/go-jsonpointer\" remote=\"couchbasedeps\" revision=\"75939f54b39e7dafae879e61f65438dadc5f288c\" />",
 "  <project name=\"go-metrics\" path=\"godeps/src/github.com/rcrowley/go-metrics\" remote=\"couchbasedeps\" revision=\"dee209f2455f101a5e4e593dea94872d2c62d85d\" />",
 "  <project name=\"go-runewidth\" path=\"godeps/src/github.com/mattn/go-runewidth\" remote=\"couchbasedeps\" revision=\"703b5e6b11ae25aeb2af9ebb5d5fdf8fa2575211\" />",
 "  <project name=\"go-slab\" path=\"godeps/src/github.com/couchbase/go-slab\" revision=\"e47646b420b3c9eb344cef022236a54e2554d40b\" groups=\"bsl\" />",
 "  <project name=\"go-unsnap-stream\" path=\"godeps/src/github.com/glycerine/go-unsnap-stream\" remote=\"couchbasedeps\" revision=\"62a9a9eb44fd8932157b1a8ace2149eff5971af6\" />",
 "  <project name=\"go-zookeeper\" path=\"godeps/src/github.com/samuel/go-zookeeper\" remote=\"couchbasedeps\" revision=\"fa6674abf3f4580b946a01bf7a1ce4ba8766205b\" />",
 "  <project name=\"go_json\" path=\"goproj/src/github.com/couchbase/go_json\" revision=\"2936e21ca64c6efb687dd891dee1f8112e11618c\" />",
 "  <project name=\"go_n1ql\" path=\"godeps/src/github.com/couchbase/go_n1ql\" revision=\"0ed4bf93e31de2371f9180e424942bd3d5235397\" groups=\"bsl\" />",
 "  <project name=\"gocb\" path=\"godeps/src/github.com/couchbase/gocb/v2\" revision=\"40020eef484873e507745107edbf99c421927a93\" upstream=\"refs/tags/v2.2.5\" dest-branch=\"refs/tags/v2.2.5\" />",
 "  <project name=\"gocb\" path=\"godeps/src/gopkg.in/couchbase/gocb.v1\" revision=\"01c846cb025ddd50a2ef4c82a27992b40c230dbb\" upstream=\"refs/tags/v1.4.2\" dest-branch=\"refs/tags/v1.4.2\" />",
 "  <project name=\"gocbconnstr\" path=\"godeps/src/gopkg.in/couchbaselabs/gocbconnstr.v1\" remote=\"couchbaselabs\" revision=\"8f9a894d174b836c6362de9af75545cf585fc278\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/gopkg.in/couchbase/gocbcore.v7\" revision=\"441cb91f01ce26932514ec10d9e59e568ee27722\" upstream=\"refs/tags/v7.1.14\" dest-branch=\"refs/tags/v7.1.14\" />",
 "  <project name=\"gocbcore\" path=\"godeps/src/github.com/couchbase/gocbcore/v9\" revision=\"0ece206041d8cf5f5fcd919767446603691bdb69\" upstream=\"refs/tags/v9.1.6\" dest-branch=\"refs/tags/v9.1.6\" />",
 "  <project name=\"godbc\" path=\"goproj/src/github.com/couchbase/godbc\" revision=\"c4f5a4b362d1213ab5fd8f19c631b824c1c8b46f\" />",
 "  <project name=\"gofarmhash\" path=\"godeps/src/github.com/leemcloughlin/gofarmhash\" remote=\"couchbasedeps\" revision=\"0a055c5b87a8c55ce83459cbf2776b563822a942\" />",
 "  <project name=\"goforestdb\" path=\"godeps/src/github.com/couchbase/goforestdb\" revision=\"0b501227de0e8c55d99ed14e900eea1a1dbaf899\" />",
 "  <project name=\"gojson\" path=\"godeps/src/github.com/dustin/gojson\" remote=\"couchbasedeps\" revision=\"af16e0e771e2ed110f2785564ae33931de8829e4\" />",
 "  <project name=\"gojsonsm\" path=\"godeps/src/github.com/couchbaselabs/gojsonsm\" remote=\"couchbaselabs\" revision=\"eec4953dcb855282c483b8cd4fe03a8074e2f7a1\" />",
 "  <project name=\"golang\" remote=\"couchbaselabs\" revision=\"fba78230720763fadaa3379988d1c28f1c1fef3d\" />",
 "  <project name=\"golang-pkg-pcre\" path=\"godeps/src/github.com/glenn-brown/golang-pkg-pcre\" remote=\"couchbasedeps\" revision=\"48bb82a8b8ceea98f4e97825b43870f6ba1970d6\" />",
 "  <project name=\"golang-snappy\" path=\"godeps/src/github.com/golang/snappy\" remote=\"couchbasedeps\" revision=\"723cc1e459b8eea2dea4583200fd60757d40097a\" />",
 "  <project name=\"golang-tools\" path=\"godeps/src/golang.org/x/tools\" remote=\"couchbasedeps\" revision=\"a28dfb48e06b2296b66678872c2cb638f0304f20\" />",
 "  <project name=\"goleveldb\" path=\"godeps/src/github.com/syndtr/goleveldb\" remote=\"couchbasedeps\" revision=\"fa5b5c78794bc5c18f330361059f871ae8c2b9d6\" />",
 "  <project name=\"gomemcached\" path=\"goproj/src/github.com/couchbase/gomemcached\" revision=\"a6b8a5bb473dea57823464bc35939733de64656a\" groups=\"backup\" />",
 "  <project name=\"gometa\" path=\"goproj/src/github.com/couchbase/gometa\" revision=\"973d61970b54b3444fca2e9562eabd91e2259b7f\" />",
 "  <project name=\"goskiplist\" path=\"godeps/src/github.com/ryszard/goskiplist\" remote=\"couchbasedeps\" revision=\"2dfbae5fcf46374f166f8969cb07e167f1be6273\" />",
 "  <project name=\"gosnappy\" path=\"godeps/src/github.com/syndtr/gosnappy\" remote=\"couchbasedeps\" revision=\"156a073208e131d7d2e212cb749feae7c339e846\" />",
 "  <project name=\"goutils\" path=\"goproj/src/github.com/couchbase/goutils\" revision=\"73dda2bf44424b5c588579948399e86e5de4be6c\" groups=\"bsl\" />",
 "  <project name=\"goxdcr\" path=\"goproj/src/github.com/couchbase/goxdcr\" revision=\"6d79cea3c1ac9a788f3af5266c4b010801c39e8d\" groups=\"bsl\" />",
 "  <project name=\"grpc-go\" path=\"godeps/src/google.golang.org/grpc\" remote=\"couchbasedeps\" revision=\"df014850f6dee74ba2fc94874043a9f3f75fbfd8\" upstream=\"refs/tags/v1.17.0\" dest-branch=\"refs/tags/v1.17.0\" />",
 "  <project name=\"gsl-lite\" path=\"third_party/gsl-lite\" remote=\"couchbasedeps\" revision=\"e1c381746c2625a76227255f999ae9f14a062208\" upstream=\"refs/tags/v0.38.1\" dest-branch=\"refs/tags/v0.38.1\" groups=\"kv\" />",
 "  <project name=\"gtreap\" path=\"godeps/src/github.com/steveyen/gtreap\" remote=\"couchbasedeps\" revision=\"0abe01ef9be25c4aedc174758ec2d917314d6d70\" />",
 "  <project name=\"hebrew\" remote=\"couchbase-priv\" revision=\"fd6a0a1013434e4ba8cae73f6c80d2433d72ed0b\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"httprouter\" path=\"godeps/src/github.com/julienschmidt/httprouter\" remote=\"couchbasedeps\" revision=\"975b5c4c7c21c0e3d2764200bf2aa8e34657ae6e\" />",
 "  <project name=\"indexing\" path=\"goproj/src/github.com/couchbase/indexing\" revision=\"58e9984dda715a2e8916ce22d9890e0c792b72ff\" upstream=\"neo\" dest-branch=\"neo\" groups=\"bsl\" />",
 "  <project name=\"json-iterator-go\" path=\"godeps/src/github.com/json-iterator/go\" remote=\"couchbasedeps\" revision=\"f7279a603edee96fe7764d3de9c6ff8cf9970994\" />",
 "  <project name=\"jsonparser\" path=\"godeps/src/github.com/buger/jsonparser\" remote=\"couchbasedeps\" revision=\"df3ea76ece10095374fd1c9a22a4fb85a44efc42\" />",
 "  <project name=\"jsonschema\" path=\"godeps/src/github.com/santhosh-tekuri/jsonschema\" remote=\"couchbasedeps\" revision=\"137f44a49015e5060a447c331aa37de6e0f50267\" />",
 "  <project name=\"jsonx\" path=\"godeps/src/gopkg.in/couchbaselabs/jsonx.v1\" remote=\"couchbaselabs\" revision=\"03f375ceefb769799cfa0d64352fdcc9f1192368\" />",
 "  <project name=\"kv_engine\" revision=\"840f209f829d1dd3d29829aff42d57b43492c729\" upstream=\"neo\" dest-branch=\"neo\" groups=\"kv,bsl\" />",
 "  <project name=\"libcouchbase\" revision=\"02e19c2144f1284846f7ce6d36b707891b8d2624\" upstream=\"refs/tags/3.3.5\" dest-branch=\"refs/tags/3.3.5\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/peterh/liner\" remote=\"couchbasedeps\" revision=\"6f820f8f90ce9482ffbd40bb15f9ea9932f4942d\" />",
 "  <project name=\"liner\" path=\"godeps/src/github.com/sbinet/liner\" remote=\"couchbasedeps\" revision=\"d9335eee40a45a4f5d74524c90040d6fe6013d50\" />",
 "  <project name=\"logstats\" path=\"godeps/src/github.com/couchbase/logstats\" revision=\"24ba9753289f155ab6d43a9a2585b9248da79791\" groups=\"bsl\" />",
 "  <project name=\"magma\" remote=\"couchbase-priv\" revision=\"6a3cf88fe676c9c49721fddaa07a929664ec5670\" upstream=\"neo\" dest-branch=\"neo\" groups=\"notdefault,enterprise,kv_ee\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/blevesearch/mmap-go\" remote=\"blevesearch\" revision=\"99940f54c59671cf69e10b2e4041fabce88eb9b2\" />",
 "  <project name=\"mmap-go\" path=\"godeps/src/github.com/edsrzf/mmap-go\" remote=\"couchbasedeps\" revision=\"935e0e8a636ca4ba70b713f3e38a19e1b77739e8\" />",
 "  <project name=\"moss\" path=\"godeps/src/github.com/couchbase/moss\" revision=\"4fae7b31078a3e2bd5848a7029754885cdc495e0\" groups=\"bsl\" />",
 "  <project name=\"mossScope\" path=\"godeps/src/github.com/couchbase/mossScope\" revision=\"9e34f3688e0abd1b057ea2196f02e45f830506f8\" groups=\"bsl\" />",
 "  <project name=\"mousetrap\" path=\"godeps/src/github.com/inconshreveable/mousetrap\" remote=\"couchbasedeps\" revision=\"76626ae9c91c4f2a10f34cad8ce83ea42c93bb75\" />",
 "  <project name=\"msgp\" path=\"godeps/src/github.com/tinylib/msgp\" remote=\"couchbasedeps\" revision=\"5bb5e1aed7ba5bcc93307153b020e7ffe79b0509\" />",
 "  <project name=\"mux\" path=\"godeps/src/github.com/gorilla/mux\" remote=\"couchbasedeps\" revision=\"043ee6597c29786140136a5747b6a886364f5282\" />",
 "  <project name=\"n1fty\" path=\"goproj/src/github.com/couchbase/n1fty\" revision=\"374be069692d011f6e075448a43c3c9ac11288b9\" groups=\"bsl\" />",
 "  <project name=\"net\" path=\"godeps/src/golang.org/x/net\" remote=\"couchbasedeps\" revision=\"44b7c21cbf19450f38b337eb6b6fe4f6496fb5b3\" />",
 "  <project name=\"nitro\" path=\"goproj/src/github.com/couchbase/nitro\" revision=\"2575ec52bf5cf4c7ccc2cbc161eb38e46ce7b4a8\" groups=\"bsl\" />",
 "  <project name=\"npipe\" path=\"godeps/src/github.com/natefinch/npipe\" remote=\"couchbasedeps\" revision=\"272c8150302e83f23d32a355364578c9c13ab20f\" />",
 "  <project name=\"ns_server\" revision=\"0874c45314b51b010a16c80872e78eff39a1eff9\" groups=\"bsl\" />",
 "  <project name=\"opentracing-go\" path=\"godeps/src/github.com/opentracing/opentracing-go\" remote=\"couchbasedeps\" revision=\"1949ddbfd147afd4d964a9f00b24eb291e0e7c38\" />",
 "  <project name=\"participle\" path=\"godeps/src/github.com/alecthomas/participle\" remote=\"couchbasedeps\" revision=\"d638c6e1953ed899e05a34da3935146790c60e46\" />",
 "  <project name=\"pflag\" path=\"godeps/src/github.com/spf13/pflag\" remote=\"couchbasedeps\" revision=\"a232f6d9f87afaaa08bafaff5da685f974b83313\" />",
 "  <project name=\"phosphor\" revision=\"2eb6c244d6910baf2834513ece579ba88e4f9b9d\" groups=\"bsl,kv\" />",
 "  <project name=\"pierrec-lz4\" path=\"godeps/src/github.com/pierrec/lz4\" remote=\"couchbasedeps\" revision=\"ed8d4cc3b461464e69798080a0092bd028910298\" />",
 "  <project name=\"pierrec-xxHash\" path=\"godeps/src/github.com/pierrec/xxHash\" remote=\"couchbasedeps\" revision=\"a0006b13c722f7f12368c00a3d3c2ae8a999a0c6\" />",
 "  <project name=\"pkcs8\" path=\"godeps/src/github.com/youmark/pkcs8\" remote=\"couchbasedeps\" revision=\"1be2e3e5546da8a58903ff4adcfab015022538ea\" upstream=\"refs/tags/v1.1\" dest-branch=\"refs/tags/v1.1\" />",
 "  <project name=\"plasma\" path=\"goproj/src/github.com/couchbase/plasma\" remote=\"couchbase-priv\" revision=\"bbf0fa45d2b5a9c80265d7061d1e3f15f1a76d3a\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"platform\" revision=\"d9c07467aacee7c5255b54591bec6549f0927427\" groups=\"bsl,kv\" />",
 "  <project name=\"product-metadata\" revision=\"ff7c24047435e393698847ec5147716017048636\" groups=\"notdefault,packaging\" />",
 "  <project name=\"product-texts\" revision=\"ec09296ae7979a7bf5ca80c25842afca45e0ec99\" upstream=\"master\" dest-branch=\"master\" />",
 "  <project name=\"protobuf\" path=\"godeps/src/github.com/golang/protobuf\" remote=\"couchbasedeps\" revision=\"ddf22928ea3c56eb4292a0adbbf5001b1e8e7d0d\" />",
 "  <project name=\"query\" path=\"goproj/src/github.com/couchbase/query\" revision=\"b23f87ebee1a1872e632bd88d6d50b38e01a0ff4\" groups=\"bsl\" />",
 "  <project name=\"query-ee\" path=\"goproj/src/github.com/couchbase/query-ee\" remote=\"couchbase-priv\" revision=\"5bdc90a7eeb901ced3119b826532238d84f5d214\" groups=\"notdefault,enterprise\" />",
 "  <project name=\"query-ui\" revision=\"2d022ec7cd7d634ef287720f0f9a65e75fd6ffe3\" groups=\"bsl\" />",
 "  <project name=\"retriever\" path=\"godeps/src/github.com/couchbase/retriever\" revision=\"295b11134f91d9451c3ae21895f5615fc7a61e31\" groups=\"bsl\" />",
 "  <project name=\"roaring\" path=\"godeps/src/github.com/RoaringBitmap/roaring\" remote=\"couchbasedeps\" revision=\"4208ad825dda03a6a3d2197df8ec57948aebcc12\" />",
 "  <project name=\"sigar\" revision=\"a6fa08d5344f6f901e0e44b9a0f2f2f7ee7d0bf3\" groups=\"kv\" />",
 "  <project name=\"subjson\" revision=\"0820f83427d69c6eb737876eb2f2cf6aefa45802\" groups=\"bsl,kv\" />",
 "  <project name=\"sys\" path=\"godeps/src/golang.org/x/sys\" remote=\"couchbasedeps\" revision=\"d36c6a25d886e7c9975d5bf247ac24887ba6da37\" />",
 "  <project name=\"testrunner\" revision=\"a5aa11295cdaebc1888e63005458fb58197e8fb6\" upstream=\"neo\" dest-branch=\"neo\" />",
 "  <project name=\"text\" path=\"godeps/src/golang.org/x/text\" remote=\"couchbasedeps\" revision=\"383b2e75a7a4198c42f8f87833eefb772868a56f\" upstream=\"refs/tags/v0.3.7\" dest-branch=\"refs/tags/v0.3.7\" />",
 "  <project name=\"tlm\" revision=\"5bee122098c58e20a51f71aa8b95e68638945090\" groups=\"bsl,kv\">",
 "    <copyfile src=\"GNUmakefile\" dest=\"GNUmakefile\" />",
 "    <copyfile src=\"Makefile\" dest=\"Makefile\" />",
 "    <copyfile src=\"CMakeLists.txt\" dest=\"CMakeLists.txt\" />",
 "    <copyfile src=\"dot-clang-format\" dest=\".clang-format\" />",
 "    <copyfile src=\"dot-clang-tidy\" dest=\".clang-tidy\" />",
 "    <copyfile src=\"third-party-CMakeLists.txt\" dest=\"third_party/CMakeLists.txt\" />",
 "  </project>",
 "  <project name=\"udf-api\" path=\"goproj/src/github.com/couchbase/udf-api\" revision=\"b2788ae3d412356a330b36d7f38ad2c66edb5879\" />",
 "  <project name=\"uuid\" path=\"godeps/src/github.com/google/uuid\" remote=\"couchbasedeps\" revision=\"dec09d789f3dba190787f8b4454c7d3c936fed9e\" />",
 "  <project name=\"vbmap\" revision=\"76ac2b92a3edbec1602d7e060d4d7d86dcb47887\" />",
 "  <project name=\"voltron\" remote=\"couchbase-priv\" revision=\"be2e144bead253e66c80e5c03e8074f9499e544a\" groups=\"notdefault,packaging\" />",
 "  <project name=\"zstd\" path=\"godeps/src/github.com/DataDog/zstd\" remote=\"couchbasedeps\" revision=\"aebefd9fcb99f22cd691ef778a12ed68f0e6a1ab\" />",
 "</manifest>"]

[error_logger:info,2023-08-29T18:14:49.370Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.220.0>},
              {id,timeout_diag_logger},
              {mfargs,{timeout_diag_logger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.372Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.221.0>},
              {id,ns_cookie_manager},
              {mfargs,{ns_cookie_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:49.381Z,ns_1@cb.local:chronicle_local<0.222.0>:chronicle_local:init:54]Ensure chronicle is started
[error_logger:info,2023-08-29T18:14:49.422Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.228.0>},
              {id,chronicle_events},
              {mfargs,{chronicle_events,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.422Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.229.0>},
              {id,chronicle_external_events},
              {mfargs,{gen_event,start_link,
                                 [{local,chronicle_external_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.426Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.230.0>},
              {id,chronicle_ets},
              {mfargs,{chronicle_ets,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.430Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.231.0>},
              {id,chronicle_settings},
              {mfargs,{chronicle_settings,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.436Z,ns_1@cb.local:chronicle_agent_sup<0.232.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.233.0>},
              {id,chronicle_snapshot_mgr},
              {mfargs,{chronicle_snapshot_mgr,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.436Z,ns_1@cb.local:chronicle_agent_sup<0.232.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.234.0>},
              {id,chronicle_rsm_events},
              {mfargs,{chronicle_events,start_link,[chronicle_rsm_events]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[chronicle:info,2023-08-29T18:14:49.619Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_storage:open_current_log:220]Error while opening log file "/opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log": no_header
[chronicle:info,2023-08-29T18:14:49.619Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_storage:create_log:268]Creating log file /opt/couchbase/var/lib/couchbase/config/chronicle/logs/0.log (high seqno = 0)
[chronicle:info,2023-08-29T18:14:49.652Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_agent:maybe_seed_storage:2444]Found empty storage. Seeding it with default metadata:
#{committed_seqno => 0,history_id => <<"no-history">>,peer => nonode@nohost,
  peer_id => <<>>,pending_branch => undefined,state => not_provisioned,
  term => {0,nonode@nohost}}
[error_logger:info,2023-08-29T18:14:49.663Z,ns_1@cb.local:chronicle_agent_sup<0.232.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_agent_sup}
    started: [{pid,<0.235.0>},
              {id,chronicle_agent},
              {mfargs,{chronicle_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.663Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.232.0>},
              {id,chronicle_agent_sup},
              {mfargs,{chronicle_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.669Z,ns_1@cb.local:chronicle_sup<0.227.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_sup}
    started: [{pid,<0.237.0>},
              {id,chronicle_secondary_sup},
              {mfargs,{chronicle_secondary_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.669Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: chronicle
    started_at: 'ns_1@cb.local'

[ns_server:debug,2023-08-29T18:14:49.673Z,ns_1@cb.local:chronicle_local<0.222.0>:chronicle_local:init:58]Chronicle state is: not_provisioned
[ns_server:debug,2023-08-29T18:14:49.673Z,ns_1@cb.local:chronicle_local<0.222.0>:chronicle_local:provision:139]Provision chronicle on this node
[chronicle:debug,2023-08-29T18:14:49.678Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_agent:handle_provision:1199]Provisioning with history <<"918babd26e22549c990a18e4926f0ef9">>. Config:
{config,undefined,0,undefined,
        #{'ns_1@cb.local' =>
              #{id => <<"0957cbe35228f96d2df30f13ea1a5217">>,role => voter}},
        undefined,
        #{chronicle_config_rsm => {rsm_config,chronicle_config_rsm,[]},
          kv => {rsm_config,chronicle_kv,[]}},
        #{},undefined,
        [{<<"918babd26e22549c990a18e4926f0ef9">>,0}]}
[error_logger:info,2023-08-29T18:14:49.689Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.239.0>},
              {id,chronicle_leader},
              {mfargs,{chronicle_leader,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.695Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.241.0>},
              {id,chronicle_status},
              {mfargs,{chronicle_status,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.698Z,ns_1@cb.local:chronicle_secondary_restartable_sup<0.240.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,chronicle_secondary_restartable_sup}
    started: [{pid,<0.242.0>},
              {id,chronicle_failover},
              {mfargs,{chronicle_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.698Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.240.0>},
              {id,chronicle_secondary_restartable_sup},
              {mfargs,{chronicle_secondary_restartable_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.702Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.243.0>},
              {id,chronicle_server},
              {mfargs,{chronicle_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[chronicle:info,2023-08-29T18:14:49.737Z,ns_1@cb.local:chronicle_config_rsm<0.247.0>:chronicle_rsm:get_incarnation:1592]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/chronicle_config_rsm/incarnation'
[chronicle:debug,2023-08-29T18:14:49.771Z,ns_1@cb.local:chronicle_server<0.243.0>:chronicle_server:handle_register_rsm:361]Registering RSM chronicle_config_rsm with pid <0.247.0>
[error_logger:info,2023-08-29T18:14:49.771Z,ns_1@cb.local:<0.246.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.246.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.247.0>},
              {id,chronicle_config_rsm},
              {mfargs,{chronicle_rsm,start_link,
                                     [chronicle_config_rsm,
                                      <<"0957cbe35228f96d2df30f13ea1a5217">>,
                                      chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.771Z,ns_1@cb.local:<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.245.0>,dynamic_supervisor}
    started: [{pid,<0.246.0>},
              {id,chronicle_config_rsm},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [chronicle_config_rsm,
                       <<"0957cbe35228f96d2df30f13ea1a5217">>,
                       chronicle_config_rsm,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.776Z,ns_1@cb.local:<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.249.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.250.0>},
              {id,'kv-events'},
              {mfargs,{gen_event,start_link,[{local,'kv-events'}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[chronicle:info,2023-08-29T18:14:49.798Z,ns_1@cb.local:kv<0.251.0>:chronicle_rsm:get_incarnation:1592]No incarnation file found at '/opt/couchbase/var/lib/couchbase/config/chronicle/rsms/kv/incarnation'
[chronicle:debug,2023-08-29T18:14:49.834Z,ns_1@cb.local:chronicle_server<0.243.0>:chronicle_server:handle_register_rsm:361]Registering RSM kv with pid <0.251.0>
[error_logger:info,2023-08-29T18:14:49.834Z,ns_1@cb.local:<0.249.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.249.0>,chronicle_single_rsm_sup}
    started: [{pid,<0.251.0>},
              {id,kv},
              {mfargs,{chronicle_rsm,start_link,
                                     [kv,
                                      <<"0957cbe35228f96d2df30f13ea1a5217">>,
                                      chronicle_kv,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.835Z,ns_1@cb.local:<0.245.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.245.0>,dynamic_supervisor}
    started: [{pid,<0.249.0>},
              {id,kv},
              {mfargs,
                  {chronicle_single_rsm_sup,start_link,
                      [kv,<<"0957cbe35228f96d2df30f13ea1a5217">>,chronicle_kv,
                       []]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.835Z,ns_1@cb.local:<0.238.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.238.0>,dynamic_supervisor}
    started: [{pid,<0.244.0>},
              {id,chronicle_rsm_sup},
              {mfargs,{chronicle_rsm_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:49.837Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.222.0>},
              {id,chronicle_local},
              {mfargs,{chronicle_local,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:49.844Z,ns_1@cb.local:ns_cluster<0.253.0>:ns_cluster:handle_info:523]Chronicle state is: provisioned
[error_logger:info,2023-08-29T18:14:49.844Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.253.0>},
              {id,ns_cluster},
              {mfargs,{ns_cluster,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:49.848Z,ns_1@cb.local:sigar<0.255.0>:sigar:spawn_sigar:135]Spawing sigar process 'portsigar for ns_1@cb.local'("/opt/couchbase/bin/sigar_port") with babysitter pid: 42
[error_logger:info,2023-08-29T18:14:49.849Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.255.0>},
              {id,sigar},
              {mfargs,{sigar,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:49.852Z,ns_1@cb.local:ns_config_sup<0.256.0>:ns_config_sup:init:26]loading static ns_config from "/opt/couchbase/etc/couchbase/config"
[error_logger:info,2023-08-29T18:14:49.856Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.257.0>},
              {id,tombstone_keeper},
              {mfargs,{tombstone_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.856Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.258.0>},
              {id,ns_config_events},
              {mfargs,{gen_event,start_link,[{local,ns_config_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:49.857Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.259.0>},
              {id,ns_config_events_local},
              {mfargs,{gen_event,start_link,[{local,ns_config_events_local}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:50.003Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1108]Loading static config from "/opt/couchbase/etc/couchbase/config"
[ns_server:info,2023-08-29T18:14:50.006Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1122]Loading dynamic config from "/opt/couchbase/var/lib/couchbase/config/config.dat"
[ns_server:info,2023-08-29T18:14:50.007Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1127]No dynamic config file found. Assuming we're brand new node
[ns_server:debug,2023-08-29T18:14:50.018Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1130]Here's full dynamic config we loaded:
[[]]
[ns_server:info,2023-08-29T18:14:50.026Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:load_config:1152]Here's full dynamic config we loaded + static & default config:
[{{node,'ns_1@cb.local',index_dir},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {{node,'ns_1@cb.local',database_dir},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
   98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]},
 {auto_failover_cfg,
  [{enabled,true},
   {timeout,120},
   {count,0},
   {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
   {failover_server_group,false},
   {max_count,1},
   {failed_over_server_groups,[]},
   {can_abort_rebalance,true}]},
 {retry_rebalance,[{enabled,false},{after_time_period,300},{max_attempts,1}]},
 {{node,'ns_1@cb.local',{project_intact,is_vulnerable}},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   false]},
 {{node,'ns_1@cb.local',backup_grpc_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9124]},
 {{node,'ns_1@cb.local',backup_https_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   18097]},
 {{node,'ns_1@cb.local',backup_http_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   8097]},
 {{node,'ns_1@cb.local',prometheus_http_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9123]},
 {{node,'ns_1@cb.local',cbas_debug_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|-1]},
 {{node,'ns_1@cb.local',cbas_parent_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9122]},
 {{node,'ns_1@cb.local',cbas_metadata_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9121]},
 {{node,'ns_1@cb.local',cbas_replication_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9120]},
 {{node,'ns_1@cb.local',cbas_metadata_callback_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9119]},
 {{node,'ns_1@cb.local',cbas_messaging_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9118]},
 {{node,'ns_1@cb.local',cbas_result_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9117]},
 {{node,'ns_1@cb.local',cbas_data_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9116]},
 {{node,'ns_1@cb.local',cbas_cluster_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9115]},
 {{node,'ns_1@cb.local',cbas_console_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9114]},
 {{node,'ns_1@cb.local',cbas_cc_client_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9113]},
 {{node,'ns_1@cb.local',cbas_cc_cluster_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9112]},
 {{node,'ns_1@cb.local',cbas_cc_http_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9111]},
 {{node,'ns_1@cb.local',cbas_admin_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9110]},
 {{node,'ns_1@cb.local',cbas_ssl_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   18095]},
 {{node,'ns_1@cb.local',cbas_http_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   8095]},
 {{node,'ns_1@cb.local',eventing_https_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   18096]},
 {{node,'ns_1@cb.local',eventing_debug_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9140]},
 {{node,'ns_1@cb.local',eventing_http_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   8096]},
 {{node,'ns_1@cb.local',fts_grpc_ssl_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   19130]},
 {{node,'ns_1@cb.local',fts_grpc_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9130]},
 {{node,'ns_1@cb.local',fts_ssl_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   18094]},
 {{node,'ns_1@cb.local',fts_http_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   8094]},
 {{node,'ns_1@cb.local',indexer_https_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   19102]},
 {{node,'ns_1@cb.local',indexer_stmaint_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9105]},
 {{node,'ns_1@cb.local',indexer_stcatchup_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9104]},
 {{node,'ns_1@cb.local',indexer_stinit_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9103]},
 {{node,'ns_1@cb.local',indexer_http_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9102]},
 {{node,'ns_1@cb.local',indexer_scan_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9101]},
 {{node,'ns_1@cb.local',indexer_admin_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9100]},
 {{node,'ns_1@cb.local',ssl_query_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   18093]},
 {{node,'ns_1@cb.local',query_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   8093]},
 {{node,'ns_1@cb.local',projector_ssl_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9999]},
 {{node,'ns_1@cb.local',projector_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9999]},
 {{node,'ns_1@cb.local',ssl_capi_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   18092]},
 {{node,'ns_1@cb.local',capi_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   8092]},
 {{node,'ns_1@cb.local',memcached_prometheus},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   11280]},
 {{node,'ns_1@cb.local',memcached_dedicated_ssl_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   11206]},
 {{node,'ns_1@cb.local',xdcr_rest_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   9998]},
 {{node,'ns_1@cb.local',ssl_rest_port},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   18091]},
 {{node,'ns_1@cb.local',rest},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   {port,8091},
   {port_meta,global}]},
 {rest,[{port,8091}]},
 {password_policy,[{min_length,6},{must_present,[]}]},
 {auto_reprovision_cfg,[{enabled,true},{max_nodes,1},{count,0}]},
 {log_redaction_default_cfg,[{redact_level,none}]},
 {replication,[{enabled,true}]},
 {alert_limits,
  [{max_overhead_perc,50},{max_disk_used,90},{max_indexer_ram,75}]},
 {email_alerts,
  [{recipients,["root@localhost"]},
   {sender,"couchbase@localhost"},
   {enabled,false},
   {email_server,
    [{user,[]},{pass,"*****"},{host,"localhost"},{port,25},{encrypt,false}]},
   {alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     history_size_warning]},
   {pop_up_alerts,
    [auto_failover_node,auto_failover_maximum_reached,
     auto_failover_other_nodes_down,auto_failover_cluster_too_small,
     auto_failover_disabled,ip,disk,overhead,ep_oom_errors,
     ep_item_commit_failed,audit_dropped_events,indexer_ram_max_usage,
     indexer_low_resident_percentage,ep_clock_cas_drift_threshold_exceeded,
     communication_issue,time_out_of_sync,disk_usage_analyzer_stuck,
     history_size_warning]}]},
 {{node,'ns_1@cb.local',event_log},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]},
 {{node,'ns_1@cb.local',ns_log},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]},
 {{node,'ns_1@cb.local',port_servers},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}]},
 {secure_headers,[]},
 {buckets,[{configs,[]}]},
 {cbas_memory_quota,1024},
 {fts_memory_quota,256},
 {memory_quota,999},
 {{node,'ns_1@cb.local',memcached_config},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
     {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
     {connection_idle_time,connection_idle_time},
     {privilege_debug,privilege_debug},
     {breakpad,
      {[{enabled,breakpad_enabled},
        {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
     {admin,{"~s",[admin_user]}},
     {verbosity,verbosity},
     {audit_file,{"~s",[audit_file]}},
     {rbac_file,{"~s",[rbac_file]}},
     {dedupe_nmvb_maps,dedupe_nmvb_maps},
     {tracing_enabled,tracing_enabled},
     {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
     {xattr_enabled,true},
     {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
     {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
     {enforce_tenant_limits_enabled,
      {memcached_config_mgr,should_enforce_limits,[]}},
     {max_connections,max_connections},
     {system_connections,system_connections},
     {num_reader_threads,num_reader_threads},
     {num_writer_threads,num_writer_threads},
     {num_auxio_threads,num_auxio_threads},
     {num_nonio_threads,num_nonio_threads},
     {num_storage_threads,num_storage_threads},
     {logger,
      {[{filename,{"~s/~s",[log_path,log_prefix]}},
        {cyclesize,log_cyclesize}]}},
     {external_auth_service,
      {memcached_config_mgr,get_external_auth_service,[]}},
     {active_external_users_push_interval,
      {memcached_config_mgr,get_external_users_push_interval,[]}},
     {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
     {connection_limit_mode,connection_limit_mode},
     {free_connection_pool_size,free_connection_pool_size},
     {tcp_keepalive_idle,tcp_keepalive_idle},
     {tcp_keepalive_interval,tcp_keepalive_interval},
     {tcp_keepalive_probes,tcp_keepalive_probes},
     {max_client_connection_details,max_client_connection_details}]}]},
 {{node,'ns_1@cb.local',memcached},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   {port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,
    ["@cbq-engine","@projector","@goxdcr","@index","@fts","@eventing","@cbas",
     "@backup"]},
   {admin_pass,"*****"},
   {engines,
    [{membase,
      [{engine,"/opt/couchbase/lib/memcached/ep.so"},
       {static_config_string,"failpartialwarmup=false"}]},
     {memcached,
      [{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
       {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]},
 {{node,'ns_1@cb.local',memcached_defaults},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   {max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {max_client_connection_details,0}]},
 {memcached,[]},
 {{node,'ns_1@cb.local',audit},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}]},
 {audit,
  [{auditd_enabled,false},
   {rotate_interval,86400},
   {rotate_size,20971520},
   {disabled,[]},
   {enabled,[]},
   {disabled_users,[]},
   {sync,[]},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"}]},
 {{node,'ns_1@cb.local',isasl},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]},
 {remote_clusters,[]},
 {scramsha_fallback_salt,<<"LÂút?ê;kg+¬V">>},
 {client_cert_auth,[{state,"disable"},{prefixes,[]}]},
 {rest_creds,null},
 {{metakv,<<"/analytics/settings/config">>},
  <<"{\"analytics.settings.num_replicas\":0}">>},
 {{metakv,<<"/query/settings/config">>},
  <<"{\"timeout\":0,\"n1ql-feat-ctrl\":12,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"max-parallelism\":1,\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"prepared-limit\":16384,\"pipeline-batch\":16,\"pipeline-cap\":512,\"scan-cap\":512,\"loglevel\":\"info\",\"completed-threshold\":1000,\"query.settings.tmp_space_size\":5120}">>},
 {{metakv,<<"/eventing/settings/config">>},<<"{\"ram_quota\":256}">>},
 {{metakv,<<"/indexing/settings/config">>},
  <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.num_replica\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {{couchdb,max_parallel_replica_indexers},2},
 {{couchdb,max_parallel_indexers},4},
 {{node,'ns_1@cb.local',membership},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   active]},
 {server_groups,
  [[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]},
 {quorum_nodes,['ns_1@cb.local']},
 {nodes_wanted,['ns_1@cb.local']},
 {{node,'ns_1@cb.local',compaction_daemon},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
   {check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]},
 {set_view_update_daemon,
  [{update_interval,5000},
   {update_min_changes,5000},
   {replica_update_min_changes,5000}]},
 {autocompaction,
  [{database_fragmentation_threshold,{30,undefined}},
   {view_fragmentation_threshold,{30,undefined}}]},
 {max_bucket_count,30},
 {index_aware_rebalance_disabled,false},
 {{node,'ns_1@cb.local',saslauthd_enabled},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   true]},
 {{node,'ns_1@cb.local',is_enterprise},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   true]},
 {{node,'ns_1@cb.local',config_version},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   {7,2}]},
 {{node,'ns_1@cb.local',uuid},
  [{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
   <<"a2acfe035c01f55c0d78fce935ad3d90">>]}]
[error_logger:info,2023-08-29T18:14:50.032Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.260.0>},
              {id,ns_config},
              {mfargs,{ns_config,start_link,
                                 ["/opt/couchbase/etc/couchbase/config",
                                  ns_config_default]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:50.036Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.263.0>},
              {id,ns_config_remote},
              {mfargs,{ns_config_replica,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:50.036Z,ns_1@cb.local:ns_config_sup<0.256.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_sup}
    started: [{pid,<0.264.0>},
              {id,ns_config_log},
              {mfargs,{ns_config_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:50.037Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.256.0>},
              {id,ns_config_sup},
              {mfargs,{ns_config_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:50.042Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',erl_external_listeners} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]},
 {inet,false}]
[error_logger:info,2023-08-29T18:14:50.042Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.266.0>},
              {id,netconfig_updater},
              {mfargs,{netconfig_updater,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:50.042Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',node_encryption} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|false]
[ns_server:debug,2023-08-29T18:14:50.042Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',address_family} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|inet]
[ns_server:debug,2023-08-29T18:14:50.043Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}]
[error_logger:info,2023-08-29T18:14:50.052Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.269.0>},
              {id,json_rpc_connection_sup},
              {mfargs,{json_rpc_connection_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:50.073Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.272.0>},
              {name,chronicle_compat_events},
              {mfargs,{chronicle_compat_events,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:50.078Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.277.0>},
              {name,remote_monitors},
              {mfargs,{remote_monitors,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:50.083Z,ns_1@cb.local:menelaus_barrier<0.278.0>:one_shot_barrier:barrier_body:52]Barrier menelaus_barrier has started
[error_logger:info,2023-08-29T18:14:50.083Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.278.0>},
              {name,menelaus_barrier},
              {mfargs,{menelaus_sup,barrier_start_link,[]}},
              {restart_type,temporary},
              {shutdown,1000},
              {child_type,worker}]
[chronicle:debug,2023-08-29T18:14:50.087Z,ns_1@cb.local:chronicle_leader<0.239.0>:chronicle_leader:handle_state_timeout:608]State timeout when state is: {observer,true,false}
[chronicle:info,2023-08-29T18:14:50.088Z,ns_1@cb.local:<0.280.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"918babd26e22549c990a18e4926f0ef9">>
Log position: {{1,'ns_1@cb.local'},1}
Peers: ['ns_1@cb.local']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],[],[],[],[],[],[],[],[],[],[],[],
                                  ['ns_1@cb.local'],
                                  [],[],[]}}}}
[chronicle:info,2023-08-29T18:14:50.088Z,ns_1@cb.local:<0.280.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2023-08-29T18:14:50.088Z,ns_1@cb.local:chronicle_leader<0.239.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {2,'ns_1@cb.local'} (history id <<"918babd26e22549c990a18e4926f0ef9">>)
[error_logger:info,2023-08-29T18:14:50.088Z,ns_1@cb.local:rest_lhttpc_pool_sup<0.279.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,rest_lhttpc_pool_sup}
    started: [{pid,<0.281.0>},
              {name,rest_lhttpc_pool},
              {mfargs,{lhttpc_manager,start_link,
                                      [[{name,rest_lhttpc_pool},
                                        {connection_timeout,120000},
                                        {pool_size,20}]]}},
              {restart_type,{permanent,1}},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:50.089Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.279.0>},
              {name,rest_lhttpc_pool_sup},
              {mfargs,{rest_lhttpc_pool_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[chronicle:debug,2023-08-29T18:14:50.105Z,ns_1@cb.local:chronicle_agent<0.235.0>:chronicle_agent:handle_establish_term:1529]Accepted term {2,'ns_1@cb.local'} in history <<"918babd26e22549c990a18e4926f0ef9">>
[chronicle:debug,2023-08-29T18:14:50.106Z,ns_1@cb.local:chronicle_proposer<0.282.0>:chronicle_proposer:establish_term_init:367]Going to establish term {2,'ns_1@cb.local'} (history id <<"918babd26e22549c990a18e4926f0ef9">>).
Quorum peers: ['ns_1@cb.local']
Metadata:
{metadata,'ns_1@cb.local',<<"0957cbe35228f96d2df30f13ea1a5217">>,
          <<"918babd26e22549c990a18e4926f0ef9">>,
          {1,'ns_1@cb.local'},
          {1,'ns_1@cb.local'},
          1,1,
          {log_entry,<<"918babd26e22549c990a18e4926f0ef9">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"0957cbe35228f96d2df30f13ea1a5217">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"918babd26e22549c990a18e4926f0ef9">>,0}]}},
          {log_entry,<<"918babd26e22549c990a18e4926f0ef9">>,
                     {1,'ns_1@cb.local'},
                     1,
                     {config,undefined,0,undefined,
                             #{'ns_1@cb.local' =>
                                   #{id =>
                                         <<"0957cbe35228f96d2df30f13ea1a5217">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"918babd26e22549c990a18e4926f0ef9">>,0}]}},
          undefined}
[chronicle:debug,2023-08-29T18:14:50.106Z,ns_1@cb.local:chronicle_proposer<0.282.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {2,'ns_1@cb.local'} (history id <<"918babd26e22549c990a18e4926f0ef9">>) successfully.
Votes: ['ns_1@cb.local']
[chronicle:debug,2023-08-29T18:14:50.106Z,ns_1@cb.local:chronicle_proposer<0.282.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {2,'ns_1@cb.local'} in history <<"918babd26e22549c990a18e4926f0ef9">>
[chronicle:debug,2023-08-29T18:14:50.120Z,ns_1@cb.local:chronicle_proposer<0.282.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {2,'ns_1@cb.local'} in history <<"918babd26e22549c990a18e4926f0ef9">> is ready. Committed seqno: 2
[chronicle:info,2023-08-29T18:14:50.120Z,ns_1@cb.local:chronicle_leader<0.239.0>:chronicle_leader:handle_note_term_status:596]Term {2,'ns_1@cb.local'} established.
[error_logger:info,2023-08-29T18:14:50.130Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.283.0>},
              {name,memcached_refresh},
              {mfargs,{memcached_refresh,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:50.134Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.285.0>},
              {name,ns_secrets},
              {mfargs,{ns_secrets,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:50.138Z,ns_1@cb.local:ns_ssl_services_sup<0.286.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.287.0>},
              {id,ssl_service_events},
              {mfargs,{gen_event,start_link,[{local,ssl_service_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:50.155Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:maybe_store_ca_certs:726]Considering to store CA certs
[ns_server:debug,2023-08-29T18:14:50.312Z,ns_1@cb.local:<0.292.0>:goport:handle_eof:585]Stream 'stderr' closed
[ns_server:debug,2023-08-29T18:14:50.312Z,ns_1@cb.local:<0.292.0>:goport:handle_eof:585]Stream 'stdout' closed
[ns_server:info,2023-08-29T18:14:50.312Z,ns_1@cb.local:<0.292.0>:goport:handle_process_exit:566]Port exited with status 0.
[ns_server:debug,2023-08-29T18:14:50.335Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_server_cert:generate_cert_and_pkey:182]Generated certificate and private key in 173023 us
[ns_server:debug,2023-08-29T18:14:50.336Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cert_and_pkey ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552090}}]}|
 {<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIF3/tSsVH0d4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYjUxZWI3\nNzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCr9XJDH3HmSp0Vabu1\nQm1+A0/9t3DlYJP9BDjf5CmXOlsWpvUIAP24RQ9rQsu3gn8VvYYnkK2mFfF0SRo5\nxJX/EZde2ejr9piUfJTuY18OYQthMm3"...>>,
  {sanitized,<<"/9xYQ6y9NlsyhTTTVQMcWN2/Dx//LilvW4FIwzwYl4Y=">>}}]
[ns_server:debug,2023-08-29T18:14:50.336Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552090}}]}]
[ns_server:debug,2023-08-29T18:14:50.467Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:maybe_store_ca_certs:741]Updating CA file with 1 certificates
[ns_server:info,2023-08-29T18:14:50.559Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:maybe_store_ca_certs:744]CA file updated: 1 cert(s) written
[ns_server:info,2023-08-29T18:14:50.563Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:maybe_generate_node_certs:791]Regenerating certs because there are no certs on this node
[ns_server:debug,2023-08-29T18:14:51.003Z,ns_1@cb.local:<0.298.0>:goport:handle_eof:585]Stream 'stderr' closed
[ns_server:debug,2023-08-29T18:14:51.003Z,ns_1@cb.local:<0.298.0>:goport:handle_eof:585]Stream 'stdout' closed
[ns_server:info,2023-08-29T18:14:51.003Z,ns_1@cb.local:<0.298.0>:goport:handle_process_exit:566]Port exited with status 0.
[ns_server:info,2023-08-29T18:14:51.058Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:save_node_certs:851]New node cert and pkey are written to tmp file
[ns_server:info,2023-08-29T18:14:51.191Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:save_node_certs_phase2:874]Node cert and pkey files updated
[ns_server:debug,2023-08-29T18:14:51.191Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{3,63860552091}}]}]
[ns_server:debug,2023-08-29T18:14:51.192Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',node_cert} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552091}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,64691827199},
 {verified_with,<<133,166,53,2,8,48,187,102,19,107,176,9,230,211,49,117>>},
 {type,generated},
 {load_timestamp,63860552090},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIF3/tSsVH0d4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYjUxZWI3\nNzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCr9XJDH3HmSp0Vabu1\nQm1+A0/9t3DlYJP9BDjf5CmXOlsWpvUIAP24RQ9rQsu3gn8VvYYnkK2mFfF0SRo5\nxJX/EZd"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIF3/tSu5x35owDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAK7J80br\nemoHet3x/9SjInqRaDHbxjoA13fYRB6cqVMFucsd+STGbshOlFzbP1rVfQD7I/tr\nytn"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {hostname,"127.0.0.1"}]
[ns_server:info,2023-08-29T18:14:51.286Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:handle_info:656]cert_and_pkey changed
[error_logger:info,2023-08-29T18:14:51.286Z,ns_1@cb.local:ns_ssl_services_sup<0.286.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.288.0>},
              {id,ns_ssl_services_setup},
              {mfargs,{ns_ssl_services_setup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:51.315Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:maybe_store_ca_certs:726]Considering to store CA certs
[ns_server:info,2023-08-29T18:14:51.337Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:validate_pkey:910]Private key passphrase validation suceeded
[ns_server:debug,2023-08-29T18:14:51.337Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1017]Going to notify following services: [cb_dist_tls,capi_ssl_service,memcached,
                                     event]
[ns_server:debug,2023-08-29T18:14:51.338Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Restarting tls distribution protocols (if any)
[ns_server:info,2023-08-29T18:14:51.338Z,ns_1@cb.local:<0.313.0>:ns_ssl_services_setup:notify_service:1053]Successfully notified service event
[ns_server:info,2023-08-29T18:14:51.345Z,ns_1@cb.local:<0.310.0>:ns_ssl_services_setup:notify_service:1053]Successfully notified service cb_dist_tls
[ns_server:info,2023-08-29T18:14:51.348Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2023-08-29T18:14:51.349Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2023-08-29T18:14:51.349Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2023-08-29T18:14:51.350Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2023-08-29T18:14:51.350Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:warn,2023-08-29T18:14:51.368Z,ns_1@cb.local:<0.312.0>:ns_ssl_services_setup:notify_service:1055]Failed to notify service memcached: {error,no_proccess}
[error_logger:info,2023-08-29T18:14:51.376Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {1,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:51.377Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:51.377Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792642.218429>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:51.377Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792642.218429>,
                                  inet_tcp_dist,<0.314.0>,
                                  #Ref<0.4274179209.2407792643.217905>}
[ns_server:debug,2023-08-29T18:14:51.378Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792642.218429>,
                               inet_tcp_dist,<0.314.0>,
                               #Ref<0.4274179209.2407792643.217905>}
[error_logger:info,2023-08-29T18:14:51.378Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.314.0>,shutdown}}
[error_logger:info,2023-08-29T18:14:51.378Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:51.378Z,ns_1@cb.local:<0.311.0>:ns_couchdb_api:rpc_couchdb_node:152]RPC to couchdb node failed for restart_capi_ssl_service with {badrpc,nodedown}
Stack: [{ns_couchdb_api,rpc_couchdb_node,4,
                        [{file,"src/ns_couchdb_api.erl"},{line,150}]},
        {ns_ssl_services_setup,do_notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1074}]},
        {ns_ssl_services_setup,notify_service,1,
                               [{file,"src/ns_ssl_services_setup.erl"},
                                {line,1050}]},
        {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,191}]}]
[ns_server:warn,2023-08-29T18:14:51.379Z,ns_1@cb.local:<0.311.0>:ns_ssl_services_setup:notify_service:1055]Failed to notify service capi_ssl_service: {'EXIT',{error,{badrpc,nodedown}}}
[ns_server:info,2023-08-29T18:14:51.379Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1033]Succesfully notified services [event,cb_dist_tls]
[ns_server:info,2023-08-29T18:14:51.421Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1044]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached},
                                                      {{'EXIT',
                                                        {error,
                                                         {badrpc,nodedown}}},
                                                       capi_ssl_service}]
[ns_server:info,2023-08-29T18:14:51.442Z,ns_1@cb.local:<0.303.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"0.0.0.0"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,33,48,130,2,9,160,3,2,1,2,2,8,23,127,237,74,197,71,
              209,222,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,98,53,49,101,98,55,55,53,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,98,53,49,
              101,98,55,55,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,171,245,114,67,31,
              113,230,74,157,21,105,187,181,66,109,126,3,79,253,183,112,229,
              96,147,253,4,56,223,228,41,151,58,91,22,166,245,8,0,253,184,69,
              15,107,66,203,183,130,127,21,189,134,39,144,173,166,21,241,116,
              73,26,57,196,149,255,17,151,94,217,232,235,246,152,148,124,148,
              238,99,95,14,97,11,97,50,109,199,41,243,178,61,63,196,37,11,34,
              64,94,203,164,121,126,29,218,32,170,209,182,145,144,210,67,27,
              112,105,249,43,240,88,132,110,213,63,93,230,209,213,36,244,67,
              5,182,16,60,40,27,133,98,45,141,140,54,210,60,163,98,20,198,
              143,64,206,167,216,47,187,53,213,245,20,13,90,162,152,161,128,
              205,97,200,183,80,17,184,159,250,143,53,229,77,228,207,47,207,
              233,125,181,129,67,88,161,115,141,141,176,83,245,171,119,171,
              168,78,88,17,145,67,200,195,226,165,185,139,123,128,231,188,
              246,253,93,116,79,128,184,36,131,206,129,42,95,204,0,189,89,
              229,28,119,198,108,164,103,107,193,101,250,184,39,30,49,229,22,
              128,165,243,238,63,7,97,142,28,175,2,3,1,0,1,163,87,48,85,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
              48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
              1,1,255,48,29,6,3,85,29,14,4,22,4,20,111,202,71,227,87,55,113,
              238,204,17,176,12,42,199,48,82,198,149,87,29,48,13,6,9,42,134,
              72,134,247,13,1,1,11,5,0,3,130,1,1,0,77,126,5,124,137,134,208,
              43,108,161,213,215,119,99,235,6,38,112,123,143,145,223,170,102,
              212,32,99,174,111,228,197,165,138,35,194,67,231,140,201,158,
              180,233,67,137,220,234,127,137,10,12,71,230,148,32,85,94,5,158,
              41,246,28,23,99,61,77,38,102,253,51,106,53,240,184,58,248,206,
              255,99,78,172,53,133,13,184,70,247,32,177,195,142,129,167,30,
              28,245,191,201,126,12,115,148,162,113,124,215,197,87,56,108,23,
              90,185,167,185,69,77,138,81,137,39,243,232,42,27,31,85,142,254,
              193,214,138,228,136,36,97,63,22,14,227,255,194,165,48,203,183,
              102,233,51,56,207,184,8,105,193,37,32,191,77,91,232,70,52,204,
              178,32,176,126,159,182,44,197,95,164,165,72,238,186,244,219,87,
              32,198,74,113,13,203,71,67,133,164,211,194,73,216,130,88,179,
              108,48,164,6,208,181,29,246,70,185,158,228,4,83,210,119,191,
              241,140,122,193,197,37,208,3,149,94,181,103,64,75,129,16,161,
              184,239,135,71,114,118,65,73,38,195,14,100,237,74,134,142,24,
              135,232,32,241,73,74,221,125>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[{any,aes_256_gcm,aead,sha384},
            {any,aes_128_gcm,aead,sha256},
            {any,chacha20_poly1305,aead,sha256},
            {any,aes_128_ccm,aead,sha256},
            {any,aes_128_ccm_8,aead,sha256},
            {ecdhe_ecdsa,aes_256_gcm,aead,sha384},
            {ecdhe_rsa,aes_256_gcm,aead,sha384},
            {ecdhe_ecdsa,aes_256_ccm,aead},
            {ecdhe_ecdsa,aes_256_ccm_8,aead},
            {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
            {ecdhe_rsa,chacha20_poly1305,aead,sha256},
            {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
            {ecdhe_rsa,aes_128_gcm,aead,sha256},
            {ecdhe_ecdsa,aes_128_ccm,aead},
            {ecdhe_ecdsa,aes_128_ccm_8,aead},
            {ecdh_ecdsa,aes_256_gcm,aead,sha384},
            {ecdh_rsa,aes_256_gcm,aead,sha384},
            {ecdh_ecdsa,aes_128_gcm,aead,sha256},
            {ecdh_rsa,aes_128_gcm,aead,sha256},
            {dhe_rsa,aes_256_gcm,aead,sha384},
            {dhe_dss,aes_256_gcm,aead,sha384},
            {dhe_rsa,aes_128_gcm,aead,sha256},
            {dhe_dss,aes_128_gcm,aead,sha256},
            {dhe_rsa,chacha20_poly1305,aead,sha256},
            {rsa_psk,aes_256_gcm,aead,sha384},
            {rsa_psk,aes_128_gcm,aead,sha256},
            {rsa,aes_256_gcm,aead,sha384},
            {rsa,aes_128_gcm,aead,sha256},
            {rsa,aes_256_cbc,sha},
            {rsa,aes_128_cbc,sha},
            {rsa,'3des_ede_cbc',sha}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2023-08-29T18:14:51.445Z,ns_1@cb.local:<0.303.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.303.0>,menelaus_web}
    started: [{pid,<0.317.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.8.31746748>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:51.454Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2023-08-29T18:14:51.454Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2023-08-29T18:14:51.455Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2023-08-29T18:14:51.455Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2023-08-29T18:14:51.455Z,ns_1@cb.local:<0.303.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2023-08-29T18:14:51.460Z,ns_1@cb.local:<0.303.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"::"},
 [{keyfile,"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem"},
  {certfile,"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem"},
  {versions,['tlsv1.2','tlsv1.3']},
  {cacerts,[<<48,130,3,33,48,130,2,9,160,3,2,1,2,2,8,23,127,237,74,197,71,
              209,222,48,13,6,9,42,134,72,134,247,13,1,1,11,5,0,48,36,49,34,
              48,32,6,3,85,4,3,19,25,67,111,117,99,104,98,97,115,101,32,83,
              101,114,118,101,114,32,98,53,49,101,98,55,55,53,48,30,23,13,49,
              51,48,49,48,49,48,48,48,48,48,48,90,23,13,52,57,49,50,51,49,50,
              51,53,57,53,57,90,48,36,49,34,48,32,6,3,85,4,3,19,25,67,111,
              117,99,104,98,97,115,101,32,83,101,114,118,101,114,32,98,53,49,
              101,98,55,55,53,48,130,1,34,48,13,6,9,42,134,72,134,247,13,1,1,
              1,5,0,3,130,1,15,0,48,130,1,10,2,130,1,1,0,171,245,114,67,31,
              113,230,74,157,21,105,187,181,66,109,126,3,79,253,183,112,229,
              96,147,253,4,56,223,228,41,151,58,91,22,166,245,8,0,253,184,69,
              15,107,66,203,183,130,127,21,189,134,39,144,173,166,21,241,116,
              73,26,57,196,149,255,17,151,94,217,232,235,246,152,148,124,148,
              238,99,95,14,97,11,97,50,109,199,41,243,178,61,63,196,37,11,34,
              64,94,203,164,121,126,29,218,32,170,209,182,145,144,210,67,27,
              112,105,249,43,240,88,132,110,213,63,93,230,209,213,36,244,67,
              5,182,16,60,40,27,133,98,45,141,140,54,210,60,163,98,20,198,
              143,64,206,167,216,47,187,53,213,245,20,13,90,162,152,161,128,
              205,97,200,183,80,17,184,159,250,143,53,229,77,228,207,47,207,
              233,125,181,129,67,88,161,115,141,141,176,83,245,171,119,171,
              168,78,88,17,145,67,200,195,226,165,185,139,123,128,231,188,
              246,253,93,116,79,128,184,36,131,206,129,42,95,204,0,189,89,
              229,28,119,198,108,164,103,107,193,101,250,184,39,30,49,229,22,
              128,165,243,238,63,7,97,142,28,175,2,3,1,0,1,163,87,48,85,48,
              14,6,3,85,29,15,1,1,255,4,4,3,2,2,164,48,19,6,3,85,29,37,4,12,
              48,10,6,8,43,6,1,5,5,7,3,1,48,15,6,3,85,29,19,1,1,255,4,5,48,3,
              1,1,255,48,29,6,3,85,29,14,4,22,4,20,111,202,71,227,87,55,113,
              238,204,17,176,12,42,199,48,82,198,149,87,29,48,13,6,9,42,134,
              72,134,247,13,1,1,11,5,0,3,130,1,1,0,77,126,5,124,137,134,208,
              43,108,161,213,215,119,99,235,6,38,112,123,143,145,223,170,102,
              212,32,99,174,111,228,197,165,138,35,194,67,231,140,201,158,
              180,233,67,137,220,234,127,137,10,12,71,230,148,32,85,94,5,158,
              41,246,28,23,99,61,77,38,102,253,51,106,53,240,184,58,248,206,
              255,99,78,172,53,133,13,184,70,247,32,177,195,142,129,167,30,
              28,245,191,201,126,12,115,148,162,113,124,215,197,87,56,108,23,
              90,185,167,185,69,77,138,81,137,39,243,232,42,27,31,85,142,254,
              193,214,138,228,136,36,97,63,22,14,227,255,194,165,48,203,183,
              102,233,51,56,207,184,8,105,193,37,32,191,77,91,232,70,52,204,
              178,32,176,126,159,182,44,197,95,164,165,72,238,186,244,219,87,
              32,198,74,113,13,203,71,67,133,164,211,194,73,216,130,88,179,
              108,48,164,6,208,181,29,246,70,185,158,228,4,83,210,119,191,
              241,140,122,193,197,37,208,3,149,94,181,103,64,75,129,16,161,
              184,239,135,71,114,118,65,73,38,195,14,100,237,74,134,142,24,
              135,232,32,241,73,74,221,125>>]},
  {dh,<<48,130,1,8,2,130,1,1,0,152,202,99,248,92,201,35,238,246,5,77,93,120,
        10,118,129,36,52,111,193,167,220,49,229,106,105,152,133,121,157,73,
        158,232,153,197,197,21,171,140,30,207,52,165,45,8,221,162,21,199,183,
        66,211,247,51,224,102,214,190,130,96,253,218,193,35,43,139,145,89,200,
        250,145,92,50,80,134,135,188,205,254,148,122,136,237,220,186,147,187,
        104,159,36,147,217,117,74,35,163,145,249,175,242,18,221,124,54,140,16,
        246,169,84,252,45,47,99,136,30,60,189,203,61,86,225,117,255,4,91,46,
        110,167,173,106,51,65,10,248,94,225,223,73,40,232,140,26,11,67,170,
        118,190,67,31,127,233,39,68,88,132,171,224,62,187,207,160,189,209,101,
        74,8,205,174,146,173,80,105,144,246,25,153,86,36,24,178,163,64,202,
        221,95,184,110,244,32,226,217,34,55,188,230,55,16,216,247,173,246,139,
        76,187,66,211,159,17,46,20,18,48,80,27,250,96,189,29,214,234,241,34,
        69,254,147,103,220,133,40,164,84,8,44,241,61,164,151,9,135,41,60,75,4,
        202,133,173,72,6,69,167,89,112,174,40,229,171,2,1,2>>},
  {ciphers,[{any,aes_256_gcm,aead,sha384},
            {any,aes_128_gcm,aead,sha256},
            {any,chacha20_poly1305,aead,sha256},
            {any,aes_128_ccm,aead,sha256},
            {any,aes_128_ccm_8,aead,sha256},
            {ecdhe_ecdsa,aes_256_gcm,aead,sha384},
            {ecdhe_rsa,aes_256_gcm,aead,sha384},
            {ecdhe_ecdsa,aes_256_ccm,aead},
            {ecdhe_ecdsa,aes_256_ccm_8,aead},
            {ecdhe_ecdsa,chacha20_poly1305,aead,sha256},
            {ecdhe_rsa,chacha20_poly1305,aead,sha256},
            {ecdhe_ecdsa,aes_128_gcm,aead,sha256},
            {ecdhe_rsa,aes_128_gcm,aead,sha256},
            {ecdhe_ecdsa,aes_128_ccm,aead},
            {ecdhe_ecdsa,aes_128_ccm_8,aead},
            {ecdh_ecdsa,aes_256_gcm,aead,sha384},
            {ecdh_rsa,aes_256_gcm,aead,sha384},
            {ecdh_ecdsa,aes_128_gcm,aead,sha256},
            {ecdh_rsa,aes_128_gcm,aead,sha256},
            {dhe_rsa,aes_256_gcm,aead,sha384},
            {dhe_dss,aes_256_gcm,aead,sha384},
            {dhe_rsa,aes_128_gcm,aead,sha256},
            {dhe_dss,aes_128_gcm,aead,sha256},
            {dhe_rsa,chacha20_poly1305,aead,sha256},
            {rsa_psk,aes_256_gcm,aead,sha384},
            {rsa_psk,aes_128_gcm,aead,sha256},
            {rsa,aes_256_gcm,aead,sha384},
            {rsa,aes_128_gcm,aead,sha256},
            {rsa,aes_256_cbc,sha},
            {rsa,aes_128_cbc,sha},
            {rsa,'3des_ede_cbc',sha}]},
  {honor_cipher_order,true},
  {secure_renegotiate,true},
  {client_renegotiation,false},
  {password,"********"}],
 {ssl,true},
 {port,18091}]
[error_logger:info,2023-08-29T18:14:51.463Z,ns_1@cb.local:<0.303.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.303.0>,menelaus_web}
    started: [{pid,<0.336.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {ssl,true},
                        {name,menelaus_web_ssl},
                        {ssl_opts_fun,#Fun<ns_ssl_services_setup.8.31746748>},
                        {port,18091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:51.463Z,ns_1@cb.local:<0.302.0>:restartable:start_child:92]Started child process <0.303.0>
  MFA: {ns_ssl_services_setup,start_link_rest_service,[]}
[error_logger:info,2023-08-29T18:14:51.463Z,ns_1@cb.local:ns_ssl_services_sup<0.286.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_ssl_services_sup}
    started: [{pid,<0.302.0>},
              {id,ns_rest_ssl_service},
              {mfargs,
                  {restartable,start_link,
                      [{ns_ssl_services_setup,start_link_rest_service,[]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:51.463Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.286.0>},
              {name,ns_ssl_services_sup},
              {mfargs,{ns_ssl_services_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:51.479Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.355.0>},
              {name,ldap_auth_cache},
              {mfargs,{ldap_auth_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:51.482Z,ns_1@cb.local:users_sup<0.357.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.358.0>},
              {id,user_storage_events},
              {mfargs,{gen_event,start_link,[{local,user_storage_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:51.493Z,ns_1@cb.local:users_storage_sup<0.359.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.360.0>},
              {id,users_replicator},
              {mfargs,{menelaus_users,start_replicator,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:51.496Z,ns_1@cb.local:users_replicator<0.360.0>:replicated_storage:wait_for_startup:46]Start waiting for startup
[ns_server:debug,2023-08-29T18:14:51.503Z,ns_1@cb.local:users_storage<0.361.0>:replicated_storage:announce_startup:60]Announce my startup to <0.360.0>
[ns_server:debug,2023-08-29T18:14:51.503Z,ns_1@cb.local:users_storage<0.361.0>:replicated_dets:open:148]Opening file "/opt/couchbase/var/lib/couchbase/config/users.dets"
[ns_server:debug,2023-08-29T18:14:51.503Z,ns_1@cb.local:users_replicator<0.360.0>:replicated_storage:wait_for_startup:49]Received replicated storage registration from <0.361.0>
[error_logger:info,2023-08-29T18:14:51.504Z,ns_1@cb.local:users_storage_sup<0.359.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_storage_sup}
    started: [{pid,<0.361.0>},
              {id,users_storage},
              {mfargs,{menelaus_users,start_storage,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:51.504Z,ns_1@cb.local:users_sup<0.357.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.359.0>},
              {id,users_storage_sup},
              {mfargs,{users_storage_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:51.516Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:versioned_cache:init:41]Starting versioned cache compiled_roles_cache
[error_logger:info,2023-08-29T18:14:51.516Z,ns_1@cb.local:users_sup<0.357.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.363.0>},
              {id,compiled_roles_cache},
              {mfargs,{menelaus_roles,start_compiled_roles_cache,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:51.522Z,ns_1@cb.local:users_sup<0.357.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,users_sup}
    started: [{pid,<0.366.0>},
              {id,roles_cache},
              {mfargs,{roles_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:51.522Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.357.0>},
              {name,users_sup},
              {mfargs,{users_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:51.524Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.370.0>},
              {id,dets_sup},
              {mfargs,{dets_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:51.525Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.371.0>},
              {id,dets},
              {mfargs,{dets_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:51.538Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{4,63860552091}}]}]
[ns_server:debug,2023-08-29T18:14:51.539Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',cbas_dirs} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552091}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2023-08-29T18:14:51.540Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{5,63860552091}}]}]
[ns_server:debug,2023-08-29T18:14:51.541Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',eventing_dir} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552091}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2023-08-29T18:14:51.591Z,ns_1@cb.local:users_storage<0.361.0>:replicated_dets:select_from_table:281][dets] Starting select with {users_storage,
                                [{{docv2,'_','_','_'},[],['$_']}],
                                100}
[ns_server:debug,2023-08-29T18:14:51.591Z,ns_1@cb.local:users_storage<0.361.0>:replicated_dets:init_after_ack:141]Loading 0 items, 305 words took 88ms
[ns_server:debug,2023-08-29T18:14:51.592Z,ns_1@cb.local:users_storage<0.361.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2023-08-29T18:14:51.592Z,ns_1@cb.local:roles_cache<0.366.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2023-08-29T18:14:51.593Z,ns_1@cb.local:roles_cache<0.366.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-08-29T18:14:51.601Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from undefined to {undefined,
                                                                             {0,
                                                                              1676588058},
                                                                             {0,
                                                                              1676588058},
                                                                             false,
                                                                             []}
[error_logger:info,2023-08-29T18:14:51.606Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.375.0>},
              {name,start_couchdb_node},
              {mfargs,{ns_server_nodes_sup,start_couchdb_node,[]}},
              {restart_type,{permanent,5}},
              {shutdown,86400000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:51.607Z,ns_1@cb.local:wait_link_to_couchdb_node<0.377.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:153]Waiting for ns_couchdb node to start
[error_logger:info,2023-08-29T18:14:51.608Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {2,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:51.609Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:51.609Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792641.218920>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:51.610Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.218920>,
                                  inet_tcp_dist,<0.379.0>,
                                  #Ref<0.4274179209.2407792643.217990>}
[ns_server:debug,2023-08-29T18:14:51.611Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.218920>,
                               inet_tcp_dist,<0.379.0>,
                               #Ref<0.4274179209.2407792643.217990>}
[error_logger:info,2023-08-29T18:14:51.611Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.379.0>,shutdown}}
[error_logger:info,2023-08-29T18:14:51.611Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:51.612Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:51.813Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {3,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:51.813Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:51.813Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792642.218467>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:51.814Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792642.218467>,
                                  inet_tcp_dist,<0.381.0>,
                                  #Ref<0.4274179209.2407792643.217997>}
[ns_server:debug,2023-08-29T18:14:51.814Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792642.218467>,
                               inet_tcp_dist,<0.381.0>,
                               #Ref<0.4274179209.2407792643.217997>}
[error_logger:info,2023-08-29T18:14:51.814Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.381.0>,shutdown}}
[error_logger:info,2023-08-29T18:14:51.814Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:51.815Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:52.016Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {4,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:52.016Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:52.016Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792643.218007>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:52.017Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792643.218007>,
                                  inet_tcp_dist,<0.383.0>,
                                  #Ref<0.4274179209.2407792643.218010>}
[error_logger:info,2023-08-29T18:14:52.017Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.383.0>,shutdown}}
[ns_server:debug,2023-08-29T18:14:52.017Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792643.218007>,
                               inet_tcp_dist,<0.383.0>,
                               #Ref<0.4274179209.2407792643.218010>}
[error_logger:info,2023-08-29T18:14:52.017Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:52.017Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:52.219Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {5,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:52.219Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:52.220Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792643.218021>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:52.220Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792643.218021>,
                                  inet_tcp_dist,<0.385.0>,
                                  #Ref<0.4274179209.2407792643.218024>}
[ns_server:debug,2023-08-29T18:14:52.221Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792643.218021>,
                               inet_tcp_dist,<0.385.0>,
                               #Ref<0.4274179209.2407792643.218024>}
[error_logger:info,2023-08-29T18:14:52.221Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.385.0>,shutdown}}
[error_logger:info,2023-08-29T18:14:52.221Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:52.221Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:52.422Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {6,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:52.422Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:52.422Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792643.218035>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:52.422Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792643.218035>,
                                  inet_tcp_dist,<0.387.0>,
                                  #Ref<0.4274179209.2407792643.218038>}
[ns_server:debug,2023-08-29T18:14:52.490Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792643.218035>,
                               inet_tcp_dist,<0.387.0>,
                               #Ref<0.4274179209.2407792643.218038>}
[error_logger:info,2023-08-29T18:14:52.490Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.387.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2023-08-29T18:14:52.491Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:52.491Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:52.692Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {7,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:52.692Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:52.692Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792643.218045>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:52.692Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792643.218045>,
                                  inet_tcp_dist,<0.389.0>,
                                  #Ref<0.4274179209.2407792643.218048>}
[ns_server:info,2023-08-29T18:14:52.701Z,ns_1@cb.local:ns_couchdb_port<0.375.0>:ns_port_server:log:226]ns_couchdb<0.375.0>: =ERROR REPORT==== 29-Aug-2023::18:14:52.490239 ===
ns_couchdb<0.375.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.375.0>: 

[ns_server:debug,2023-08-29T18:14:52.705Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792643.218045>,
                               inet_tcp_dist,<0.389.0>,
                               #Ref<0.4274179209.2407792643.218048>}
[error_logger:info,2023-08-29T18:14:52.705Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.389.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2023-08-29T18:14:52.705Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:52.705Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[ns_server:info,2023-08-29T18:14:52.906Z,ns_1@cb.local:ns_couchdb_port<0.375.0>:ns_port_server:log:226]ns_couchdb<0.375.0>: =ERROR REPORT==== 29-Aug-2023::18:14:52.704791 ===
ns_couchdb<0.375.0>: ** Connection attempt to/from node 'ns_1@cb.local' rejected. Cookie is not set. **
ns_couchdb<0.375.0>: 

[error_logger:info,2023-08-29T18:14:52.906Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {8,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:52.907Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:52.907Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792643.218057>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:52.907Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792643.218057>,
                                  inet_tcp_dist,<0.391.0>,
                                  #Ref<0.4274179209.2407792643.218060>}
[ns_server:debug,2023-08-29T18:14:52.929Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792643.218057>,
                               inet_tcp_dist,<0.391.0>,
                               #Ref<0.4274179209.2407792643.218060>}
[error_logger:info,2023-08-29T18:14:52.929Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.391.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2023-08-29T18:14:52.930Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:52.930Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:53.131Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {9,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:53.131Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:53.131Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792641.218934>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:53.132Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.218934>,
                                  inet_tcp_dist,<0.393.0>,
                                  #Ref<0.4274179209.2407792641.218937>}
[ns_server:debug,2023-08-29T18:14:53.134Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.218934>,
                               inet_tcp_dist,<0.393.0>,
                               #Ref<0.4274179209.2407792641.218937>}
[error_logger:info,2023-08-29T18:14:53.134Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.393.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2023-08-29T18:14:53.134Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:53.134Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:53.336Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {10,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:53.336Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:53.336Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792641.218947>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:53.336Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.218947>,
                                  inet_tcp_dist,<0.395.0>,
                                  #Ref<0.4274179209.2407792641.218950>}
[ns_server:debug,2023-08-29T18:14:53.352Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.218947>,
                               inet_tcp_dist,<0.395.0>,
                               #Ref<0.4274179209.2407792641.218950>}
[error_logger:info,2023-08-29T18:14:53.352Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.395.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2023-08-29T18:14:53.352Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:53.353Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:53.554Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {11,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:53.554Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:53.555Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792644.218122>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:53.555Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792644.218122>,
                                  inet_tcp_dist,<0.397.0>,
                                  #Ref<0.4274179209.2407792642.218491>}
[error_logger:info,2023-08-29T18:14:53.557Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.397.0>,{recv_challenge_failed,{error,closed}}}}
[ns_server:debug,2023-08-29T18:14:53.557Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792644.218122>,
                               inet_tcp_dist,<0.397.0>,
                               #Ref<0.4274179209.2407792642.218491>}
[error_logger:info,2023-08-29T18:14:53.557Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:53.557Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:53.759Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {12,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:53.760Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:53.760Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792644.218135>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:53.760Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792644.218135>,
                                  inet_tcp_dist,<0.399.0>,
                                  #Ref<0.4274179209.2407792644.218138>}
[ns_server:debug,2023-08-29T18:14:53.762Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792644.218135>,
                               inet_tcp_dist,<0.399.0>,
                               #Ref<0.4274179209.2407792644.218138>}
[error_logger:info,2023-08-29T18:14:53.762Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.399.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2023-08-29T18:14:53.762Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:53.762Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:53.963Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {13,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:53.963Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:53.963Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792641.218967>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:53.964Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.218967>,
                                  inet_tcp_dist,<0.401.0>,
                                  #Ref<0.4274179209.2407792641.218970>}
[ns_server:debug,2023-08-29T18:14:53.968Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.218967>,
                               inet_tcp_dist,<0.401.0>,
                               #Ref<0.4274179209.2407792641.218970>}
[error_logger:info,2023-08-29T18:14:53.968Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.401.0>,{recv_challenge_failed,{error,closed}}}}
[error_logger:info,2023-08-29T18:14:53.968Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'couchdb_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:14:53.969Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: {badrpc,nodedown}
[error_logger:info,2023-08-29T18:14:54.170Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {14,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:14:54.170Z,ns_1@cb.local:net_kernel<0.212.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:14:54.170Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792641.218977>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:14:54.170Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.218977>,
                                  inet_tcp_dist,<0.403.0>,
                                  #Ref<0.4274179209.2407792641.218980>}
[ns_server:debug,2023-08-29T18:14:54.225Z,ns_1@cb.local:<0.378.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:167]ns_couchdb is not ready: false
[ns_server:info,2023-08-29T18:14:54.830Z,ns_1@cb.local:ns_couchdb_port<0.375.0>:ns_port_server:log:226]ns_couchdb<0.375.0>: Apache CouchDB v7.1.3-6-g24833e2 (LogLevel=info) is starting.

[ns_server:info,2023-08-29T18:14:55.122Z,ns_1@cb.local:ns_couchdb_port<0.375.0>:ns_port_server:log:226]ns_couchdb<0.375.0>: Apache CouchDB has started. Time to relax.

[error_logger:info,2023-08-29T18:14:55.177Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.377.0>},
              {name,wait_for_couchdb_node},
              {mfargs,{erlang,apply,
                              [#Fun<ns_server_nodes_sup.0.64114752>,[]]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.187Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.408.0>},
              {name,ns_disksup},
              {mfargs,{ns_disksup,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.191Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.409.0>},
              {name,diag_handler_worker},
              {mfargs,{work_queue,start_link,[diag_handler_worker]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2023-08-29T18:14:55.196Z,ns_1@cb.local:ns_server_sup<0.407.0>:dir_size:start_link:33]Starting quick version of dir_size with program name: godu
[error_logger:info,2023-08-29T18:14:55.197Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.410.0>},
              {name,dir_size},
              {mfargs,{dir_size,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.203Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.411.0>},
              {name,request_tracker},
              {mfargs,{request_tracker,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.210Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.412.0>},
              {name,chronicle_kv_log},
              {mfargs,{chronicle_kv_log,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2023-08-29T18:14:55.223Z,ns_1@cb.local:ns_log<0.414.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/ns_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2023-08-29T18:14:55.223Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.414.0>},
              {name,ns_log},
              {mfargs,{ns_log,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.224Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.415.0>},
              {name,event_log_events},
              {mfargs,{gen_event,start_link,[{local,event_log_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2023-08-29T18:14:55.229Z,ns_1@cb.local:event_log_server<0.416.0>:gossip_replicator:read_logs:244]Couldn't load logs from "/opt/couchbase/var/lib/couchbase/event_log" (perhaps it's first
                         startup): {error,enoent}
[error_logger:info,2023-08-29T18:14:55.230Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.416.0>},
              {name,event_log_server},
              {mfargs,{event_log_server,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.289Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.418.0>},
              {name,initargs_updater},
              {mfargs,{initargs_updater,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.293Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.420.0>},
              {name,timer_lag_recorder},
              {mfargs,{timer_lag_recorder,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:55.293Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.421.0>},
              {name,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2023-08-29T18:14:55.378Z,ns_1@cb.local:ns_couchdb_port<0.375.0>:ns_port_server:log:226]ns_couchdb<0.375.0>: 245: Booted. Waiting for shutdown request
ns_couchdb<0.375.0>: working as port

[ns_server:debug,2023-08-29T18:14:55.424Z,ns_1@cb.local:<0.426.0>:goport:handle_eof:585]Stream 'stdout' closed
[ns_server:debug,2023-08-29T18:14:55.425Z,ns_1@cb.local:<0.426.0>:goport:handle_eof:585]Stream 'stderr' closed
[ns_server:info,2023-08-29T18:14:55.425Z,ns_1@cb.local:<0.426.0>:goport:handle_process_exit:566]Port exited with status 0.
[ns_server:debug,2023-08-29T18:14:55.469Z,ns_1@cb.local:prometheus_cfg<0.422.0>:prometheus_cfg:ensure_prometheus_config:804]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus.yml
[ns_server:debug,2023-08-29T18:14:55.517Z,ns_1@cb.local:prometheus_cfg<0.422.0>:prometheus_cfg:ensure_prometheus_config:804]Updating prometheus config file: /opt/couchbase/var/lib/couchbase/config/prometheus_rules.yml
[ns_server:debug,2023-08-29T18:14:55.634Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{6,63860552095}}]}]
[ns_server:debug,2023-08-29T18:14:55.634Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',prometheus_auth_info} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552095}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"DxBp77BWEtDRnhVC01VIpYURs7Gn5zjftfzSQ4+vkRI=">>}},
    {<<"sha512">>,
     {[{<<"h">>,
        {sanitized,<<"cwWcSL12N4zWttNlTH4XtlOX2Mf/4sX4mzl99mnuuYk=">>}},
       {<<"s">>,
        <<"ZO4AsN0VtNAJ/HUwKHmxkQdPXcFYsiSQL5dJF13JYcY63pU/uLlEqkvwKoDIRcT6jdmUuzSJscSsaVB81udUOw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,
        {sanitized,<<"V58Yrqjy9WA2D9HoExukibtMDOTSf5pMNxy/+jhZKNo=">>}},
       {<<"s">>,<<"OoiOw4UgLXbJ8bO1hb3dwbpOtObbR8RTflQq9vde8pU=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,
        {sanitized,<<"7N1ZV3XdMV7vuQtOnXnSnPCSq2dc/VxI1Ls4nQ7f+fI=">>}},
       {<<"s">>,<<"WwNr6OiWKpSPGlmapdb14zR+9Zw=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2023-08-29T18:14:55.719Z,ns_1@cb.local:prometheus_cfg<0.422.0>:prometheus_cfg:apply_config:619]Restarting Prometheus as the start specs have changed
[error_logger:info,2023-08-29T18:14:55.762Z,ns_1@cb.local:ale_dynamic_sup<0.77.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ale_dynamic_sup}
    started: [{pid,<0.429.0>},
              {id,'sink-prometheus'},
              {mfargs,
                  {ale_dynamic_sup,delay_death,
                      [{ale_disk_sink,start_link,
                           ['sink-prometheus',
                            "/opt/couchbase/var/lib/couchbase/logs/prometheus.log",
                            [{rotation,
                                 [{compress,true},
                                  {size,41943040},
                                  {num_files,10},
                                  {buffer_size_max,52428800}]}]]},
                       1000]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:55.919Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.422.0>},
              {name,prometheus_cfg},
              {mfargs,{prometheus_cfg,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:55.929Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:init:58]Init config writer for memcached_passwords, "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-08-29T18:14:55.935Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-08-29T18:14:56.004Z,ns_1@cb.local:memcached_passwords<0.435.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2023-08-29T18:14:56.007Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2023-08-29T18:14:56.017Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2023-08-29T18:14:56.017Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[error_logger:info,2023-08-29T18:14:56.017Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.435.0>},
              {name,memcached_passwords},
              {mfargs,{memcached_passwords,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.032Z,ns_1@cb.local:inet_gethost_native_sup<0.438.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,inet_gethost_native_sup}
    started: [{pid,<0.439.0>},{mfa,{inet_gethost_native,init,[[]]}}]

[error_logger:info,2023-08-29T18:14:56.033Z,ns_1@cb.local:kernel_safe_sup<0.67.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_safe_sup}
    started: [{pid,<0.438.0>},
              {id,inet_gethost_native_sup},
              {mfargs,{inet_gethost_native,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.038Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:init:58]Init config writer for memcached_permissions, "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-08-29T18:14:56.056Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-08-29T18:14:56.065Z,ns_1@cb.local:memcached_permissions<0.440.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:warn,2023-08-29T18:14:56.085Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:56.086Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-08-29T18:14:56.086Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2023-08-29T18:14:56.095Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-08-29T18:14:56.095Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[error_logger:info,2023-08-29T18:14:56.095Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.440.0>},
              {name,memcached_permissions},
              {mfargs,{memcached_permissions,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2023-08-29T18:14:56.097Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:56.097Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-08-29T18:14:56.100Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.446.0>},
              {name,ns_email_alert},
              {mfargs,{ns_email_alert,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.105Z,ns_1@cb.local:ns_node_disco_sup<0.447.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.448.0>},
              {id,ns_node_disco_events},
              {mfargs,{gen_event,start_link,[{local,ns_node_disco_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.106Z,ns_1@cb.local:ns_node_disco<0.450.0>:ns_node_disco:init:112]Initting ns_node_disco with []
[ns_server:debug,2023-08-29T18:14:56.106Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[error_logger:info,2023-08-29T18:14:56.106Z,ns_1@cb.local:ns_node_disco_sup<0.447.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.450.0>},
              {id,ns_node_disco},
              {mfargs,{ns_node_disco,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.107Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{7,63860552096}}]}]
[user:info,2023-08-29T18:14:56.107Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_init:78]Initial otp cookie generated: {sanitized,
                                  <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:56.108Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
otp ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552096}}]},
 {cookie,{sanitized,<<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}}]
[ns_server:debug,2023-08-29T18:14:56.108Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-08-29T18:14:56.108Z,ns_1@cb.local:<0.452.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:56.108Z,ns_1@cb.local:<0.454.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[error_logger:info,2023-08-29T18:14:56.111Z,ns_1@cb.local:ns_node_disco_sup<0.447.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.455.0>},
              {id,ns_node_disco_log},
              {mfargs,{ns_node_disco_log,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.117Z,ns_1@cb.local:<0.454.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:56.117Z,ns_1@cb.local:<0.452.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[error_logger:info,2023-08-29T18:14:56.131Z,ns_1@cb.local:ns_config_rep_sup<0.456.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.457.0>},
              {id,ns_config_rep_merger},
              {mfargs,{ns_config_rep,start_link_merger,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,brutal_kill},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.132Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:init:80]init pulling
[ns_server:debug,2023-08-29T18:14:56.132Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:init:82]init pushing
[error_logger:info,2023-08-29T18:14:56.136Z,ns_1@cb.local:ns_config_rep_sup<0.456.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_config_rep_sup}
    started: [{pid,<0.458.0>},
              {id,ns_config_rep},
              {mfargs,{ns_config_rep,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.136Z,ns_1@cb.local:ns_node_disco_sup<0.447.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_node_disco_sup}
    started: [{pid,<0.456.0>},
              {id,ns_config_rep_sup},
              {mfargs,{ns_config_rep_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:56.137Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.447.0>},
              {name,ns_node_disco_sup},
              {mfargs,{ns_node_disco_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:56.138Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.469.0>},
              {name,tombstone_agent},
              {mfargs,{tombstone_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.148Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.471.0>},
              {name,vbucket_map_mirror},
              {mfargs,{vbucket_map_mirror,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.156Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.473.0>},
              {name,capi_url_cache},
              {mfargs,{capi_url_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.168Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.475.0>},
              {name,bucket_info_cache},
              {mfargs,{bucket_info_cache,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.169Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.478.0>},
              {name,ns_tick_event},
              {mfargs,{gen_event,start_link,[{local,ns_tick_event}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.171Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.479.0>},
              {name,buckets_events},
              {mfargs,{gen_event,start_link,[{local,buckets_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.172Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.480.0>},
              {name,ns_stats_event},
              {mfargs,{gen_event,start_link,[{local,ns_stats_event}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.188Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.481.0>},
              {name,samples_loader_tasks},
              {mfargs,{samples_loader_tasks,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.219Z,ns_1@cb.local:ns_heart_sup<0.483.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.485.0>},
              {id,ns_heart},
              {mfargs,{ns_heart,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.219Z,ns_1@cb.local:ns_heart_sup<0.483.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.487.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.220Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.483.0>},
              {name,ns_heart_sup},
              {mfargs,{ns_heart_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:56.232Z,ns_1@cb.local:ns_doctor_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.493.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.248Z,ns_1@cb.local:ns_doctor_sup<0.492.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.498.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.249Z,ns_1@cb.local:<0.490.0>:restartable:start_child:92]Started child process <0.492.0>
  MFA: {ns_doctor_sup,start_link,[]}
[error_logger:info,2023-08-29T18:14:56.249Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.490.0>},
              {name,ns_doctor_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_doctor_sup,start_link,[]},infinity]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:56.250Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.502.0>},
              {name,master_activity_events},
              {mfargs,{gen_event,start_link,[{local,master_activity_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.269Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.509.0>},
              {name,xdcr_ckpt_store},
              {mfargs,{simple_store,start_link,[xdcr_ckpt_data]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.270Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.510.0>},
              {name,metakv_worker},
              {mfargs,{work_queue,start_link,[metakv_worker]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.270Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.511.0>},
              {name,index_events},
              {mfargs,{gen_event,start_link,[{local,index_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.271Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.512.0>},
              {name,index_settings_manager},
              {mfargs,{index_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.273Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.514.0>},
              {name,query_settings_manager},
              {mfargs,{query_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.274Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.516.0>},
              {name,eventing_settings_manager},
              {mfargs,{eventing_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.274Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.518.0>},
              {name,analytics_settings_manager},
              {mfargs,{analytics_settings_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.275Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.520.0>},
              {name,audit_events},
              {mfargs,{gen_event,start_link,[{local,audit_events}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.286Z,ns_1@cb.local:user_uuid_limits<0.521.0>:versioned_cache:init:41]Starting versioned cache user_uuid_limits
[error_logger:info,2023-08-29T18:14:56.287Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.521.0>},
              {name,user_limits_cache},
              {mfargs,{user_request_throttler,start_limits_cache,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.288Z,ns_1@cb.local:user_request_throttler<0.523.0>:user_request_throttler:handle_info:286]Clearing all user stats {<0.525.0>,#Ref<0.4274179209.2407792643.218208>}
[error_logger:info,2023-08-29T18:14:56.288Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.523.0>},
              {name,user_request_throttler},
              {mfargs,{user_request_throttler,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.313Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.527.0>},
              {id,menelaus_ui_auth},
              {mfargs,{menelaus_ui_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.314Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.529.0>},
              {id,scram_sha},
              {mfargs,{scram_sha,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.328Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.531.0>},
              {id,menelaus_local_auth},
              {mfargs,{menelaus_local_auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.349Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.532.0>},
              {id,menelaus_web_cache},
              {mfargs,{menelaus_web_cache,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.369Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.533.0>},
              {id,menelaus_stats_gatherer},
              {mfargs,{menelaus_stats_gatherer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.370Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.534.0>},
              {id,json_rpc_events},
              {mfargs,{gen_event,start_link,[{local,json_rpc_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.386Z,ns_1@cb.local:menelaus_web_sup<0.535.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.536.0>},
              {id,menelaus_event},
              {mfargs,{menelaus_event,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:56.387Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2023-08-29T18:14:56.388Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2023-08-29T18:14:56.389Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2023-08-29T18:14:56.389Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2023-08-29T18:14:56.390Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2023-08-29T18:14:56.391Z,ns_1@cb.local:<0.538.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"0.0.0.0"},{port,8091}]
[error_logger:info,2023-08-29T18:14:56.391Z,ns_1@cb.local:<0.538.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.538.0>,menelaus_web}
    started: [{pid,<0.539.0>},
              {id,menelaus_web_ipv4},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:56.392Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for backup from "/opt/couchbase/etc/couchbase/pluggable-ui-backup.json"
[ns_server:info,2023-08-29T18:14:56.393Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for cbas from "/opt/couchbase/etc/couchbase/pluggable-ui-cbas.json"
[ns_server:info,2023-08-29T18:14:56.393Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for eventing from "/opt/couchbase/etc/couchbase/pluggable-ui-eventing.json"
[ns_server:info,2023-08-29T18:14:56.394Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for fts from "/opt/couchbase/etc/couchbase/pluggable-ui-fts.json"
[ns_server:info,2023-08-29T18:14:56.394Z,ns_1@cb.local:<0.538.0>:menelaus_pluggable_ui:read_and_validate_plugin_spec:129]Loaded pluggable UI specification for n1ql from "/opt/couchbase/etc/couchbase/pluggable-ui-query.json"
[ns_server:info,2023-08-29T18:14:56.396Z,ns_1@cb.local:<0.538.0>:menelaus_web:maybe_start_http_server:126]Started web service with options:
[{ip,"::"},{port,8091}]
[error_logger:info,2023-08-29T18:14:56.397Z,ns_1@cb.local:<0.538.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.538.0>,menelaus_web}
    started: [{pid,<0.556.0>},
              {id,menelaus_web_ipv6},
              {mfargs,
                  {menelaus_web,http_server,
                      [[{afamily,inet6},
                        {name,menelaus_web},
                        {port,8091},
                        {nodelay,true},
                        {approot,
                            "/opt/couchbase/lib/ns_server/erlang/lib/ns_server/priv/public"}]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.397Z,ns_1@cb.local:<0.537.0>:restartable:start_child:92]Started child process <0.538.0>
  MFA: {menelaus_web,start_link,[]}
[error_logger:info,2023-08-29T18:14:56.398Z,ns_1@cb.local:menelaus_web_sup<0.535.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_web_sup}
    started: [{pid,<0.537.0>},
              {id,menelaus_web},
              {mfargs,{restartable,start_link,
                                   [{menelaus_web,start_link,[]},infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[user:info,2023-08-29T18:14:56.398Z,ns_1@cb.local:menelaus_sup<0.526.0>:menelaus_web_sup:start_link:38]Couchbase Server has started on web port 8091 on node 'ns_1@cb.local'. Version: "7.2.0-5325-enterprise".
[error_logger:info,2023-08-29T18:14:56.399Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.535.0>},
              {id,menelaus_web_sup},
              {mfargs,{menelaus_web_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:56.403Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.573.0>},
              {id,hot_keys_keeper},
              {mfargs,{hot_keys_keeper,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.403Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.574.0>},
              {id,menelaus_web_alerts_srv},
              {mfargs,{menelaus_web_alerts_srv,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.411Z,ns_1@cb.local:menelaus_sup<0.526.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,menelaus_sup}
    started: [{pid,<0.575.0>},
              {id,menelaus_cbauth},
              {mfargs,{menelaus_cbauth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.411Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.526.0>},
              {name,menelaus},
              {mfargs,{menelaus_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:56.411Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.581.0>},
              {name,ns_ports_setup},
              {mfargs,{ns_ports_setup,start,[]}},
              {restart_type,{permanent,4}},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.416Z,ns_1@cb.local:service_agent_sup<0.584.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.585.0>},
              {id,service_agent_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_agent_children_sup},
                                   service_agent_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:56.418Z,ns_1@cb.local:service_agent_sup<0.584.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_agent_sup}
    started: [{pid,<0.586.0>},
              {id,service_agent_worker},
              {mfargs,{erlang,apply,[#Fun<service_agent_sup.0.32483565>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.418Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.584.0>},
              {name,service_agent_sup},
              {mfargs,{service_agent_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2023-08-29T18:14:56.424Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1017]Going to notify following services: [memcached,capi_ssl_service]
[ns_server:warn,2023-08-29T18:14:56.426Z,ns_1@cb.local:<0.592.0>:ns_ssl_services_setup:notify_service:1055]Failed to notify service memcached: {error,no_proccess}
[ns_server:debug,2023-08-29T18:14:56.428Z,ns_1@cb.local:ns_ports_setup<0.581.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[error_logger:info,2023-08-29T18:14:56.431Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.594.0>},
              {name,ns_memcached_sockets_pool},
              {mfargs,{ns_memcached_sockets_pool,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.442Z,ns_1@cb.local:memcached_auth_server<0.595.0>:memcached_auth_server:reconnect:234]Skipping creation of 'Auth provider' connection because external users are disabled
[error_logger:info,2023-08-29T18:14:56.442Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.595.0>},
              {name,memcached_auth_server},
              {mfargs,{memcached_auth_server,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.442Z,ns_1@cb.local:ns_audit_cfg<0.598.0>:ns_audit_cfg:write_audit_json:237]Writing new content to "/opt/couchbase/var/lib/couchbase/config/audit.json", Params [{descriptors_path,
                                                                                      "/opt/couchbase/etc/security"},
                                                                                     {version,
                                                                                      2},
                                                                                     {uuid,
                                                                                      "55497325"},
                                                                                     {event_states,
                                                                                      {[]}},
                                                                                     {filtering_enabled,
                                                                                      true},
                                                                                     {disabled_userids,
                                                                                      []},
                                                                                     {auditd_enabled,
                                                                                      false},
                                                                                     {log_path,
                                                                                      "/opt/couchbase/var/lib/couchbase/logs"},
                                                                                     {rotate_interval,
                                                                                      86400},
                                                                                     {rotate_size,
                                                                                      20971520},
                                                                                     {sync,
                                                                                      []}]
[ns_server:info,2023-08-29T18:14:56.468Z,ns_1@cb.local:<0.593.0>:ns_ssl_services_setup:notify_service:1053]Successfully notified service capi_ssl_service
[ns_server:info,2023-08-29T18:14:56.469Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1033]Succesfully notified services [capi_ssl_service]
[ns_server:debug,2023-08-29T18:14:56.515Z,ns_1@cb.local:ns_audit_cfg<0.598.0>:ns_audit_cfg:notify_memcached:151]Instruct memcached to reload audit config
[error_logger:info,2023-08-29T18:14:56.516Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.598.0>},
              {name,ns_audit_cfg},
              {mfargs,{ns_audit_cfg,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:warn,2023-08-29T18:14:56.518Z,ns_1@cb.local:<0.602.0>:ns_memcached:connect:1249]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2023-08-29T18:14:56.551Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.603.0>},
              {name,ns_audit},
              {mfargs,{ns_audit,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.551Z,ns_1@cb.local:memcached_config_mgr<0.604.0>:memcached_config_mgr:memcached_port_pid:151]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-08-29T18:14:56.551Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.604.0>},
              {name,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:info,2023-08-29T18:14:56.556Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1044]Failed to notify some services. Will retry in 5 sec, [{{error,no_proccess},
                                                       memcached}]
[ns_server:info,2023-08-29T18:14:56.579Z,ns_1@cb.local:<0.608.0>:ns_memcached_log_rotator:init:36]Starting log rotator on "/opt/couchbase/var/lib/couchbase/logs"/"memcached.log"* with an initial period of 39003ms
[error_logger:info,2023-08-29T18:14:56.579Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.608.0>},
              {name,ns_memcached_log_rotator},
              {mfargs,{ns_memcached_log_rotator,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.591Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.611.0>},
              {name,testconditions_store},
              {mfargs,{simple_store,start_link,[testconditions]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.596Z,ns_1@cb.local:<0.613.0>:memcached_config_mgr:memcached_port_pid:151]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-08-29T18:14:56.596Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.613.0>},
              {name,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.613Z,ns_1@cb.local:ns_bucket_worker_sup<0.615.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.616.0>},
              {id,ns_bucket_sup},
              {mfargs,{ns_bucket_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:56.622Z,ns_1@cb.local:ns_bucket_worker_sup<0.615.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_bucket_worker_sup}
    started: [{pid,<0.618.0>},
              {id,ns_bucket_worker},
              {mfargs,{ns_bucket_worker,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.622Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.615.0>},
              {name,ns_bucket_worker_sup},
              {mfargs,{ns_bucket_worker_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:56.624Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.620.0>},
              {name,ns_server_stats},
              {mfargs,{ns_server_stats,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.640Z,ns_1@cb.local:ns_heart<0.485.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2023-08-29T18:14:56.648Z,ns_1@cb.local:ns_heart<0.485.0>:cluster_logs_collection_task:maybe_build_cluster_logs_task:43]Ignoring exception trying to read cluster_logs_collection_task_status table: error:badarg
[error_logger:info,2023-08-29T18:14:56.660Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.623.0>},
              {name,{stats_reader,"@system"}},
              {mfargs,{stats_reader,start_link,["@system"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.660Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.625.0>},
              {name,{stats_reader,"@system-processes"}},
              {mfargs,{stats_reader,start_link,["@system-processes"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.661Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.627.0>},
              {name,{stats_reader,"@query"}},
              {mfargs,{stats_reader,start_link,["@query"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.661Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.629.0>},
              {name,{stats_reader,"@global"}},
              {mfargs,{stats_reader,start_link,["@global"]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.666Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.631.0>},
              {name,goxdcr_status_keeper},
              {mfargs,{goxdcr_status_keeper,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.668Z,ns_1@cb.local:goxdcr_status_keeper<0.631.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[ns_server:debug,2023-08-29T18:14:56.673Z,ns_1@cb.local:goxdcr_status_keeper<0.631.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-08-29T18:14:56.673Z,ns_1@cb.local:services_stats_sup<0.634.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.635.0>},
              {id,service_stats_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_stats_children_sup},
                                   services_stats_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:56.679Z,ns_1@cb.local:service_status_keeper_sup<0.638.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.639.0>},
              {id,service_status_keeper_worker},
              {mfargs,{work_queue,start_link,[service_status_keeper_worker]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.697Z,ns_1@cb.local:service_status_keeper_sup<0.638.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.643.0>},
              {id,service_status_keeper_index},
              {mfargs,{service_index,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.703Z,ns_1@cb.local:service_status_keeper_sup<0.638.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.655.0>},
              {id,service_status_keeper_fts},
              {mfargs,{service_fts,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.712Z,ns_1@cb.local:service_status_keeper_sup<0.638.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,service_status_keeper_sup}
    started: [{pid,<0.658.0>},
              {id,service_status_keeper_eventing},
              {mfargs,{service_eventing,start_keeper,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.713Z,ns_1@cb.local:services_stats_sup<0.634.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.638.0>},
              {id,service_status_keeper_sup},
              {mfargs,{service_status_keeper_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:56.720Z,ns_1@cb.local:services_stats_sup<0.634.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,services_stats_sup}
    started: [{pid,<0.661.0>},
              {id,service_stats_worker},
              {mfargs,{erlang,apply,
                              [#Fun<services_stats_sup.0.114823200>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.721Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.634.0>},
              {name,services_stats_sup},
              {mfargs,{services_stats_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2023-08-29T18:14:56.738Z,ns_1@cb.local:<0.671.0>:new_concurrency_throttle:init:109]init concurrent throttle process, pid: <0.671.0>, type: kv_throttle# of available token: 1
[ns_server:debug,2023-08-29T18:14:56.744Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:14:56.744Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[error_logger:info,2023-08-29T18:14:56.744Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.666.0>},
              {name,compaction_daemon},
              {mfargs,{compaction_daemon,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,86400000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.744Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:14:56.744Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:14:56.745Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_master. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:14:56.745Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_master too soon. Next run will be in 3600s
[error_logger:info,2023-08-29T18:14:56.750Z,ns_1@cb.local:cluster_logs_sup<0.672.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,cluster_logs_sup}
    started: [{pid,<0.673.0>},
              {id,ets_holder},
              {mfargs,{cluster_logs_collection_task,start_link_ets_holder,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.750Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.672.0>},
              {name,cluster_logs_sup},
              {mfargs,{cluster_logs_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:56.751Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.675.0>},
              {name,collections},
              {mfargs,{collections,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:56.751Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.677.0>},
              {name,leader_events},
              {mfargs,{gen_event,start_link,[{local,leader_events}]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:56.752Z,ns_1@cb.local:ns_heart_slow_status_updater<0.487.0>:goxdcr_rest:get_from_goxdcr:137]Goxdcr is temporary not available. Return empty list.
[error_logger:info,2023-08-29T18:14:56.776Z,ns_1@cb.local:leader_leases_sup<0.680.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.681.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.791Z,ns_1@cb.local:leader_leases_sup<0.680.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.682.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:56.791Z,ns_1@cb.local:leader_services_sup<0.679.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.680.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:56.804Z,ns_1@cb.local:leader_registry_sup<0.688.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.689.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.812Z,ns_1@cb.local:leader_registry_sup<0.688.0>:mb_master:check_master_takeover_needed:254]Sending master node question to the following nodes: []
[ns_server:debug,2023-08-29T18:14:56.812Z,ns_1@cb.local:leader_registry_sup<0.688.0>:mb_master:check_master_takeover_needed:256]Got replies: []
[ns_server:debug,2023-08-29T18:14:56.812Z,ns_1@cb.local:leader_registry_sup<0.688.0>:mb_master:check_master_takeover_needed:262]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2023-08-29T18:14:56.813Z,ns_1@cb.local:mb_master<0.691.0>:mb_master:init:80]Heartbeat interval is 2000
[user:info,2023-08-29T18:14:56.813Z,ns_1@cb.local:mb_master<0.691.0>:mb_master:init:85]I'm the only node, so I'm the master.
[ns_server:debug,2023-08-29T18:14:56.813Z,ns_1@cb.local:leader_registry<0.689.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@cb.local'. Invalidating name cache.
[ns_server:debug,2023-08-29T18:14:56.843Z,ns_1@cb.local:mb_master<0.691.0>:master_activity_events:submit_cast:76]Failed to send master activity event {became_master,'ns_1@cb.local'}: {error,
                                                                       badarg}
[error_logger:info,2023-08-29T18:14:56.850Z,ns_1@cb.local:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.694.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.858Z,ns_1@cb.local:ns_ports_setup<0.581.0>:ns_ports_setup:set_children:60]Monitor ns_child_ports_sup <16118.129.0>
[ns_server:debug,2023-08-29T18:14:56.859Z,ns_1@cb.local:leader_quorum_nodes_manager<0.696.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[error_logger:info,2023-08-29T18:14:56.859Z,ns_1@cb.local:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.696.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.859Z,ns_1@cb.local:<0.613.0>:memcached_config_mgr:memcached_port_pid:153]ns_ports_setup seems to be ready
[ns_server:debug,2023-08-29T18:14:56.859Z,ns_1@cb.local:leader_quorum_nodes_manager<0.696.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:debug,2023-08-29T18:14:56.859Z,ns_1@cb.local:memcached_config_mgr<0.604.0>:memcached_config_mgr:memcached_port_pid:153]ns_ports_setup seems to be ready
[ns_server:info,2023-08-29T18:14:56.876Z,ns_1@cb.local:mb_master_sup<0.693.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.702.0> on 'ns_1@cb.local'

[error_logger:info,2023-08-29T18:14:56.876Z,ns_1@cb.local:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.702.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.885Z,ns_1@cb.local:leader_lease_agent<0.682.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"c068c6b6b1dd5df104ba1bbe14a568e8">>,
                                'ns_1@cb.local'} for 15000ms
[ns_server:debug,2023-08-29T18:14:56.892Z,ns_1@cb.local:<0.613.0>:memcached_config_mgr:find_port_pid_loop:161]Found memcached port <16118.135.0>
[ns_server:debug,2023-08-29T18:14:56.904Z,ns_1@cb.local:<0.706.0>:chronicle_master:do_init:141]Starting with SelfRef = #Ref<0.4274179209.2407792645.217849>
[ns_server:debug,2023-08-29T18:14:56.905Z,ns_1@cb.local:memcached_config_mgr<0.604.0>:memcached_config_mgr:find_port_pid_loop:161]Found memcached port <16118.135.0>
[ns_server:info,2023-08-29T18:14:56.905Z,ns_1@cb.local:mb_master_sup<0.693.0>:misc:start_singleton:901]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.706.0> on 'ns_1@cb.local'

[error_logger:info,2023-08-29T18:14:56.905Z,ns_1@cb.local:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.706.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:56.925Z,ns_1@cb.local:<0.613.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":7,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]}}">>
[ns_server:warn,2023-08-29T18:14:56.928Z,ns_1@cb.local:<0.712.0>:ns_memcached:connect:1249]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}, retrying.
[error_logger:info,2023-08-29T18:14:56.931Z,ns_1@cb.local:ns_orchestrator_sup<0.713.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.714.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:56.949Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[6,5]}]

[ns_server:info,2023-08-29T18:14:56.951Z,ns_1@cb.local:ns_config<0.260.0>:ns_online_config_upgrader:do_upgrade_config:67]Performing online config upgrade to [6,6]
[ns_server:info,2023-08-29T18:14:56.951Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[6,6]},{set,buckets,[{configs,[]}]}]

[ns_server:info,2023-08-29T18:14:56.962Z,ns_1@cb.local:<0.701.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@cb.local' (lease uuid: <<"c068c6b6b1dd5df104ba1bbe14a568e8">>)
[ns_server:info,2023-08-29T18:14:56.987Z,ns_1@cb.local:ns_config<0.260.0>:chronicle_compat:upgrade:366]Keys are migrated to chronicle. Rev = {<<"918babd26e22549c990a18e4926f0ef9">>,
                                       3}. Sets = [{set,counters,[]},
                                                   {set,auto_reprovision_cfg,
                                                    [{enabled,true},
                                                     {max_nodes,1},
                                                     {count,0}]},
                                                   {set,bucket_names,[]},
                                                   {set,nodes_wanted,
                                                    ['ns_1@cb.local']},
                                                   {set,server_groups,
                                                    [[{uuid,<<"0">>},
                                                      {name,<<"Group 1">>},
                                                      {nodes,
                                                       ['ns_1@cb.local']}]]},
                                                   {set,
                                                    {node,'ns_1@cb.local',
                                                     membership},
                                                    active}]
[ns_server:info,2023-08-29T18:14:56.988Z,ns_1@cb.local:ns_config<0.260.0>:ns_online_config_upgrader:do_upgrade_config:67]Performing online config upgrade to [7,0]
[ns_server:info,2023-08-29T18:14:56.988Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[7,0]},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.num_replica\":0,\"indexer.settings.compaction.abort_exceed_interval\":false}">>},
 {set,{metakv,<<"/query/settings/config">>},
      <<"{\"timeout\":0,\"numatrs\":1024,\"n1ql-feat-ctrl\":76,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"pipeline-batch\":16,\"prepared-limit\":16384,\"pipeline-cap\":512,\"memory-quota\":0,\"cleanupwindow\":\"60s\",\"use-cbo\":true,\"scan-cap\":512,\"query.settings.tmp_space_size\":5120,\"completed-threshold\":1000,\"loglevel\":\"info\",\"cleanuplostattempts\":true,\"cleanupclientattempts\":true,\"txtimeout\":\"0ms\"}">>},
 {set,after_upgrade_cleanup,
      [{counters,'_deleted'},
       {{node,'ns_1@cb.local',membership},'_deleted'},
       {server_groups,'_deleted'},
       {nodes_wanted,[]},
       {buckets,[{configs,[]}]},
       {auto_reprovision_cfg,'_deleted'},
       {auto_reprovision_cfg,'_deleted'},
       {buckets,[{configs,[]}]},
       {{node,'ns_1@cb.local',membership},'_deleted'},
       {server_groups,'_deleted'},
       {nodes_wanted,[]}]}]

[ns_server:debug,2023-08-29T18:14:56.991Z,ns_1@cb.local:roles_cache<0.366.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2023-08-29T18:14:56.991Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-08-29T18:14:56.991Z,ns_1@cb.local:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_apply_new_settings:601]Settings didn't change, ignoring update
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
after_upgrade_cleanup ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552096}}]},
 {counters,'_deleted'},
 {{node,'ns_1@cb.local',membership},'_deleted'},
 {server_groups,'_deleted'},
 {nodes_wanted,[]},
 {buckets,[{configs,[]}]},
 {auto_reprovision_cfg,'_deleted'},
 {auto_reprovision_cfg,'_deleted'},
 {buckets,[{configs,[]}]},
 {{node,'ns_1@cb.local',membership},'_deleted'},
 {server_groups,'_deleted'},
 {nodes_wanted,[]}]
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([after_upgrade_cleanup,buckets,
                               cluster_compat_version,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>},
                               {metakv,<<"/query/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_compat_version ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{3,63860552096}}]},7,0]
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:<0.718.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
buckets ->
[{0,[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552096}}]},{configs,[]}]
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:roles_cache<0.366.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:<0.718.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {undefined,
                                                                {0,1676588058},
                                                                {0,1676588058},
                                                                false,[]} to {[7,
                                                                               0],
                                                                              {0,
                                                                               1676588058},
                                                                              {0,
                                                                               1676588058},
                                                                              false,
                                                                              []}
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:maybe_store_ca_certs:726]Considering to store CA certs
[ns_server:debug,2023-08-29T18:14:56.992Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552096}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:14:56.993Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/query/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552096}}]}|
 <<"{\"timeout\":0,\"numatrs\":1024,\"n1ql-feat-ctrl\":76,\"max-parallelism\":1,\"query.settings.curl_whitelist\":{\"all_access\":false,\"allowed_urls\":[],\"disallowed_urls\":[]},\"query.settings.tmp_space_dir\":\"/opt/couchbase/var/lib/couchbase/tmp\",\"completed-limit\":4000,\"pipeline-batch\":16,\"prepared-limit\":16384,\"pipeline-cap\":512,\"memory-quota\":0,\"cleanupwindow\":\"60s\",\"use-cbo\":true,\"scan-cap\":512,\"que"...>>]
[ns_server:debug,2023-08-29T18:14:56.993Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{8,63860552096}}]}]
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 46 us
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@cb.local',membership}, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                                      3})
active
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:<0.729.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 47 us
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,3})
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@cb.local']}]]
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,3})
['ns_1@cb.local']
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:<0.729.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:56.994Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: bucket_names, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,3})
[]
[ns_server:debug,2023-08-29T18:14:56.995Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: auto_reprovision_cfg, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                         3})
[{enabled,true},{max_nodes,1},{count,0}]
[ns_server:debug,2023-08-29T18:14:56.995Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: counters, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,3})
[]
[ns_server:debug,2023-08-29T18:14:57.021Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-08-29T18:14:57.027Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2023-08-29T18:14:57.031Z,ns_1@cb.local:kv<0.251.0>:chronicle_upgrade:upgrade_loop:80]Upgading chronicle from [7,0]. Final version = [7,2]
[ns_server:info,2023-08-29T18:14:57.031Z,ns_1@cb.local:kv<0.251.0>:ns_ssl_services_setup:chronicle_upgrade_to_71:1236]Upgrading CA certs to 7.1: setting ca_certificates to the following props:
 [[{id,0},
   {subject,<<"CN=Couchbase Server b51eb775">>},
   {not_before,63524217600},
   {not_after,64691827199},
   {type,generated},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIF3/tSsVH0d4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYjUxZWI3\nNzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCr9XJDH3HmSp0Vabu1\nQm1+A0/9t3DlYJP9BDjf5CmXOlsWpvUIAP24RQ9rQsu3gn8VvYYnkK2mFfF0SRo5\nxJX/EZde2ejr9piUfJTuY18OYQthMm3HKfOyPT/EJQsiQF7LpHl+HdogqtG2kZDS\nQxtwafkr8FiEbtU/XebR1ST0QwW2EDwoG4ViLY2MNtI8o2IUxo9AzqfYL7s11fUU\nDVqimKGAzWHIt1ARuJ/6jzXlTeTPL8/pfbWBQ1ihc42NsFP1q3erqE5YEZFDyMPi\npbmLe4DnvPb9XXRPgLgkg86BKl/MAL1Z5Rx3xmykZ2vBZfq4Jx4x5RaApfPuPwdh\njhyvAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRvykfjVzdx7swRsAwqxzBSxpVX\nHTANBgkqhkiG9w0BAQsFAAOCAQEATX4FfImG0CtsodXXd2PrBiZwe4+R36pm1CBj\nrm/kxaWKI8JD54zJnrTpQ4nc6n+JCgxH5pQgVV4Fnin2HBdjPU0mZv0zajXwuDr4\nzv9jTqw1hQ24RvcgscOOgaceHPW/yX4Mc5SicXzXxVc4bBdauae5RU2KUYkn8+gq\nGx9Vjv7B1orkiCRhPxYO4//CpTDLt2bpMzjPuAhpwSUgv01b6EY0zLIgsH6ftizF\nX6SlSO669NtXIMZKcQ3LR0OFpNPCSdiCWLNsMKQG0LUd9ka5nuQEU9J3v/GMesHF\nJdADlV61Z0BLgRChuO+HR3J2QUkmww5k7UqGjhiH6CDxSUrdfQ==\n-----END CERTIFICATE-----\n\n">>},
   {origin,upgrade}]]
[ns_server:info,2023-08-29T18:14:57.033Z,ns_1@cb.local:kv<0.251.0>:compaction_daemon:chronicle_upgrade_to_71:189]Upgrading autocompaction to 7.1: 
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:info,2023-08-29T18:14:57.033Z,ns_1@cb.local:kv<0.251.0>:chronicle_upgrade:upgrade_loop:80]Upgading chronicle from [7,1]. Final version = [7,2]
[ns_server:debug,2023-08-29T18:14:57.044Z,ns_1@cb.local:memcached_permissions<0.440.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2023-08-29T18:14:57.066Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2023-08-29T18:14:57.082Z,ns_1@cb.local:roles_cache<0.366.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:info,2023-08-29T18:14:57.082Z,ns_1@cb.local:ns_config<0.260.0>:ns_online_config_upgrader:do_upgrade_config:67]Performing online config upgrade to [7,1]
[ns_server:debug,2023-08-29T18:14:57.082Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-08-29T18:14:57.082Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: root_cert_and_pkey, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                       4})
{sanitized,<<"P8dqRIt14llhdphN+G7YRAUd21iUvQKBD45Dqoki9Yw=">>}
[ns_server:debug,2023-08-29T18:14:57.082Z,ns_1@cb.local:<0.733.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:57.082Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: cluster_compat_version, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                           4})
[7,2]
[ns_server:info,2023-08-29T18:14:57.082Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[7,1]},
 {set,{metakv,<<"/indexing/settings/config">>},
      <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.min_frag\":30,\"indexer.settings.inmemory_snapshot.interval\":200,\"indexer.settings.enable_page_bloom_filter\":false,\"indexer.settings.max_cpu_percent\":0,\"indexer.settings.storage_mode\":\"\",\"indexer.settings.recovery.max_rollbacks\":2,\"indexer.settings.num_replica\":0,\"indexer.settings.memory_quota\":536870912,\"indexer.settings.compaction.abort_exceed_interval\":false}">>}]

[ns_server:debug,2023-08-29T18:14:57.082Z,ns_1@cb.local:<0.733.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:57.083Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: ca_certificates, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,4})
[[{id,0},
  {subject,<<"CN=Couchbase Server b51eb775">>},
  {not_before,63524217600},
  {not_after,64691827199},
  {type,generated},
  {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIF3/tSsVH0d4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYjUxZWI3\nNzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCr9XJDH3HmSp0Vabu1\nQm1+A0/9t3DlYJP9BDjf5CmXOlsWpvUIAP24RQ9rQsu3gn8VvYYnkK2mFfF0SRo5\nxJX/EZd"...>>},
  {origin,upgrade}]]
[ns_server:info,2023-08-29T18:14:57.083Z,ns_1@cb.local:ns_config<0.260.0>:ns_online_config_upgrader:do_upgrade_config:67]Performing online config upgrade to [7,2]
[ns_server:debug,2023-08-29T18:14:57.083Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: autocompaction, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,4})
[{database_fragmentation_threshold,{30,undefined}},
 {view_fragmentation_threshold,{30,undefined}},
 {magma_fragmentation_percentage,50}]
[ns_server:info,2023-08-29T18:14:57.089Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:handle_info:656]cert_and_pkey changed
[ns_server:debug,2023-08-29T18:14:57.091Z,ns_1@cb.local:memcached_config_mgr<0.604.0>:memcached_config_mgr:init:92]wrote memcached config to /opt/couchbase/var/lib/couchbase/config/memcached.json. Will activate memcached port server
[ns_server:debug,2023-08-29T18:14:57.093Z,ns_1@cb.local:memcached_config_mgr<0.604.0>:memcached_config_mgr:init:96]activated memcached port server
[ns_server:debug,2023-08-29T18:14:57.120Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:info,2023-08-29T18:14:57.138Z,ns_1@cb.local:ns_config<0.260.0>:ns_config:do_upgrade_config:728]Upgrading config by changes:
[{set,cluster_compat_version,[7,2]},
 {set,audit_decriptors,
      [{8243,
        [{name,<<"mutate document">>},
         {description,<<"Document was mutated via the REST API">>},
         {enabled,true},
         {module,ns_server}]},
       {8255,
        [{name,<<"read document">>},
         {description,<<"Document was read via the REST API">>},
         {enabled,false},
         {module,ns_server}]},
       {8257,
        [{name,<<"alert email sent">>},
         {description,<<"An alert email was successfully sent">>},
         {enabled,true},
         {module,ns_server}]},
       {8265,
        [{name,<<"RBAC information retrieved">>},
         {description,<<"RBAC information was retrieved">>},
         {enabled,true},
         {module,ns_server}]},
       {20480,
        [{name,<<"opened DCP connection">>},
         {description,<<"opened DCP connection">>},
         {enabled,true},
         {module,memcached}]},
       {20482,
        [{name,<<"external memcached bucket flush">>},
         {description,<<"External user flushed the content of a memcached bucket">>},
         {enabled,true},
         {module,memcached}]},
       {20483,
        [{name,<<"invalid packet">>},
         {description,<<"Rejected an invalid packet">>},
         {enabled,true},
         {module,memcached}]},
       {20485,
        [{name,<<"authentication succeeded">>},
         {description,<<"Authentication to the cluster succeeded">>},
         {enabled,false},
         {module,memcached}]},
       {20488,
        [{name,<<"document read">>},
         {description,<<"Document was read">>},
         {enabled,false},
         {module,memcached}]},
       {20489,
        [{name,<<"document locked">>},
         {description,<<"Document was locked">>},
         {enabled,false},
         {module,memcached}]},
       {20490,
        [{name,<<"document modify">>},
         {description,<<"Document was modified">>},
         {enabled,false},
         {module,memcached}]},
       {20491,
        [{name,<<"document delete">>},
         {description,<<"Document was deleted">>},
         {enabled,false},
         {module,memcached}]},
       {20492,
        [{name,<<"select bucket">>},
         {description,<<"The specified bucket was selected">>},
         {enabled,true},
         {module,memcached}]},
       {20493,
        [{name,<<"session terminated">>},
         {description,<<"Session to the cluster has terminated">>},
         {enabled,false},
         {module,memcached}]},
       {20494,
        [{name,<<"tenant rate limited">>},
         {description,<<"The given tenant was rate limited">>},
         {enabled,true},
         {module,memcached}]},
       {28672,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28673,
        [{name,<<"EXPLAIN statement">>},
         {description,<<"A N1QL EXPLAIN statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28674,
        [{name,<<"PREPARE statement">>},
         {description,<<"A N1QL PREPARE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28675,
        [{name,<<"INFER statement">>},
         {description,<<"A N1QL INFER statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28676,
        [{name,<<"INSERT statement">>},
         {description,<<"A N1QL INSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28677,
        [{name,<<"UPSERT statement">>},
         {description,<<"A N1QL UPSERT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28678,
        [{name,<<"DELETE statement">>},
         {description,<<"A N1QL DELETE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28679,
        [{name,<<"UPDATE statement">>},
         {description,<<"A N1QL UPDATE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28680,
        [{name,<<"MERGE statement">>},
         {description,<<"A N1QL MERGE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28681,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28682,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28683,
        [{name,<<"ALTER INDEX statement">>},
         {description,<<"A N1QL ALTER INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28684,
        [{name,<<"BUILD INDEX statement">>},
         {description,<<"A N1QL BUILD INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28685,
        [{name,<<"GRANT ROLE statement">>},
         {description,<<"A N1QL GRANT ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28686,
        [{name,<<"REVOKE ROLE statement">>},
         {description,<<"A N1QL REVOKE ROLE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28687,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An unrecognized statement was received by the N1QL query engine">>},
         {enabled,false},
         {module,n1ql}]},
       {28688,
        [{name,<<"CREATE PRIMARY INDEX statement">>},
         {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28689,
        [{name,<<"/admin/stats API request">>},
         {description,<<"An HTTP request was made to the API at /admin/stats.">>},
         {enabled,false},
         {module,n1ql}]},
       {28690,
        [{name,<<"/admin/vitals API request">>},
         {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
         {enabled,false},
         {module,n1ql}]},
       {28691,
        [{name,<<"/admin/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28692,
        [{name,<<"/admin/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28693,
        [{name,<<"/admin/indexes/prepareds API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
         {enabled,false},
         {module,n1ql}]},
       {28694,
        [{name,<<"/admin/indexes/active_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28695,
        [{name,<<"/admin/indexes/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28697,
        [{name,<<"/admin/ping API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ping.">>},
         {enabled,false},
         {module,n1ql}]},
       {28698,
        [{name,<<"/admin/config API request">>},
         {description,<<"An HTTP request was made to the API at /admin/config.">>},
         {enabled,false},
         {module,n1ql}]},
       {28699,
        [{name,<<"/admin/ssl_cert API request">>},
         {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
         {enabled,false},
         {module,n1ql}]},
       {28700,
        [{name,<<"/admin/settings API request">>},
         {description,<<"An HTTP request was made to the API at /admin/settings.">>},
         {enabled,false},
         {module,n1ql}]},
       {28701,
        [{name,<<"/admin/clusters API request">>},
         {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
         {enabled,false},
         {module,n1ql}]},
       {28702,
        [{name,<<"/admin/completed_requests API request">>},
         {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
         {enabled,false},
         {module,n1ql}]},
       {28704,
        [{name,<<"/admin/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28705,
        [{name,<<"/admin/indexes/functions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28706,
        [{name,<<"CREATE FUNCTION statement">>},
         {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28707,
        [{name,<<"DROP FUNCTION statement">>},
         {description,<<"A N1QL DROP FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28708,
        [{name,<<"EXECUTE FUNCTION statement">>},
         {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28709,
        [{name,<<"/admin/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28710,
        [{name,<<"/admin/indexes/tasks API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
         {enabled,false},
         {module,n1ql}]},
       {28711,
        [{name,<<"/admin/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28712,
        [{name,<<"/admin/indexes/dictionary_cache API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
         {enabled,false},
         {module,n1ql}]},
       {28713,
        [{name,<<"CREATE SCOPE statement">>},
         {description,<<"A N1QL CREATE SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28714,
        [{name,<<"DROP SCOPE statement">>},
         {description,<<"A N1QL DROP SCOPE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28715,
        [{name,<<"CREATE COLLECTION statement">>},
         {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28716,
        [{name,<<"DROP COLLECTION statement">>},
         {description,<<"A N1QL DROP COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28717,
        [{name,<<"FLUSH COLLECTION statement">>},
         {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28718,
        [{name,<<"UPDATE STATISTICS statement">>},
         {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28719,
        [{name,<<"ADVISE statement">>},
         {description,<<"A N1QL ADVISE statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28720,
        [{name,<<"START TRANSACTION statement">>},
         {description,<<"A N1QL START TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28721,
        [{name,<<"COMMIT TRANSACTION statement">>},
         {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28722,
        [{name,<<"ROLLBACK TRANSACTION statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28723,
        [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
         {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28724,
        [{name,<<"SET TRANSACTION ISOLATION statement">>},
         {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28725,
        [{name,<<"SAVEPOINT statement">>},
         {description,<<"A N1QL SAVEPOINT statement was executed">>},
         {enabled,false},
         {module,n1ql}]},
       {28726,
        [{name,<<"/admin/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28727,
        [{name,<<"/admin/indexes/transactions API request">>},
         {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
         {enabled,false},
         {module,n1ql}]},
       {28728,
        [{name,<<"N1QL backup / restore API request">>},
         {description,<<"An HTTP request was made to archive or restore N1QL metadata ">>},
         {enabled,false},
         {module,n1ql}]},
       {28729,
        [{name,<<"/admin/shutdown API request">>},
         {description,<<"An HTTP request was made to initate graceful shutdown">>},
         {enabled,false},
         {module,n1ql}]},
       {32768,
        [{name,<<"Create Function">>},
         {description,<<"Request to create or update eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32769,
        [{name,<<"Delete Function">>},
         {description,<<"Request to delete eventing function definition">>},
         {enabled,true},
         {module,eventing}]},
       {32770,
        [{name,<<"Fetch Functions">>},
         {description,<<"Request to fetch eventing function definition">>},
         {enabled,false},
         {module,eventing}]},
       {32771,
        [{name,<<"List Deployed">>},
         {description,<<"Request to fetch eventing deployed functions list">>},
         {enabled,false},
         {module,eventing}]},
       {32772,
        [{name,<<"Fetch Drafts">>},
         {description,<<"Request to fetch eventing function draft definitions">>},
         {enabled,false},
         {module,eventing}]},
       {32773,
        [{name,<<"Delete Drafts">>},
         {description,<<"Request to delete eventing function draft definitions">>},
         {enabled,true},
         {module,eventing}]},
       {32774,
        [{name,<<"Save Draft">>},
         {description,<<"Request to save a draft definition">>},
         {enabled,true},
         {module,eventing}]},
       {32775,
        [{name,<<"Start Debug">>},
         {description,<<"Request to start eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32776,
        [{name,<<"Stop Debug">>},
         {description,<<"Request to stop eventing function debugger">>},
         {enabled,true},
         {module,eventing}]},
       {32777,
        [{name,<<"Start Tracing">>},
         {description,<<"Request to start tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32778,
        [{name,<<"Stop Tracing">>},
         {description,<<"Request to stop tracing eventing function execution">>},
         {enabled,true},
         {module,eventing}]},
       {32779,
        [{name,<<"Set Settings">>},
         {description,<<"Request to save settings for an eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32780,
        [{name,<<"Fetch Config">>},
         {description,<<"Request to fetch eventing config">>},
         {enabled,false},
         {module,eventing}]},
       {32781,
        [{name,<<"Save Config">>},
         {description,<<"Request to save eventing config">>},
         {enabled,true},
         {module,eventing}]},
       {32783,
        [{name,<<"Get Settings">>},
         {description,<<"Request to fetch eventing function settings">>},
         {enabled,false},
         {module,eventing}]},
       {32784,
        [{name,<<"Import Functions">>},
         {description,<<"Request to import one or more eventing functions">>},
         {enabled,true},
         {module,eventing}]},
       {32785,
        [{name,<<"Export Functions">>},
         {description,<<"Request to export all eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32786,
        [{name,<<"List Running">>},
         {description,<<"Request to fetch eventing running function list">>},
         {enabled,false},
         {module,eventing}]},
       {32789,
        [{name,<<"Deploy Function">>},
         {description,<<"Request to deploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32790,
        [{name,<<"Undeploy Function">>},
         {description,<<"Request to undeploy eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32791,
        [{name,<<"Pause Function">>},
         {description,<<"Request to pause eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32792,
        [{name,<<"Resume Function">>},
         {description,<<"Request to resume eventing function">>},
         {enabled,true},
         {module,eventing}]},
       {32793,
        [{name,<<"Backup Functions">>},
         {description,<<"Request to backup one or more eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32794,
        [{name,<<"Restore Functions">>},
         {description,<<"Request to restore one or more eventing functions from a backup">>},
         {enabled,true},
         {module,eventing}]},
       {32795,
        [{name,<<"List Function">>},
         {description,<<"Request to fetch eventing functions">>},
         {enabled,false},
         {module,eventing}]},
       {32796,
        [{name,<<"Function Status">>},
         {description,<<"Request to fetch eventing function status">>},
         {enabled,false},
         {module,eventing}]},
       {32797,
        [{name,<<"Clear Stats">>},
         {description,<<"Request to reset eventing function stats">>},
         {enabled,true},
         {module,eventing}]},
       {32798,
        [{name,<<"Fetch Stats">>},
         {description,<<"Request to fetch eventing function stats">>},
         {enabled,false},
         {module,eventing}]},
       {32799,
        [{name,<<"Eventing Cluster Stats">>},
         {description,<<"Request to fetch eventing cluster stats">>},
         {enabled,false},
         {module,eventing}]},
       {32801,
        [{name,<<"Eventing System Event">>},
         {description,<<"Request to execute eventing node related functions">>},
         {enabled,false},
         {module,eventing}]},
       {32802,
        [{name,<<"Get User Info">>},
         {description,<<"Request to get user eventing permissions">>},
         {enabled,false},
         {module,eventing}]},
       {36865,
        [{name,<<"Service configuration change">>},
         {description,<<"A successful service configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36866,
        [{name,<<"Node configuration change">>},
         {description,<<"A successful node configuration change was made.">>},
         {enabled,true},
         {module,analytics}]},
       {36867,
        [{name,<<"SELECT statement">>},
         {description,<<"A N1QL SELECT statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36868,
        [{name,<<"CREATE DATAVERSE statement">>},
         {description,<<"A N1QL CREATE DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36869,
        [{name,<<"DROP DATAVERSE statement">>},
         {description,<<"A N1QL DROP DATAVERSE statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36870,
        [{name,<<"CREATE DATASET statement">>},
         {description,<<"A N1QL CREATE DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36871,
        [{name,<<"DROP DATASET statement">>},
         {description,<<"A N1QL DROP DATASET statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36872,
        [{name,<<"CREATE INDEX statement">>},
         {description,<<"A N1QL CREATE INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36873,
        [{name,<<"DROP INDEX statement">>},
         {description,<<"A N1QL DROP INDEX statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36877,
        [{name,<<"CONNECT LINK statement">>},
         {description,<<"A N1QL CONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36878,
        [{name,<<"DISCONNECT LINK statement">>},
         {description,<<"A N1QL DISCONNECT LINK statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {36879,
        [{name,<<"UNRECOGNIZED statement">>},
         {description,<<"An UNRECOGNIZED N1QL statement was encountered">>},
         {enabled,false},
         {module,analytics}]},
       {36880,
        [{name,<<"ALTER COLLECTION statement">>},
         {description,<<"A N1QL ALTER COLLECTION statement was executed">>},
         {enabled,false},
         {module,analytics}]},
       {40960,
        [{name,<<"Create Design Doc">>},
         {description,<<"Design Doc is Created">>},
         {enabled,true},
         {module,view_engine}]},
       {40961,
        [{name,<<"Delete Design Doc">>},
         {description,<<"Design Doc is Deleted">>},
         {enabled,true},
         {module,view_engine}]},
       {40962,
        [{name,<<"Query DDoc Meta Data">>},
         {description,<<"Design Doc Meta Data Query Request">>},
         {enabled,true},
         {module,view_engine}]},
       {40963,
        [{name,<<"View Query">>},
         {description,<<"View Query Request">>},
         {enabled,false},
         {module,view_engine}]},
       {40964,
        [{name,<<"Update Design Doc">>},
         {description,<<"Design Doc is Updated">>},
         {enabled,true},
         {module,view_engine}]},
       {40966,
        [{name,<<"Access denied">>},
         {description,<<"Access denied to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,view_engine}]},
       {45056,
        [{name,<<"Modify configuration">>},
         {description,<<"Backup service configuration was modified">>},
         {enabled,true},
         {module,backup}]},
       {45057,
        [{name,<<"Fetch configuration">>},
         {description,<<"Backup service configuration was retrieved">>},
         {enabled,false},
         {module,backup}]},
       {45058,
        [{name,<<"Add plan">>},
         {description,<<"A new backup plan was added">>},
         {enabled,true},
         {module,backup}]},
       {45059,
        [{name,<<"Modify plan">>},
         {description,<<"Existing backup plan was modified">>},
         {enabled,true},
         {module,backup}]},
       {45060,
        [{name,<<"Delete plan">>},
         {description,<<"A backup plan was removed">>},
         {enabled,true},
         {module,backup}]},
       {45061,
        [{name,<<"Fetch plan">>},
         {description,<<"One or more backup plans where fetched">>},
         {enabled,false},
         {module,backup}]},
       {45062,
        [{name,<<"Add repository">>},
         {description,<<"A new active backup repository was added">>},
         {enabled,true},
         {module,backup}]},
       {45063,
        [{name,<<"Archive repository">>},
         {description,<<"An active repository was archived">>},
         {enabled,true},
         {module,backup}]},
       {45064,
        [{name,<<"Pause repository">>},
         {description,<<"An active repository was paused">>},
         {enabled,true},
         {module,backup}]},
       {45065,
        [{name,<<"Resume repository">>},
         {description,<<"An active repository was resumed">>},
         {enabled,true},
         {module,backup}]},
       {45066,
        [{name,<<"Fetch repository">>},
         {description,<<"A repository was fetched">>},
         {enabled,false},
         {module,backup}]},
       {45067,
        [{name,<<"Restore repository">>},
         {description,<<"The repository data was restored">>},
         {enabled,true},
         {module,backup}]},
       {45068,
        [{name,<<"Backup repository">>},
         {description,<<"A manual backup was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45069,
        [{name,<<"Merge repository">>},
         {description,<<"A manual merge was triggered on an active repository">>},
         {enabled,true},
         {module,backup}]},
       {45070,
        [{name,<<"Info repository">>},
         {description,<<"Information about the structure and contents of the backup repository was fetched.">>},
         {enabled,false},
         {module,backup}]},
       {45071,
        [{name,<<"Examine repository">>},
         {description,<<"A document was retrieved from the repository backups">>},
         {enabled,true},
         {module,backup}]},
       {45072,
        [{name,<<"Delete repository">>},
         {description,<<"A repository was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45073,
        [{name,<<"Delete backup">>},
         {description,<<"An active repository backup was deleted">>},
         {enabled,true},
         {module,backup}]},
       {45074,
        [{name,<<"Access denied">>},
         {description,<<"A user has been denied access to the REST API due to invalid permissions or credentials">>},
         {enabled,true},
         {module,backup}]}]},
 {delete,rbac_upgrade},
 {set,auto_failover_cfg,
      [{enabled,true},
       {timeout,120},
       {count,0},
       {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
       {failover_server_group,false},
       {max_count,1},
       {failed_over_server_groups,[]},
       {can_abort_rebalance,true},
       {failover_preserve_durability_majority,false}]}]

[ns_server:debug,2023-08-29T18:14:57.165Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-08-29T18:14:57.166Z,ns_1@cb.local:roles_cache<0.366.0>:active_cache:clean:181]Clearing the roles_cache cache
[ns_server:debug,2023-08-29T18:14:57.167Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:versioned_cache:handle_info:89]Flushing cache compiled_roles_cache due to version change from {[7,0],
                                                                {0,1676588058},
                                                                {0,1676588058},
                                                                false,[]} to {[7,
                                                                               2],
                                                                              {0,
                                                                               1676588058},
                                                                              {0,
                                                                               1676588058},
                                                                              false,
                                                                              []}
[ns_server:debug,2023-08-29T18:14:57.167Z,ns_1@cb.local:<0.737.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:57.168Z,ns_1@cb.local:<0.737.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:14:57.169Z,ns_1@cb.local:memcached_passwords<0.435.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2023-08-29T18:14:57.178Z,ns_1@cb.local:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_apply_new_settings:601]Settings didn't change, ignoring update
[ns_server:debug,2023-08-29T18:14:57.178Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:maybe_store_ca_certs:726]Considering to store CA certs
[ns_server:debug,2023-08-29T18:14:57.179Z,ns_1@cb.local:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_apply_new_settings:601]Settings didn't change, ignoring update
[ns_server:debug,2023-08-29T18:14:57.180Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([audit_decriptors,auto_failover_cfg,
                               cluster_compat_version,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:14:57.181Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 22 us
[ns_server:debug,2023-08-29T18:14:57.182Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_call:149]Got full synchronization request from 'ns_1@cb.local'
[ns_server:debug,2023-08-29T18:14:57.182Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 8 us
[ns_server:debug,2023-08-29T18:14:57.182Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
audit_decriptors ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552097}}]},
 {8243,
  [{name,<<"mutate document">>},
   {description,<<"Document was mutated via the REST API">>},
   {enabled,true},
   {module,ns_server}]},
 {8255,
  [{name,<<"read document">>},
   {description,<<"Document was read via the REST API">>},
   {enabled,false},
   {module,ns_server}]},
 {8257,
  [{name,<<"alert email sent">>},
   {description,<<"An alert email was successfully sent">>},
   {enabled,true},
   {module,ns_server}]},
 {8265,
  [{name,<<"RBAC information retrieved">>},
   {description,<<"RBAC information was retrieved">>},
   {enabled,true},
   {module,ns_server}]},
 {20480,
  [{name,<<"opened DCP connection">>},
   {description,<<"opened DCP connection">>},
   {enabled,true},
   {module,memcached}]},
 {20482,
  [{name,<<"external memcached bucket flush">>},
   {description,<<"External user flushed the content of a memcached bucket">>},
   {enabled,true},
   {module,memcached}]},
 {20483,
  [{name,<<"invalid packet">>},
   {description,<<"Rejected an invalid packet">>},
   {enabled,true},
   {module,memcached}]},
 {20485,
  [{name,<<"authentication succeeded">>},
   {description,<<"Authentication to the cluster succeeded">>},
   {enabled,false},
   {module,memcached}]},
 {20488,
  [{name,<<"document read">>},
   {description,<<"Document was read">>},
   {enabled,false},
   {module,memcached}]},
 {20489,
  [{name,<<"document locked">>},
   {description,<<"Document was locked">>},
   {enabled,false},
   {module,memcached}]},
 {20490,
  [{name,<<"document modify">>},
   {description,<<"Document was modified">>},
   {enabled,false},
   {module,memcached}]},
 {20491,
  [{name,<<"document delete">>},
   {description,<<"Document was deleted">>},
   {enabled,false},
   {module,memcached}]},
 {20492,
  [{name,<<"select bucket">>},
   {description,<<"The specified bucket was selected">>},
   {enabled,true},
   {module,memcached}]},
 {20493,
  [{name,<<"session terminated">>},
   {description,<<"Session to the cluster has terminated">>},
   {enabled,false},
   {module,memcached}]},
 {20494,
  [{name,<<"tenant rate limited">>},
   {description,<<"The given tenant was rate limited">>},
   {enabled,true},
   {module,memcached}]},
 {28672,
  [{name,<<"SELECT statement">>},
   {description,<<"A N1QL SELECT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28673,
  [{name,<<"EXPLAIN statement">>},
   {description,<<"A N1QL EXPLAIN statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28674,
  [{name,<<"PREPARE statement">>},
   {description,<<"A N1QL PREPARE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28675,
  [{name,<<"INFER statement">>},
   {description,<<"A N1QL INFER statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28676,
  [{name,<<"INSERT statement">>},
   {description,<<"A N1QL INSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28677,
  [{name,<<"UPSERT statement">>},
   {description,<<"A N1QL UPSERT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28678,
  [{name,<<"DELETE statement">>},
   {description,<<"A N1QL DELETE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28679,
  [{name,<<"UPDATE statement">>},
   {description,<<"A N1QL UPDATE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28680,
  [{name,<<"MERGE statement">>},
   {description,<<"A N1QL MERGE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28681,
  [{name,<<"CREATE INDEX statement">>},
   {description,<<"A N1QL CREATE INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28682,
  [{name,<<"DROP INDEX statement">>},
   {description,<<"A N1QL DROP INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28683,
  [{name,<<"ALTER INDEX statement">>},
   {description,<<"A N1QL ALTER INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28684,
  [{name,<<"BUILD INDEX statement">>},
   {description,<<"A N1QL BUILD INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28685,
  [{name,<<"GRANT ROLE statement">>},
   {description,<<"A N1QL GRANT ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28686,
  [{name,<<"REVOKE ROLE statement">>},
   {description,<<"A N1QL REVOKE ROLE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28687,
  [{name,<<"UNRECOGNIZED statement">>},
   {description,<<"An unrecognized statement was received by the N1QL query engine">>},
   {enabled,false},
   {module,n1ql}]},
 {28688,
  [{name,<<"CREATE PRIMARY INDEX statement">>},
   {description,<<"A N1QL CREATE PRIMARY INDEX statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28689,
  [{name,<<"/admin/stats API request">>},
   {description,<<"An HTTP request was made to the API at /admin/stats.">>},
   {enabled,false},
   {module,n1ql}]},
 {28690,
  [{name,<<"/admin/vitals API request">>},
   {description,<<"An HTTP request was made to the API at /admin/vitals.">>},
   {enabled,false},
   {module,n1ql}]},
 {28691,
  [{name,<<"/admin/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28692,
  [{name,<<"/admin/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28693,
  [{name,<<"/admin/indexes/prepareds API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/prepareds.">>},
   {enabled,false},
   {module,n1ql}]},
 {28694,
  [{name,<<"/admin/indexes/active_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/active_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28695,
  [{name,<<"/admin/indexes/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28697,
  [{name,<<"/admin/ping API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ping.">>},
   {enabled,false},
   {module,n1ql}]},
 {28698,
  [{name,<<"/admin/config API request">>},
   {description,<<"An HTTP request was made to the API at /admin/config.">>},
   {enabled,false},
   {module,n1ql}]},
 {28699,
  [{name,<<"/admin/ssl_cert API request">>},
   {description,<<"An HTTP request was made to the API at /admin/ssl_cert.">>},
   {enabled,false},
   {module,n1ql}]},
 {28700,
  [{name,<<"/admin/settings API request">>},
   {description,<<"An HTTP request was made to the API at /admin/settings.">>},
   {enabled,false},
   {module,n1ql}]},
 {28701,
  [{name,<<"/admin/clusters API request">>},
   {description,<<"An HTTP request was made to the API at /admin/clusters.">>},
   {enabled,false},
   {module,n1ql}]},
 {28702,
  [{name,<<"/admin/completed_requests API request">>},
   {description,<<"An HTTP request was made to the API at /admin/completed_requests.">>},
   {enabled,false},
   {module,n1ql}]},
 {28704,
  [{name,<<"/admin/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28705,
  [{name,<<"/admin/indexes/functions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/functions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28706,
  [{name,<<"CREATE FUNCTION statement">>},
   {description,<<"A N1QL CREATE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28707,
  [{name,<<"DROP FUNCTION statement">>},
   {description,<<"A N1QL DROP FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28708,
  [{name,<<"EXECUTE FUNCTION statement">>},
   {description,<<"A N1QL EXECUTE FUNCTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28709,
  [{name,<<"/admin/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28710,
  [{name,<<"/admin/indexes/tasks API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/tasks.">>},
   {enabled,false},
   {module,n1ql}]},
 {28711,
  [{name,<<"/admin/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28712,
  [{name,<<"/admin/indexes/dictionary_cache API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/dictionary_cache.">>},
   {enabled,false},
   {module,n1ql}]},
 {28713,
  [{name,<<"CREATE SCOPE statement">>},
   {description,<<"A N1QL CREATE SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28714,
  [{name,<<"DROP SCOPE statement">>},
   {description,<<"A N1QL DROP SCOPE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28715,
  [{name,<<"CREATE COLLECTION statement">>},
   {description,<<"A N1QL CREATE COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28716,
  [{name,<<"DROP COLLECTION statement">>},
   {description,<<"A N1QL DROP COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28717,
  [{name,<<"FLUSH COLLECTION statement">>},
   {description,<<"A N1QL FLUSH COLLECTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28718,
  [{name,<<"UPDATE STATISTICS statement">>},
   {description,<<"A N1QL UPDATE STATISTICS statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28719,
  [{name,<<"ADVISE statement">>},
   {description,<<"A N1QL ADVISE statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28720,
  [{name,<<"START TRANSACTION statement">>},
   {description,<<"A N1QL START TRANSACTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28721,
  [{name,<<"COMMIT TRANSACTION statement">>},
   {description,<<"A N1QL COMMIT TRANSACTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28722,
  [{name,<<"ROLLBACK TRANSACTION statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28723,
  [{name,<<"ROLLBACK TRANSACTION TO SAVEPOINT statement">>},
   {description,<<"A N1QL ROLLBACK TRANSACTION TO SAVEPOINT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28724,
  [{name,<<"SET TRANSACTION ISOLATION statement">>},
   {description,<<"A N1QL SET TRANSACTION ISOLATION statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28725,
  [{name,<<"SAVEPOINT statement">>},
   {description,<<"A N1QL SAVEPOINT statement was executed">>},
   {enabled,false},
   {module,n1ql}]},
 {28726,
  [{name,<<"/admin/transactions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/transactions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28727,
  [{name,<<"/admin/indexes/transactions API request">>},
   {description,<<"An HTTP request was made to the API at /admin/indexes/transactions.">>},
   {enabled,false},
   {module,n1ql}]},
 {28728,
  [{name,<<"N1QL backup / restore API request">>},
   {description,<<"An HTTP request was made to archive or restore N1QL metadata ">>},
   {enabled,false},
   {module,n1ql}]},
 {28729,
  [{name,<<"/admin/shutdown API request">>},
   {description,<<"An HTTP request was made to initate graceful shutdown">>},
   {enabled,false},
   {module,n1ql}]},
 {32768,
  [{name,<<"Create Function">>},
   {description,<<"Request to create or update eventing function definition">>},
   {enabled,true},
   {module,eventing}]},
 {32769,
  [{name,<<"Delete Function">>},
   {description,<<"Request to delete eventing function definition">>},
   {enabled,true},
   {module,eventing}]},
 {32770,
  [{name,<<"Fetch Functions">>},
   {description,<<"Request to fetch eventing function definition">>},
   {enabled,false},
   {module,eventing}]},
 {32771,
  [{name,<<"List Deployed">>},
   {description,<<"Request to fetch eventing deployed functions list">>},
   {enabled,false},
   {module,eventing}]},
 {32772,
  [{name,<<"Fetch Drafts">>},
   {description,<<"Request to fetch eventing function draft definitions">>},
   {enabled,false},
   {module,eventing}]},
 {32773,
  [{name,<<"Delete Drafts">>},
   {description,<<"Request to delete eventing function draft definitions">>},
   {enabled,true},
   {module,eventing}]},
 {32774,
  [{name,<<"Save Draft">>},
   {description,<<"Request to save a draft definition">>},
   {enabled,true},
   {module,eventing}]},
 {32775,
  [{name,<<"Start Debug">>},
   {description,<<"Request to start eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32776,
  [{name,<<"Stop Debug">>},
   {description,<<"Request to stop eventing function debugger">>},
   {enabled,true},
   {module,eventing}]},
 {32777,
  [{name,<<"Start Tracing">>},
   {description,<<"Request to start tracing eventing function e"...>>},
   {enabled,true},
   {module,eventing}]},
 {32778,
  [{name,<<"Stop Tracing">>},
   {description,<<"Request to stop tracing eventing functio"...>>},
   {enabled,true},
   {module,eventing}]},
 {32779,
  [{name,<<"Set Settings">>},
   {description,<<"Request to save settings for an even"...>>},
   {enabled,true},
   {module,eventing}]},
 {32780,
  [{name,<<"Fetch Config">>},
   {description,<<"Request to fetch eventing config">>},
   {enabled,false},
   {module,eventing}]},
 {32781,
  [{name,<<"Save Config">>},
   {description,<<"Request to save eventing con"...>>},
   {enabled,true},
   {module,eventing}]},
 {32783,
  [{name,<<"Get Settings">>},
   {description,<<"Request to fetch eventin"...>>},
   {enabled,false},
   {module,eventing}]},
 {32784,
  [{name,<<"Import Functions">>},
   {description,<<"Request to import on"...>>},
   {enabled,true},
   {module,eventing}]},
 {32785,
  [{name,<<"Export Functions">>},
   {description,<<"Request to expor"...>>},
   {enabled,false},
   {module,eventing}]},
 {32786,
  [{name,<<"List Running">>},
   {description,<<"Request to f"...>>},
   {enabled,false},
   {module,eventing}]},
 {32789,
  [{name,<<"Deploy Funct"...>>},
   {description,<<"Request "...>>},
   {enabled,true},
   {module,eventing}]},
 {32790,
  [{name,<<"Undeploy"...>>},
   {description,<<"Requ"...>>},
   {enabled,true},
   {module,...}]},
 {32791,[{name,<<"Paus"...>>},{description,<<...>>},{enabled,...},{...}]},
 {32792,[{name,<<...>>},{description,...},{...}|...]},
 {32793,[{name,...},{...}|...]},
 {32794,[{...}|...]},
 {32795,[...]},
 {32796,...},
 {...}|...]
[user:warn,2023-08-29T18:14:57.182Z,ns_1@cb.local:compat_mode_manager<0.715.0>:compat_mode_manager:handle_consider_switching_compat_mode:43]Changed cluster compat mode from undefined to [7,2]
[error_logger:info,2023-08-29T18:14:57.183Z,ns_1@cb.local:ns_orchestrator_sup<0.713.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.715.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:57.183Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_compat_version ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{5,63860552097}}]},7,2]
[ns_server:warn,2023-08-29T18:14:57.186Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:57.187Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-08-29T18:14:57.183Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552097}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {max_count,1},
 {failed_over_server_groups,[]},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[ns_server:debug,2023-08-29T18:14:57.190Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552097}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:14:57.190Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{9,63860552097}}]}]
[ns_server:warn,2023-08-29T18:14:57.192Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:57.193Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:warn,2023-08-29T18:14:57.194Z,ns_1@cb.local:<0.740.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet6,{error,eaddrnotavail}},
                                     {inet,{error,econnrefused}}]}}.
[error_logger:error,2023-08-29T18:14:57.195Z,ns_1@cb.local:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]Error in process <0.740.0> on node 'ns_1@cb.local' with exit value:
{{badmatch,{error,couldnt_connect_to_memcached}},
 [{ns_memcached,'-config_validate/2-fun-0-',2,
                [{file,"src/ns_memcached.erl"},{line,1591}]},
  {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,191}]}]}

[ns_server:debug,2023-08-29T18:14:57.196Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2023-08-29T18:14:57.196Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2023-08-29T18:14:57.201Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[error_logger:error,2023-08-29T18:14:57.211Z,ns_1@cb.local:memcached_config_mgr<0.604.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================
** Generic server <0.604.0> terminating 
** Last message in was do_check
** When Server state == {state,<16118.135.0>,
                               <<"{\n  \"active_external_users_push_interval\": 600,\n  \"admin\": \"@ns_server\",\n  \"audit_file\": \"/opt/couchbase/var/lib/couchbase/config/audit.json\",\n  \"breakpad\": {\n    \"enabled\": true,\n    "...>>,
                               undefined}
** Reason for termination ==
** {{badmatch,{error,couldnt_connect_to_memcached}},
    [{ns_memcached,'-config_validate/2-fun-0-',2,
                   [{file,"src/ns_memcached.erl"},{line,1591}]},
     {async,'-async_init/4-fun-1-',3,[{file,"src/async.erl"},{line,191}]}]}

[error_logger:info,2023-08-29T18:14:57.217Z,ns_1@cb.local:ns_orchestrator_child_sup<0.754.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.755.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:57.227Z,ns_1@cb.local:memcached_permissions<0.440.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[error_logger:error,2023-08-29T18:14:57.235Z,ns_1@cb.local:memcached_config_mgr<0.604.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: memcached_config_mgr:init/1
    pid: <0.604.0>
    registered_name: memcached_config_mgr
    exception error: no match of right hand side value {error,
                                                        couldnt_connect_to_memcached}
      in function  ns_memcached:'-config_validate/2-fun-0-'/2 (src/ns_memcached.erl, line 1591)
      in call from async:'-async_init/4-fun-1-'/3 (src/async.erl, line 191)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.270.0>,
                  ns_server_cluster_sup,root_sup,<0.146.0>]
    message_queue_len: 3
    messages: [upload_tls_config,do_check,do_check]
    links: [<0.407.0>,<0.710.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 6772
    stack_size: 29
    reductions: 60253
  neighbours:

[ns_server:debug,2023-08-29T18:14:57.236Z,ns_1@cb.local:<0.710.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.604.0>} exited with reason {{badmatch,
                                                                                 {error,
                                                                                  couldnt_connect_to_memcached}},
                                                                                [{ns_memcached,
                                                                                  '-config_validate/2-fun-0-',
                                                                                  2,
                                                                                  [{file,
                                                                                    "src/ns_memcached.erl"},
                                                                                   {line,
                                                                                    1591}]},
                                                                                 {async,
                                                                                  '-async_init/4-fun-1-',
                                                                                  3,
                                                                                  [{file,
                                                                                    "src/async.erl"},
                                                                                   {line,
                                                                                    191}]}]}
[ns_server:debug,2023-08-29T18:14:57.236Z,ns_1@cb.local:<0.709.0>:remote_monitors:handle_down:151]Caller of remote monitor <0.604.0> died with {{badmatch,
                                               {error,
                                                couldnt_connect_to_memcached}},
                                              [{ns_memcached,
                                                '-config_validate/2-fun-0-',2,
                                                [{file,"src/ns_memcached.erl"},
                                                 {line,1591}]},
                                               {async,'-async_init/4-fun-1-',
                                                3,
                                                [{file,"src/async.erl"},
                                                 {line,191}]}]}. Exiting
[ns_server:debug,2023-08-29T18:14:57.250Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:warn,2023-08-29T18:14:57.252Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:57.252Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-08-29T18:14:57.252Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[ns_server:debug,2023-08-29T18:14:57.253Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:info,2023-08-29T18:14:57.258Z,ns_1@cb.local:ns_orchestrator_child_sup<0.754.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.756.0> on 'ns_1@cb.local'

[error_logger:info,2023-08-29T18:14:57.259Z,ns_1@cb.local:ns_orchestrator_child_sup<0.754.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.756.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:57.276Z,ns_1@cb.local:ns_orchestrator_child_sup<0.754.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.757.0> on 'ns_1@cb.local'

[error_logger:info,2023-08-29T18:14:57.277Z,ns_1@cb.local:ns_orchestrator_child_sup<0.754.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.757.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:14:57.277Z,ns_1@cb.local:ns_orchestrator_child_sup<0.754.0>:misc:start_singleton:901]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.758.0> on 'ns_1@cb.local'

[error_logger:info,2023-08-29T18:14:57.278Z,ns_1@cb.local:ns_orchestrator_child_sup<0.754.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.758.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:57.278Z,ns_1@cb.local:ns_orchestrator_sup<0.713.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.754.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:57.278Z,ns_1@cb.local:<0.760.0>:auto_failover:init:198]init auto_failover.
[user:info,2023-08-29T18:14:57.278Z,ns_1@cb.local:<0.760.0>:auto_failover:handle_call:229]Enabled auto-failover with timeout 120 and max count 1
[ns_server:debug,2023-08-29T18:14:57.284Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2023-08-29T18:14:57.284Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-08-29T18:14:57.284Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:warn,2023-08-29T18:14:57.298Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:57.298Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:info,2023-08-29T18:14:57.303Z,ns_1@cb.local:ns_orchestrator_sup<0.713.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.760.0> on 'ns_1@cb.local'

[ns_server:debug,2023-08-29T18:14:57.303Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{10,63860552097}}]}]
[ns_server:debug,2023-08-29T18:14:57.303Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
auto_failover_cfg ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552097}}]},
 {enabled,true},
 {timeout,120},
 {count,0},
 {max_count,1},
 {failed_over_server_groups,[]},
 {failover_on_data_disk_issues,[{enabled,false},{timePeriod,120}]},
 {failover_server_group,false},
 {can_abort_rebalance,true},
 {failover_preserve_durability_majority,false}]
[error_logger:info,2023-08-29T18:14:57.303Z,ns_1@cb.local:ns_orchestrator_sup<0.713.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.760.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:warn,2023-08-29T18:14:57.304Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[error_logger:info,2023-08-29T18:14:57.304Z,ns_1@cb.local:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.713.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:57.304Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-08-29T18:14:57.304Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([auto_failover_cfg,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:14:57.305Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:info,2023-08-29T18:14:57.314Z,ns_1@cb.local:mb_master_sup<0.693.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.768.0> on 'ns_1@cb.local'

[error_logger:info,2023-08-29T18:14:57.315Z,ns_1@cb.local:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.768.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:14:57.321Z,ns_1@cb.local:memcached_permissions<0.440.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{user,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2023-08-29T18:14:57.334Z,ns_1@cb.local:<0.771.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2023-08-29T18:14:57.334Z,ns_1@cb.local:mb_master_sup<0.693.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.771.0> on 'ns_1@cb.local'

[error_logger:info,2023-08-29T18:14:57.335Z,ns_1@cb.local:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.771.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:57.335Z,ns_1@cb.local:leader_registry_sup<0.688.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.691.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:57.336Z,ns_1@cb.local:leader_services_sup<0.679.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.688.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:57.336Z,ns_1@cb.local:<0.678.0>:restartable:start_child:92]Started child process <0.679.0>
  MFA: {leader_services_sup,start_link,[]}
[error_logger:info,2023-08-29T18:14:57.336Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.678.0>},
              {name,leader_services_sup},
              {mfargs,{restartable,start_link,
                                   [{leader_services_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2023-08-29T18:14:57.341Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac" is requested
[error_logger:info,2023-08-29T18:14:57.359Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.773.0>},
              {name,ns_tick_agent},
              {mfargs,{ns_tick_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:57.360Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.775.0>},
              {name,master_activity_events_ingress},
              {mfargs,{gen_event,start_link,
                                 [{local,master_activity_events_ingress}]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:57.360Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.776.0>},
              {name,master_activity_events_timestamper},
              {mfargs,{master_activity_events,start_link_timestamper,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:57.374Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.777.0>},
              {name,master_activity_events_pids_watcher},
              {mfargs,{master_activity_events_pids_watcher,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:57.383Z,ns_1@cb.local:memcached_permissions<0.440.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/config/memcached.rbac.tmp" to "/opt/couchbase/var/lib/couchbase/config/memcached.rbac"
[ns_server:debug,2023-08-29T18:14:57.384Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of rbac requested
[ns_server:debug,2023-08-29T18:14:57.386Z,ns_1@cb.local:memcached_passwords<0.435.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:warn,2023-08-29T18:14:57.386Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:57.386Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[error_logger:info,2023-08-29T18:14:57.390Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.778.0>},
              {name,master_activity_events_keeper},
              {mfargs,{master_activity_events_keeper,start_link,[]}},
              {restart_type,permanent},
              {shutdown,brutal_kill},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:57.392Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2023-08-29T18:14:57.421Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-08-29T18:14:57.421Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:warn,2023-08-29T18:14:57.424Z,ns_1@cb.local:memcached_refresh<0.283.0>:ns_memcached:connect:1246]Unable to connect: {error,{badmatch,[{inet,{error,econnrefused}}]}}.
[ns_server:debug,2023-08-29T18:14:57.424Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:93]Refresh of [rbac,isasl] failed. Retry in 1000 ms.
[ns_server:debug,2023-08-29T18:14:57.433Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:write_cfg:148]Writing config file for: "/opt/couchbase/var/lib/couchbase/isasl.pw"
[error_logger:info,2023-08-29T18:14:57.433Z,ns_1@cb.local:health_monitor_sup<0.780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.781.0>},
              {id,ns_server_monitor},
              {mfargs,{ns_server_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:57.434Z,ns_1@cb.local:health_monitor_sup<0.780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.783.0>},
              {id,service_monitor_children_sup},
              {mfargs,{supervisor,start_link,
                                  [{local,service_monitor_children_sup},
                                   health_monitor_sup,child]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:57.435Z,ns_1@cb.local:health_monitor_sup<0.780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.784.0>},
              {id,service_monitor_worker},
              {mfargs,{erlang,apply,[#Fun<health_monitor_sup.0.70530162>,[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:57.453Z,ns_1@cb.local:health_monitor_sup<0.780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.790.0>},
              {id,node_monitor},
              {mfargs,{node_monitor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:57.478Z,ns_1@cb.local:health_monitor_sup<0.780.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,health_monitor_sup}
    started: [{pid,<0.796.0>},
              {id,node_status_analyzer},
              {mfargs,{node_status_analyzer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:57.478Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.780.0>},
              {name,health_monitor_sup},
              {mfargs,{health_monitor_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[error_logger:info,2023-08-29T18:14:57.491Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.798.0>},
              {name,rebalance_agent},
              {mfargs,{rebalance_agent,start_link,[]}},
              {restart_type,permanent},
              {shutdown,5000},
              {child_type,worker}]
[error_logger:info,2023-08-29T18:14:57.516Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.799.0>},
              {name,ns_rebalance_report_manager},
              {mfargs,{ns_rebalance_report_manager,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:error,2023-08-29T18:14:57.517Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {{badmatch,{error,couldnt_connect_to_memcached}},
             [{ns_memcached,'-config_validate/2-fun-0-',2,
                            [{file,"src/ns_memcached.erl"},{line,1591}]},
              {async,'-async_init/4-fun-1-',3,
                     [{file,"src/async.erl"},{line,191}]}]}
    offender: [{pid,<0.604.0>},
               {name,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,{permanent,4}},
               {shutdown,1000},
               {child_type,worker}]
[error_logger:info,2023-08-29T18:14:57.517Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_nodes_sup}
    started: [{pid,<0.407.0>},
              {name,ns_server_sup},
              {mfargs,{ns_server_sup,start_link,[]}},
              {restart_type,permanent},
              {shutdown,infinity},
              {child_type,supervisor}]
[ns_server:debug,2023-08-29T18:14:57.517Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:one_shot_barrier:notify:21]Notifying on barrier menelaus_barrier
[ns_server:debug,2023-08-29T18:14:57.517Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:memcached_port_pid:151]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-08-29T18:14:57.517Z,ns_1@cb.local:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.801.0>},
              {name,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:14:57.517Z,ns_1@cb.local:menelaus_barrier<0.278.0>:one_shot_barrier:barrier_body:56]Barrier menelaus_barrier got notification from <0.271.0>
[ns_server:debug,2023-08-29T18:14:57.517Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:memcached_port_pid:153]ns_ports_setup seems to be ready
[ns_server:debug,2023-08-29T18:14:57.518Z,ns_1@cb.local:ns_server_nodes_sup<0.271.0>:one_shot_barrier:notify:26]Successfuly notified on barrier menelaus_barrier
[ns_server:debug,2023-08-29T18:14:57.518Z,ns_1@cb.local:<0.270.0>:restartable:start_child:92]Started child process <0.271.0>
  MFA: {ns_server_nodes_sup,start_link,[]}
[error_logger:info,2023-08-29T18:14:57.518Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.270.0>},
              {id,ns_server_nodes_sup},
              {mfargs,{restartable,start_link,
                                   [{ns_server_nodes_sup,start_link,[]},
                                    infinity]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:14:57.528Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:find_port_pid_loop:161]Found memcached port <16118.135.0>
[ns_server:debug,2023-08-29T18:14:57.531Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:menelaus_roles:build_compiled_roles:1062]Compile roles for user {"@",admin}
[ns_server:debug,2023-08-29T18:14:57.532Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:menelaus_roles:build_compiled_roles:1062]Compile roles for user {"@ns_server",admin}
[ns_server:debug,2023-08-29T18:14:57.533Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:do_read_current_memcached_config:368]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2023-08-29T18:14:57.541Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:init:99]found memcached port to be already active
[error_logger:info,2023-08-29T18:14:57.543Z,ns_1@cb.local:ns_server_cluster_sup<0.218.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_cluster_sup}
    started: [{pid,<0.804.0>},
              {id,remote_api},
              {mfargs,{remote_api,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:14:57.543Z,ns_1@cb.local:root_sup<0.196.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,root_sup}
    started: [{pid,<0.218.0>},
              {id,ns_server_cluster_sup},
              {mfargs,{ns_server_cluster_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:14:57.544Z,ns_1@cb.local:application_controller<0.44.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    application: ns_server
    started_at: 'ns_1@cb.local'

[ns_server:debug,2023-08-29T18:14:57.544Z,ns_1@cb.local:<0.9.0>:child_erlang:child_loop:128]159: Entered child_loop
[ns_server:debug,2023-08-29T18:14:57.551Z,ns_1@cb.local:memcached_passwords<0.435.0>:replicated_dets:select_from_table:281][ets] Starting select with {users_storage,
                               [{{docv2,{auth,{'_',local}},'_','_'},
                                 [],
                                 ['$_']}],
                               100}
[ns_server:debug,2023-08-29T18:14:57.555Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_call:49]File rename from "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw" is requested
[ns_server:debug,2023-08-29T18:14:57.570Z,ns_1@cb.local:memcached_passwords<0.435.0>:memcached_cfg:rename_and_refresh:170]Successfully renamed "/opt/couchbase/var/lib/couchbase/isasl.pw.tmp" to "/opt/couchbase/var/lib/couchbase/isasl.pw"
[ns_server:debug,2023-08-29T18:14:57.570Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_cast:55]Refresh of isasl requested
[ns_server:debug,2023-08-29T18:14:57.612Z,ns_1@cb.local:json_rpc_connection-saslauthd-saslauthd-port<0.807.0>:json_rpc_connection:init:68]Observed revrpc connection: label "saslauthd-saslauthd-port", handling process <0.807.0>
[ns_server:debug,2023-08-29T18:14:57.612Z,ns_1@cb.local:json_rpc_connection-goxdcr-cbauth<0.808.0>:json_rpc_connection:init:68]Observed revrpc connection: label "goxdcr-cbauth", handling process <0.808.0>
[ns_server:debug,2023-08-29T18:14:57.613Z,ns_1@cb.local:menelaus_cbauth<0.575.0>:menelaus_cbauth:handle_cast:101]Observed json rpc process {"goxdcr-cbauth",<0.808.0>} started
[ns_server:debug,2023-08-29T18:14:57.619Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:apply_changed_memcached_config:259]New memcached config is hot-reloadable.
[ns_server:debug,2023-08-29T18:14:57.622Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:do_read_current_memcached_config:368]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2023-08-29T18:14:57.688Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:menelaus_roles:build_compiled_roles:1062]Compile roles for user {"@goxdcr",admin}
[ns_server:debug,2023-08-29T18:14:57.711Z,ns_1@cb.local:memcached_refresh<0.283.0>:memcached_refresh:handle_info:89]Refresh of [rbac,isasl] succeeded
[user:info,2023-08-29T18:14:57.766Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:hot_reload_config:328]Hot-reloaded memcached.json for config change of the following keys: [<<"collections_enabled">>]
[ns_server:info,2023-08-29T18:14:57.768Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:push_tls_config:230]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2023-08-29T18:14:57.806Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:push_tls_config:234]Successfully pushed TLS config to memcached
[ns_server:debug,2023-08-29T18:14:57.934Z,ns_1@cb.local:<0.613.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":14,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2023-08-29T18:14:58.280Z,ns_1@cb.local:<0.760.0>:auto_failover_logic:log_master_activity:141]Transitioned node {'ns_1@cb.local',<<"a2acfe035c01f55c0d78fce935ad3d90">>} state new -> up
[ns_server:debug,2023-08-29T18:15:01.557Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1017]Going to notify following services: [memcached]
[ns_server:info,2023-08-29T18:15:01.558Z,ns_1@cb.local:<0.964.0>:ns_ssl_services_setup:notify_service:1053]Successfully notified service memcached
[ns_server:info,2023-08-29T18:15:01.559Z,ns_1@cb.local:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:notify_services:1033]Succesfully notified services [memcached]
[ns_server:info,2023-08-29T18:15:01.560Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:push_tls_config:230]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2023-08-29T18:15:01.600Z,ns_1@cb.local:memcached_config_mgr<0.801.0>:memcached_config_mgr:push_tls_config:234]Successfully pushed TLS config to memcached
[ns_server:debug,2023-08-29T18:15:05.519Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:menelaus_roles:build_compiled_roles:1062]Compile roles for user {"@prometheus",stats_reader}
[ns_server:debug,2023-08-29T18:15:26.745Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:15:26.745Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:15:26.746Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:15:26.747Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[error_logger:info,2023-08-29T18:15:45.198Z,ns_1@cb.local:alarm_handler<0.130.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
    alarm_handler: {set,{system_memory_high_watermark,[]}}
[ns_server:debug,2023-08-29T18:15:56.747Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:15:56.747Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:15:56.748Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:15:56.749Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:15:56.993Z,ns_1@cb.local:<0.706.0>:chronicle_master:do_handle_info:296]Successful after upgrade cleanup
[ns_server:debug,2023-08-29T18:15:56.993Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
nodes_wanted ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552156}}]}]
[ns_server:debug,2023-08-29T18:15:56.993Z,ns_1@cb.local:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-08-29T18:15:56.993Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
server_groups ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552156}}]}|
 '_deleted']
[ns_server:debug,2023-08-29T18:15:56.993Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@cb.local',membership} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552156}}]}|
 '_deleted']
[ns_server:debug,2023-08-29T18:15:56.994Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([after_upgrade_cleanup,auto_reprovision_cfg,
                               counters,nodes_wanted,server_groups,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {node,'ns_1@cb.local',membership}]..)
[ns_server:debug,2023-08-29T18:15:56.994Z,ns_1@cb.local:<0.2858.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                      <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:15:56.994Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
auto_reprovision_cfg ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552156}}]}|
 '_deleted']
[ns_server:debug,2023-08-29T18:15:56.994Z,ns_1@cb.local:<0.2858.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@cb.local'], with cookie: {sanitized,
                                                                   <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:15:56.994Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
counters ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552156}}]}|
 '_deleted']
[ns_server:debug,2023-08-29T18:15:56.994Z,ns_1@cb.local:<0.613.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":18,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2023-08-29T18:15:56.995Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
after_upgrade_cleanup ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552156}}]}|
 '_deleted']
[ns_server:debug,2023-08-29T18:15:56.995Z,ns_1@cb.local:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{11,63860552156}}]}]
[ns_server:debug,2023-08-29T18:16:06.480Z,ns_1@cb.local:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:16:26.748Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:16:26.749Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:16:26.749Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:16:26.749Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:16:56.750Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:16:56.750Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:16:56.751Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:16:56.751Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:17:21.480Z,ns_1@cb.local:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:17:26.752Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:17:26.752Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:17:26.752Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:17:26.752Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:17:56.753Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:17:56.753Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:17:56.754Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:17:56.754Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:18:26.755Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:18:26.756Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:18:26.756Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:18:26.757Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:18:36.482Z,ns_1@cb.local:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[error_logger:info,2023-08-29T18:18:45.268Z,ns_1@cb.local:alarm_handler<0.130.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
    alarm_handler: {clear,system_memory_high_watermark}
[ns_server:debug,2023-08-29T18:18:52.592Z,ns_1@cb.local:compiled_roles_cache<0.363.0>:menelaus_roles:build_compiled_roles:1062]Compile roles for user {[],wrong_token}
[ns_server:debug,2023-08-29T18:18:56.757Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:18:56.758Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:18:56.759Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:18:56.759Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:19:26.759Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:19:26.759Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:19:26.761Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:19:26.761Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:19:51.483Z,ns_1@cb.local:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:19:56.761Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:19:56.761Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:19:56.762Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:19:56.762Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:20:26.762Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:20:26.763Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:20:26.763Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:20:26.763Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:20:56.764Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:20:56.764Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:20:56.765Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:20:56.765Z,ns_1@cb.local:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:20:57.366Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: ns_config_vclock_ts, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                        9})
63860552157
[ns_server:debug,2023-08-29T18:20:57.366Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 18 us
[ns_server:debug,2023-08-29T18:20:57.366Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,9})
'ns_1@cb.local'
[ns_server:debug,2023-08-29T18:20:57.366Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 10 us
[ns_server:debug,2023-08-29T18:20:57.368Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([after_upgrade_cleanup,auto_reprovision_cfg,
                               counters,server_groups,
                               {node,'ns_1@cb.local',membership}]..)
[ns_server:debug,2023-08-29T18:20:57.371Z,ns_1@cb.local:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 994 us
[ns_server:debug,2023-08-29T18:20:57.404Z,ns_1@cb.local:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: ns_config_purge_ts, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                       10})
63860552157
[ns_server:debug,2023-08-29T18:20:57.405Z,ns_1@cb.local:tombstone_agent<0.469.0>:tombstone_agent:purge:195]Purged 5 ns_config tombstone(s) up to timestamp 63860552157. Tombstones:
[after_upgrade_cleanup,counters,auto_reprovision_cfg,{node,'ns_1@cb.local',membership},server_groups]
[ns_server:debug,2023-08-29T18:21:06.485Z,ns_1@cb.local:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[cluster:info,2023-08-29T18:21:14.767Z,ns_1@cb.local:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:21:14.767Z,ns_1@cb.local:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:debug,2023-08-29T18:21:14.768Z,ns_1@cb.local:ns_node_disco<0.450.0>:ns_node_disco:maybe_monitor_rename_txn:215]Monitor node renaming transaction. Pid = <0.13768.0>, MRef = #Ref<0.4274179209.2407792641.232339>
[ns_server:debug,2023-08-29T18:21:14.770Z,ns_1@cb.local:remote_monitors<0.277.0>:remote_monitors:maybe_monitor_rename_txn:159]Monitor node renaming transaction. Pid = <0.13768.0>, MRef = #Ref<0.4274179209.2407792642.227918>
[ns_server:debug,2023-08-29T18:21:14.771Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Closing listener {external,inet_tcp_dist}
[ns_server:debug,2023-08-29T18:21:14.772Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Full list of processes expected to stop: [<0.214.0>]
[ns_server:debug,2023-08-29T18:21:14.772Z,ns_1@cb.local:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Down from <0.214.0>
[error_logger:info,2023-08-29T18:21:14.774Z,ns_1@cb.local:net_kernel<0.212.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,858,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:21:14.775Z,ns_1@cb.local:<0.421.0>:misc:delaying_crash:1736]Delaying crash exit:{{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}} by 1000ms
Stacktrace: [{gen_server,call,3,[{file,"gen_server.erl"},{line,247}]},
             {ns_log,babysitter_log_consumption_loop,0,
                     [{file,"src/ns_log.erl"},{line,66}]},
             {misc,delaying_crash,2,[{file,"src/misc.erl"},{line,1734}]},
             {proc_lib,init_p,3,[{file,"proc_lib.erl"},{line,211}]}]
[ns_server:debug,2023-08-29T18:21:14.777Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.218432>,
                               inet_tcp_dist,<0.216.0>,
                               #Ref<0.4274179209.2407792641.218434>}
[user:warn,2023-08-29T18:21:14.777Z,nonode@nohost:ns_node_disco<0.450.0>:ns_node_disco:handle_info:183]Node nonode@nohost saw that node 'ns_1@cb.local' went down. Details: [{nodedown_reason,
                                                                       net_kernel_terminated}]
[chronicle:info,2023-08-29T18:21:14.777Z,nonode@nohost:chronicle_proposer<0.282.0>:chronicle_proposer:handle_nodedown:1135]Peer 'ns_1@cb.local' went down: [{nodedown_reason,net_kernel_terminated}]
[ns_server:debug,2023-08-29T18:21:14.778Z,nonode@nohost:cb_dist<0.209.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.218977>,
                               inet_tcp_dist,<0.403.0>,
                               #Ref<0.4274179209.2407792641.218980>}
[error_logger:error,2023-08-29T18:21:14.779Z,nonode@nohost:cb_dist<0.209.0>:ale_error_logger_handler:do_log:101]cb_dist: terminating with reason: shutdown
[ns_server:debug,2023-08-29T18:21:14.788Z,nonode@nohost:<0.13770.0>:dist_manager:teardown:311]Got nodedown msg {nodedown,'ns_1@cb.local',
                           [{nodedown_reason,net_kernel_terminated}]} after terminating net kernel
[ns_server:info,2023-08-29T18:21:14.789Z,nonode@nohost:<0.13768.0>:dist_manager:do_adjust_address:353]Adjusted IP to "127.0.0.1"
[ns_server:info,2023-08-29T18:21:14.790Z,nonode@nohost:<0.13768.0>:dist_manager:bringup:245]Attempting to bring up net_kernel with name 'ns_1@127.0.0.1'
[error_logger:info,2023-08-29T18:21:14.799Z,nonode@nohost:ssl_dist_admin_sup<0.13773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.13774.0>},
              {id,ssl_pem_cache_dist},
              {mfargs,{ssl_pem_cache,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:14.800Z,nonode@nohost:ssl_dist_admin_sup<0.13773.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_admin_sup}
    started: [{pid,<0.13775.0>},
              {id,ssl_dist_manager},
              {mfargs,{ssl_manager,start_link_dist,[[]]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:14.801Z,nonode@nohost:ssl_dist_sup<0.13772.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.13773.0>},
              {id,ssl_dist_admin_sup},
              {mfargs,{ssl_dist_admin_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:14.802Z,nonode@nohost:tls_dist_sup<0.13776.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.13777.0>},
              {id,dist_tls_connection_sup},
              {mfargs,{tls_connection_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:14.803Z,nonode@nohost:tls_dist_server_sup<0.13778.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.13779.0>},
              {id,dist_ssl_listen_tracker_sup},
              {mfargs,{ssl_listen_tracker_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:14.804Z,nonode@nohost:tls_dist_server_sup<0.13778.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.13780.0>},
              {id,dist_tls_server_session_ticket},
              {mfargs,{tls_server_session_ticket_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:14.805Z,nonode@nohost:tls_dist_server_sup<0.13778.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_server_sup}
    started: [{pid,<0.13781.0>},
              {id,dist_ssl_upgrade_server_session_cache_sup},
              {mfargs,
                  {ssl_upgrade_server_session_cache_sup,start_link_dist,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:14.806Z,nonode@nohost:tls_dist_sup<0.13776.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,tls_dist_sup}
    started: [{pid,<0.13778.0>},
              {id,tls_dist_server_sup},
              {mfargs,{tls_dist_server_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:14.806Z,nonode@nohost:ssl_dist_sup<0.13772.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ssl_dist_sup}
    started: [{pid,<0.13776.0>},
              {id,tls_dist_sup},
              {mfargs,{tls_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,4000},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:14.807Z,nonode@nohost:net_sup<0.13771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.13772.0>},
              {id,ssl_dist_sup},
              {mfargs,{ssl_dist_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:21:14.810Z,nonode@nohost:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Starting cb_dist with config []
[error_logger:info,2023-08-29T18:21:14.825Z,nonode@nohost:net_sup<0.13771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.13782.0>},
              {id,cb_dist},
              {mfargs,{cb_dist,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:14.827Z,nonode@nohost:net_sup<0.13771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.13783.0>},
              {id,cb_epmd},
              {mfargs,{cb_epmd,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:14.828Z,nonode@nohost:net_sup<0.13771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.13784.0>},
              {id,auth},
              {mfargs,{auth,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:14.830Z,nonode@nohost:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Initial protos: [{external,inet_tcp_dist}], required protos: [{external,
                                                                        inet_tcp_dist}]
[ns_server:debug,2023-08-29T18:21:14.830Z,nonode@nohost:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Starting {external,inet_tcp_dist} listener on 21100...
[ns_server:debug,2023-08-29T18:21:14.832Z,nonode@nohost:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Started listener: inet_tcp_dist
[chronicle:info,2023-08-29T18:21:14.834Z,ns_1@127.0.0.1:chronicle_proposer<0.282.0>:chronicle_proposer:handle_nodeup:1093]Peer 'ns_1@127.0.0.1' came up
[user:info,2023-08-29T18:21:14.834Z,ns_1@127.0.0.1:ns_node_disco<0.450.0>:ns_node_disco:handle_info:177]Node 'ns_1@127.0.0.1' saw that node 'ns_1@127.0.0.1' came up. Tags: []
[ns_server:debug,2023-08-29T18:21:14.834Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Started acceptor inet_tcp_dist: <0.13786.0>
[ns_server:debug,2023-08-29T18:21:14.834Z,ns_1@127.0.0.1:<0.362.0>:doc_replicator:nodeup_monitoring_loop:136]got nodeup event. Considering ddocs replication
[chronicle:debug,2023-08-29T18:21:14.834Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_note_term_status:603]Ignoring stale term status {<<"918babd26e22549c990a18e4926f0ef9">>,
                            {2,'ns_1@cb.local'},
                            finished}: {error,{not_a_leader,follower}}
[chronicle:info,2023-08-29T18:21:14.835Z,ns_1@127.0.0.1:chronicle_proposer<0.282.0>:chronicle_proposer:handle_nodeup:1127]Peer 'ns_1@127.0.0.1' is not in peers: ['ns_1@cb.local']
[chronicle:info,2023-08-29T18:21:14.835Z,ns_1@127.0.0.1:chronicle_proposer<0.282.0>:chronicle_proposer:handle_stop:1269]Proposer for term {2,'ns_1@cb.local'} in history <<"918babd26e22549c990a18e4926f0ef9">> is terminating.
[ns_server:debug,2023-08-29T18:21:14.835Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'ns_1@cb.local' using inet_tcp_dist
[chronicle:debug,2023-08-29T18:21:14.835Z,ns_1@127.0.0.1:chronicle_agent<0.235.0>:chronicle_agent:handle_local_mark_committed:1997]Marked seqno 11 committed
[error_logger:info,2023-08-29T18:21:14.835Z,ns_1@127.0.0.1:net_sup<0.13771.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,net_sup}
    started: [{pid,<0.13785.0>},
              {id,net_kernel},
              {mfargs,{net_kernel,start_link,
                                  [#{clean_halt => false,
                                     name => 'ns_1@127.0.0.1',
                                     name_domain => longnames,
                                     net_tickintensity => 4,
                                     net_ticktime => 60,
                                     supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,2000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:14.836Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792643.227297>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:21:14.836Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792643.227297>,
                                  inet_tcp_dist,<0.13790.0>,
                                  #Ref<0.4274179209.2407792642.227931>}
[chronicle:info,2023-08-29T18:21:14.836Z,ns_1@127.0.0.1:chronicle_server<0.243.0>:chronicle_proposer:stop:136]Proposer <0.282.0> stopped: {shutdown,stop}
[error_logger:info,2023-08-29T18:21:14.837Z,ns_1@127.0.0.1:kernel_sup<0.49.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,kernel_sup}
    started: [{pid,<0.13771.0>},
              {id,net_sup_dynamic},
              {mfargs,
                  {erl_distribution,start_link,
                      [#{clean_halt => false,name => 'ns_1@127.0.0.1',
                         name_domain => longnames,net_tickintensity => 4,
                         net_ticktime => 60,supervisor => net_sup_dynamic}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:21:14.839Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:configure_net_kernel:298]Set net_kernel vebosity to 10 -> 0
[ns_server:debug,2023-08-29T18:21:14.840Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.13786.0> DistCtrl #Port<0.340>: {con,
                                                                          #Ref<0.4274179209.2407792641.232373>,
                                                                          inet_tcp_dist,
                                                                          undefined,
                                                                          undefined}
[ns_server:debug,2023-08-29T18:21:14.840Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.13786.0> using module inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:14.841Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.232373>,
                                  inet_tcp_dist,<0.13792.0>,
                                  #Ref<0.4274179209.2407792641.232376>}
[error_logger:error,2023-08-29T18:21:14.842Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================ERROR REPORT=========================

** Cannot get connection id for node 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:14.844Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13792.0>,shutdown}}
[ns_server:debug,2023-08-29T18:21:14.844Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.232373>,
                               inet_tcp_dist,<0.13792.0>,
                               #Ref<0.4274179209.2407792641.232376>}
[ns_server:info,2023-08-29T18:21:14.852Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:save_node:159]saving node name '"ns_1@127.0.0.1"' to "/opt/couchbase/var/lib/couchbase/couchbase-server.node"
[ns_server:debug,2023-08-29T18:21:14.898Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.13786.0> DistCtrl #Port<0.341>: {con,
                                                                          #Ref<0.4274179209.2407792644.224153>,
                                                                          inet_tcp_dist,
                                                                          undefined,
                                                                          undefined}
[ns_server:debug,2023-08-29T18:21:14.899Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.13786.0> using module inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:14.900Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792644.224153>,
                                  inet_tcp_dist,<0.13794.0>,
                                  #Ref<0.4274179209.2407792644.224155>}
[error_logger:info,2023-08-29T18:21:14.905Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13794.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2023-08-29T18:21:14.905Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792644.224153>,
                               inet_tcp_dist,<0.13794.0>,
                               #Ref<0.4274179209.2407792644.224155>}
[ns_server:debug,2023-08-29T18:21:14.916Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:bringup:265]Attempted to save node name to disk: ok
[ns_server:debug,2023-08-29T18:21:14.927Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:wait_for_node:272]Waiting for connection to node 'babysitter_of_ns_1@cb.local' to be established
[error_logger:info,2023-08-29T18:21:14.928Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'babysitter_of_ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:21:14.928Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'babysitter_of_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:14.928Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792645.223204>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:21:14.929Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792645.223204>,
                                  inet_tcp_dist,<0.13800.0>,
                                  #Ref<0.4274179209.2407792643.227330>}
[ns_server:debug,2023-08-29T18:21:14.985Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:wait_for_node:284]Observed node 'babysitter_of_ns_1@cb.local' to come up
[ns_server:info,2023-08-29T18:21:14.985Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:do_adjust_address:357]Re-setting cookie {{sanitized,<<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>},
                   'ns_1@127.0.0.1'}
[ns_server:debug,2023-08-29T18:21:14.994Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.13786.0> DistCtrl #Port<0.344>: {con,
                                                                          #Ref<0.4274179209.2407792644.224162>,
                                                                          inet_tcp_dist,
                                                                          undefined,
                                                                          undefined}
[ns_server:debug,2023-08-29T18:21:14.995Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.13786.0> using module inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:14.995Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792644.224162>,
                                  inet_tcp_dist,<0.13810.0>,
                                  #Ref<0.4274179209.2407792644.224165>}
[error_logger:info,2023-08-29T18:21:15.005Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13810.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2023-08-29T18:21:15.005Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792644.224162>,
                               inet_tcp_dist,<0.13810.0>,
                               #Ref<0.4274179209.2407792644.224165>}
[ns_server:debug,2023-08-29T18:21:15.012Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.13786.0> DistCtrl #Port<0.346>: {con,
                                                                          #Ref<0.4274179209.2407792644.224171>,
                                                                          inet_tcp_dist,
                                                                          undefined,
                                                                          undefined}
[ns_server:debug,2023-08-29T18:21:15.012Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.13786.0> using module inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:15.013Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792644.224171>,
                                  inet_tcp_dist,<0.13814.0>,
                                  #Ref<0.4274179209.2407792644.224174>}
[error_logger:info,2023-08-29T18:21:15.020Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13814.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2023-08-29T18:21:15.021Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792644.224171>,
                               inet_tcp_dist,<0.13814.0>,
                               #Ref<0.4274179209.2407792644.224174>}
[error_logger:info,2023-08-29T18:21:15.051Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{auto_connect,'couchdb_ns_1@cb.local',
                          {21,#Ref<0.4274179209.2407923715.217902>}}}
[ns_server:debug,2023-08-29T18:21:15.052Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Setting up new connection to 'couchdb_ns_1@cb.local' using inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:15.053Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Added connection {con,#Ref<0.4274179209.2407792643.227344>,
                               inet_tcp_dist,undefined,undefined}
[ns_server:debug,2023-08-29T18:21:15.054Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792643.227344>,
                                  inet_tcp_dist,<0.13819.0>,
                                  #Ref<0.4274179209.2407792643.227346>}
[ns_server:info,2023-08-29T18:21:15.056Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:save_address_config:146]Deleting irrelevant ip file "/opt/couchbase/var/lib/couchbase/ip_start": {error,
                                                                          enoent}
[ns_server:info,2023-08-29T18:21:15.057Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:save_address_config:147]saving ip config to "/opt/couchbase/var/lib/couchbase/ip"
[ns_server:debug,2023-08-29T18:21:15.062Z,ns_1@127.0.0.1:node_monitor<0.790.0>:health_monitor:handle_cast:78]Ignoring heartbeat from an unknown node 'ns_1@127.0.0.1'
[error_logger:error,2023-08-29T18:21:15.113Z,ns_1@127.0.0.1:ns_heart_sup<0.483.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_heart_sup}
    errorContext: child_terminated
    reason: {{badmatch,false},
             [{ns_storage_conf,get_node_dir,1,
                               [{file,"src/ns_storage_conf.erl"},{line,321}]},
              {ns_storage_conf,this_node_dbdir,0,
                               [{file,"src/ns_storage_conf.erl"},{line,329}]},
              {ns_storage_conf,get_db_and_ix_paths,0,
                               [{file,"src/ns_storage_conf.erl"},{line,86}]},
              {ns_storage_conf,query_storage_conf,0,
                               [{file,"src/ns_storage_conf.erl"},{line,385}]},
              {ns_heart,current_status_slow_inner,0,
                        [{file,"src/ns_heart.erl"},{line,274}]},
              {ns_heart,current_status_slow,1,
                        [{file,"src/ns_heart.erl"},{line,235}]},
              {ns_heart,slow_updater_loop,0,
                        [{file,"src/ns_heart.erl"},{line,229}]}]}
    offender: [{pid,<0.487.0>},
               {id,ns_heart_slow_updater},
               {mfargs,{ns_heart,start_link_slow_updater,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[error_logger:error,2023-08-29T18:21:15.114Z,ns_1@127.0.0.1:logger_proxy<0.70.0>:ale_error_logger_handler:do_log:101]Error in process <0.487.0> on node 'ns_1@127.0.0.1' with exit value:
{{badmatch,false},
 [{ns_storage_conf,get_node_dir,1,
                   [{file,"src/ns_storage_conf.erl"},{line,321}]},
  {ns_storage_conf,this_node_dbdir,0,
                   [{file,"src/ns_storage_conf.erl"},{line,329}]},
  {ns_storage_conf,get_db_and_ix_paths,0,
                   [{file,"src/ns_storage_conf.erl"},{line,86}]},
  {ns_storage_conf,query_storage_conf,0,
                   [{file,"src/ns_storage_conf.erl"},{line,385}]},
  {ns_heart,current_status_slow_inner,0,
            [{file,"src/ns_heart.erl"},{line,274}]},
  {ns_heart,current_status_slow,1,[{file,"src/ns_heart.erl"},{line,235}]},
  {ns_heart,slow_updater_loop,0,[{file,"src/ns_heart.erl"},{line,229}]}]}

[error_logger:info,2023-08-29T18:21:15.115Z,ns_1@127.0.0.1:ns_heart_sup<0.483.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_heart_sup}
    started: [{pid,<0.13829.0>},
              {id,ns_heart_slow_updater},
              {mfargs,{ns_heart,start_link_slow_updater,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:21:15.123Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:save_address_config:150]Persisted the address successfully
[ns_server:debug,2023-08-29T18:21:15.123Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:rename_node_in_configs:438]Renaming node from 'ns_1@cb.local' to 'ns_1@127.0.0.1' in config
[ns_server:debug,2023-08-29T18:21:15.124Z,ns_1@127.0.0.1:chronicle_local<0.222.0>:chronicle_local:handle_rename:149]Handle renaming from 'ns_1@cb.local' to 'ns_1@127.0.0.1'
[chronicle:debug,2023-08-29T18:21:15.124Z,ns_1@127.0.0.1:chronicle_agent<0.235.0>:chronicle_agent:handle_reprovision:1140]Reprovisioning peer with config:
{log_entry,<<"918babd26e22549c990a18e4926f0ef9">>,
           {3,'ns_1@127.0.0.1'},
           12,
           {config,undefined,0,<<"3a854e031a1cb4754e44383256799533">>,
                   #{'ns_1@127.0.0.1' =>
                         #{id => <<"0957cbe35228f96d2df30f13ea1a5217">>,
                           role => voter}},
                   undefined,
                   #{chronicle_config_rsm =>
                         {rsm_config,chronicle_config_rsm,[]},
                     kv => {rsm_config,chronicle_kv,[]}},
                   #{},undefined,
                   [{<<"918babd26e22549c990a18e4926f0ef9">>,0}]}}
[ns_server:debug,2023-08-29T18:21:15.127Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.13786.0> DistCtrl #Port<0.349>: {con,
                                                                          #Ref<0.4274179209.2407792641.232437>,
                                                                          inet_tcp_dist,
                                                                          undefined,
                                                                          undefined}
[ns_server:debug,2023-08-29T18:21:15.128Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.13786.0> using module inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:15.129Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792641.232437>,
                                  inet_tcp_dist,<0.13830.0>,
                                  #Ref<0.4274179209.2407792644.224198>}
[chronicle:info,2023-08-29T18:21:15.131Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_reprovisioned:498]System reprovisioned.
[chronicle:info,2023-08-29T18:21:15.131Z,ns_1@127.0.0.1:<0.13832.0>:chronicle_leader:do_election_worker:892]Starting election.
History ID: <<"918babd26e22549c990a18e4926f0ef9">>
Log position: {{3,'ns_1@127.0.0.1'},12}
Peers: ['ns_1@127.0.0.1']
Required quorum: {majority,{set,1,16,16,8,80,48,
                                {[],[],[],[],[],[],[],[],[],[],[],[],[],[],[],
                                 []},
                                {{[],
                                  ['ns_1@127.0.0.1'],
                                  [],[],[],[],[],[],[],[],[],[],[],[],[],
                                  []}}}}
[chronicle:info,2023-08-29T18:21:15.132Z,ns_1@127.0.0.1:<0.13832.0>:chronicle_leader:do_election_worker:901]I'm the only peer, so I'm the leader.
[chronicle:info,2023-08-29T18:21:15.132Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_election_result:691]Going to become a leader in term {4,'ns_1@127.0.0.1'} (history id <<"918babd26e22549c990a18e4926f0ef9">>)
[ns_server:debug,2023-08-29T18:21:15.137Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792641.232437>,
                               inet_tcp_dist,<0.13830.0>,
                               #Ref<0.4274179209.2407792644.224198>}
[error_logger:info,2023-08-29T18:21:15.136Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13830.0>,{recv_challenge_reply_failed,{error,closed}}}}
[chronicle:debug,2023-08-29T18:21:15.139Z,ns_1@127.0.0.1:chronicle_agent<0.235.0>:chronicle_agent:handle_establish_term:1529]Accepted term {4,'ns_1@127.0.0.1'} in history <<"918babd26e22549c990a18e4926f0ef9">>
[chronicle:debug,2023-08-29T18:21:15.139Z,ns_1@127.0.0.1:chronicle_proposer<0.13834.0>:chronicle_proposer:establish_term_init:367]Going to establish term {4,'ns_1@127.0.0.1'} (history id <<"918babd26e22549c990a18e4926f0ef9">>).
Quorum peers: ['ns_1@127.0.0.1']
Metadata:
{metadata,'ns_1@127.0.0.1',<<"0957cbe35228f96d2df30f13ea1a5217">>,
          <<"918babd26e22549c990a18e4926f0ef9">>,
          {3,'ns_1@127.0.0.1'},
          {3,'ns_1@127.0.0.1'},
          12,12,
          {log_entry,<<"918babd26e22549c990a18e4926f0ef9">>,
                     {3,'ns_1@127.0.0.1'},
                     12,
                     {config,undefined,0,
                             <<"3a854e031a1cb4754e44383256799533">>,
                             #{'ns_1@127.0.0.1' =>
                                   #{id =>
                                         <<"0957cbe35228f96d2df30f13ea1a5217">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"918babd26e22549c990a18e4926f0ef9">>,0}]}},
          {log_entry,<<"918babd26e22549c990a18e4926f0ef9">>,
                     {3,'ns_1@127.0.0.1'},
                     12,
                     {config,undefined,0,
                             <<"3a854e031a1cb4754e44383256799533">>,
                             #{'ns_1@127.0.0.1' =>
                                   #{id =>
                                         <<"0957cbe35228f96d2df30f13ea1a5217">>,
                                     role => voter}},
                             undefined,
                             #{chronicle_config_rsm =>
                                   {rsm_config,chronicle_config_rsm,[]},
                               kv => {rsm_config,chronicle_kv,[]}},
                             #{},undefined,
                             [{<<"918babd26e22549c990a18e4926f0ef9">>,0}]}},
          undefined}
[chronicle:debug,2023-08-29T18:21:15.141Z,ns_1@127.0.0.1:chronicle_proposer<0.13834.0>:chronicle_proposer:establish_term_maybe_transition:686]Established term {4,'ns_1@127.0.0.1'} (history id <<"918babd26e22549c990a18e4926f0ef9">>) successfully.
Votes: ['ns_1@127.0.0.1']
[chronicle:debug,2023-08-29T18:21:15.142Z,ns_1@127.0.0.1:chronicle_proposer<0.13834.0>:chronicle_proposer:handle_state_enter:284]Starting recovery for term {4,'ns_1@127.0.0.1'} in history <<"918babd26e22549c990a18e4926f0ef9">>
[chronicle:debug,2023-08-29T18:21:15.148Z,ns_1@127.0.0.1:chronicle_proposer<0.13834.0>:chronicle_proposer:handle_state_enter:296]Proposer for term {4,'ns_1@127.0.0.1'} in history <<"918babd26e22549c990a18e4926f0ef9">> is ready. Committed seqno: 13
[chronicle:info,2023-08-29T18:21:15.149Z,ns_1@127.0.0.1:chronicle_leader<0.239.0>:chronicle_leader:handle_note_term_status:596]Term {4,'ns_1@127.0.0.1'} established.
[ns_server:debug,2023-08-29T18:21:15.181Z,ns_1@127.0.0.1:chronicle_kv_log<0.412.0>:chronicle_kv_log:handle_info:42]delete (key: {node,'ns_1@cb.local',membership}, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                                      14})
[ns_server:debug,2023-08-29T18:21:15.182Z,ns_1@127.0.0.1:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: {node,'ns_1@127.0.0.1',membership}, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                                       14})
active
[ns_server:debug,2023-08-29T18:21:15.182Z,ns_1@127.0.0.1:mb_master<0.691.0>:mb_master:update_peers:562]List of peers has changed from ['ns_1@cb.local'] to ['ns_1@127.0.0.1']
[ns_server:debug,2023-08-29T18:21:15.182Z,ns_1@127.0.0.1:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: server_groups, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,14})
[[{uuid,<<"0">>},{name,<<"Group 1">>},{nodes,['ns_1@127.0.0.1']}]]
[ns_server:debug,2023-08-29T18:21:15.183Z,ns_1@127.0.0.1:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: ns_config_purger, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,
                                     14})
'ns_1@127.0.0.1'
[ns_server:debug,2023-08-29T18:21:15.183Z,ns_1@127.0.0.1:chronicle_kv_log<0.412.0>:chronicle_kv_log:log:59]update (key: nodes_wanted, rev: {<<"918babd26e22549c990a18e4926f0ef9">>,14})
['ns_1@127.0.0.1']
[ns_server:debug,2023-08-29T18:21:15.200Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',prometheus_auth_info} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   prometheus_auth_info}:
  {"@prometheus",
   {auth,[{<<"plain">>,
           {sanitized,<<"DxBp77BWEtDRnhVC01VIpYURs7Gn5zjftfzSQ4+vkRI=">>}},
          {<<"sha512">>,
           {[{<<"h">>,
              {sanitized,<<"cwWcSL12N4zWttNlTH4XtlOX2Mf/4sX4mzl99mnuuYk=">>}},
             {<<"s">>,
              <<"ZO4AsN0VtNAJ/HUwKHmxkQdPXcFYsiSQL5dJF13JYcY63pU/uLlEqkvwKoDIRcT6jdmUuzSJscSsaVB81udUOw==">>},
             {<<"i">>,4000}]}},
          {<<"sha256">>,
           {[{<<"h">>,
              {sanitized,<<"V58Yrqjy9WA2D9HoExukibtMDOTSf5pMNxy/+jhZKNo=">>}},
             {<<"s">>,<<"OoiOw4UgLXbJ8bO1hb3dwbpOtObbR8RTflQq9vde8pU=">>},
             {<<"i">>,4000}]}},
          {<<"sha1">>,
           {[{<<"h">>,
              {sanitized,<<"7N1ZV3XdMV7vuQtOnXnSnPCSq2dc/VxI1Ls4nQ7f+fI=">>}},
             {<<"s">>,<<"WwNr6OiWKpSPGlmapdb14zR+9Zw=">>},
             {<<"i">>,4000}]}}]}} ->
  {"@prometheus",
   {auth,[{<<"plain">>,
           {sanitized,<<"DxBp77BWEtDRnhVC01VIpYURs7Gn5zjftfzSQ4+vkRI=">>}},
          {<<"sha512">>,
           {[{<<"h">>,
              {sanitized,<<"cwWcSL12N4zWttNlTH4XtlOX2Mf/4sX4mzl99mnuuYk=">>}},
             {<<"s">>,
              <<"ZO4AsN0VtNAJ/HUwKHmxkQdPXcFYsiSQL5dJF13JYcY63pU/uLlEqkvwKoDIRcT6jdmUuzSJscSsaVB81udUOw==">>},
             {<<"i">>,4000}]}},
          {<<"sha256">>,
           {[{<<"h">>,
              {sanitized,<<"V58Yrqjy9WA2D9HoExukibtMDOTSf5pMNxy/+jhZKNo=">>}},
             {<<"s">>,<<"OoiOw4UgLXbJ8bO1hb3dwbpOtObbR8RTflQq9vde8pU=">>},
             {<<"i">>,4000}]}},
          {<<"sha1">>,
           {[{<<"h">>,
              {sanitized,<<"7N1ZV3XdMV7vuQtOnXnSnPCSq2dc/VxI1Ls4nQ7f+fI=">>}},
             {<<"s">>,<<"WwNr6OiWKpSPGlmapdb14zR+9Zw=">>},
             {<<"i">>,4000}]}}]}}
[ns_server:debug,2023-08-29T18:21:15.205Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',eventing_dir} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           eventing_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2023-08-29T18:21:15.206Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_dirs} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        cbas_dirs}:
  ["/opt/couchbase/var/lib/couchbase/data"] ->
  ["/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2023-08-29T18:21:15.209Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.13786.0> DistCtrl #Port<0.350>: {con,
                                                                          #Ref<0.4274179209.2407792644.224216>,
                                                                          inet_tcp_dist,
                                                                          undefined,
                                                                          undefined}
[ns_server:debug,2023-08-29T18:21:15.209Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.13786.0> using module inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:15.210Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792644.224216>,
                                  inet_tcp_dist,<0.13836.0>,
                                  #Ref<0.4274179209.2407792648.219784>}
[ns_server:debug,2023-08-29T18:21:15.207Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',node_cert} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        node_cert}:
  [{subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
   {not_after,64691827199},
   {verified_with,<<133,166,53,2,8,48,187,102,19,107,176,9,230,211,49,117>>},
   {type,generated},
   {load_timestamp,63860552090},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIF3/tSsVH0d4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYjUxZWI3\nNzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCr9XJDH3HmSp0Vabu1\nQm1+A0/9t3DlYJP9BDjf5CmXOlsWpvUIAP24RQ9rQsu3gn8VvYYnkK2mFfF0SRo5\nxJX/EZde2ejr9piUfJTuY18OYQthMm3HKfOyPT/EJQsiQF7LpHl+HdogqtG2kZDS\nQxtwafkr8FiEbtU/XebR1ST0QwW2EDwoG4ViLY2MNtI8o2IUxo9AzqfYL7s11fUU\nDVqimKGAzWHIt1ARuJ/6jzXlTeTPL8/pfbWBQ1ihc42NsFP1q3erqE5YEZFDyMPi\npbmLe4DnvPb9XXRPgLgkg86BKl/MAL1Z5Rx3xmykZ2vBZfq4Jx4x5RaApfPuPwdh\njhyvAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRvykfjVzdx7swRsAwqxzBSxpVX\nHTANBgkqhkiG9w0BAQsFAAOCAQEATX4FfImG0CtsodXXd2PrBiZwe4+R36pm1CBj\nrm/kxaWKI8JD54zJnrTpQ4nc6n+JCgxH5pQgVV4Fnin2HBdjPU0mZv0zajXwuDr4\nzv9jTqw1hQ24RvcgscOOgaceHPW/yX4Mc5SicXzXxVc4bBdauae5RU2KUYkn8+gq\nGx9Vjv7B1orkiCRhPxYO4//CpTDLt2bpMzjPuAhpwSUgv01b6EY0zLIgsH6ftizF\nX6SlSO669NtXIMZKcQ3LR0OFpNPCSdiCWLNsMKQG0LUd9ka5nuQEU9J3v/GMesHF\nJdADlV61Z0BLgRChuO+HR3J2QUkmww5k7UqGjhiH6CDxSUrdfQ==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIF3/tSu5x35owDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAK7J80br\nemoHet3x/9SjInqRaDHbxjoA13fYRB6cqVMFucsd+STGbshOlFzbP1rVfQD7I/tr\nytnNch+HiX1bubv7XVUoHyKrOourkf171LEQQP/SZjeV2Hg+8gxpg1jLg7V9sxGz\nIVI+0qcQ9wNacd44GzAGT6NYJaJY8VNt7vcVCkFFaliLj0pY/C2KYjNpP2BzdGY7\nWSvnaEtessX3isYiU6ep0triynxgn2U0n7qNPPiE+Jj0tRnnlH7TfishRZ1aEySG\nWDeiPRMde4x33mghzPfVCSxWyBen6pCfpa+Mb+SsPwCnnXXArbJixKVDSWQDX8/h\nKFrrItdVk+Q7koECAwEAAaNnMGUwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoG\nCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUb8pH41c3ce7MEbAM\nKscwUsaVVx0wDwYDVR0RBAgwBocEfwAAATANBgkqhkiG9w0BAQsFAAOCAQEAmFlj\nFBzGaGseqivvTwgt9ZX/myDOFtKkcTPYKzceUo5/Fvy9C5JM206KG+LcqhA7voiV\nJWoofud6DYTTD+5gm7tnKNKwhpQOqZNaGiQpFNFC46i7d8neqMenG4ChVEiHdxqa\ncNpC9/93zTWOdCnwNZo9C1SBy6ZeiECqXSxJahk7pRpBsVSs9saQWxl6EIqk5kyy\n8VqiKo62GWrtkjE3fHucHtN1b/Ym5ax1+Zzsx/TUajyJ9gzHvMTMIjXtwPO+i5wV\nhw0plQtkBM0CJS3yMi0CmvflAuH9tD9rf+JGvlGAZZveaZwiozA70ZCfdITsDLHK\nfADX4MnTBjTcZpV/Ew==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {hostname,"127.0.0.1"}] ->
  [{subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
   {not_after,64691827199},
   {verified_with,<<133,166,53,2,8,48,187,102,19,107,176,9,230,211,49,117>>},
   {type,generated},
   {load_timestamp,63860552090},
   {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIF3/tSsVH0d4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYjUxZWI3\nNzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCr9XJDH3HmSp0Vabu1\nQm1+A0/9t3DlYJP9BDjf5CmXOlsWpvUIAP24RQ9rQsu3gn8VvYYnkK2mFfF0SRo5\nxJX/EZde2ejr9piUfJTuY18OYQthMm3HKfOyPT/EJQsiQF7LpHl+HdogqtG2kZDS\nQxtwafkr8FiEbtU/XebR1ST0QwW2EDwoG4ViLY2MNtI8o2IUxo9AzqfYL7s11fUU\nDVqimKGAzWHIt1ARuJ/6jzXlTeTPL8/pfbWBQ1ihc42NsFP1q3erqE5YEZFDyMPi\npbmLe4DnvPb9XXRPgLgkg86BKl/MAL1Z5Rx3xmykZ2vBZfq4Jx4x5RaApfPuPwdh\njhyvAgMBAAGjVzBVMA4GA1UdDwEB/wQEAwICpDATBgNVHSUEDDAKBggrBgEFBQcD\nATAPBgNVHRMBAf8EBTADAQH/MB0GA1UdDgQWBBRvykfjVzdx7swRsAwqxzBSxpVX\nHTANBgkqhkiG9w0BAQsFAAOCAQEATX4FfImG0CtsodXXd2PrBiZwe4+R36pm1CBj\nrm/kxaWKI8JD54zJnrTpQ4nc6n+JCgxH5pQgVV4Fnin2HBdjPU0mZv0zajXwuDr4\nzv9jTqw1hQ24RvcgscOOgaceHPW/yX4Mc5SicXzXxVc4bBdauae5RU2KUYkn8+gq\nGx9Vjv7B1orkiCRhPxYO4//CpTDLt2bpMzjPuAhpwSUgv01b6EY0zLIgsH6ftizF\nX6SlSO669NtXIMZKcQ3LR0OFpNPCSdiCWLNsMKQG0LUd9ka5nuQEU9J3v/GMesHF\nJdADlV61Z0BLgRChuO+HR3J2QUkmww5k7UqGjhiH6CDxSUrdfQ==\n-----END CERTIFICATE-----\n">>},
   {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIF3/tSu5x35owDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAK7J80br\nemoHet3x/9SjInqRaDHbxjoA13fYRB6cqVMFucsd+STGbshOlFzbP1rVfQD7I/tr\nytnNch+HiX1bubv7XVUoHyKrOourkf171LEQQP/SZjeV2Hg+8gxpg1jLg7V9sxGz\nIVI+0qcQ9wNacd44GzAGT6NYJaJY8VNt7vcVCkFFaliLj0pY/C2KYjNpP2BzdGY7\nWSvnaEtessX3isYiU6ep0triynxgn2U0n7qNPPiE+Jj0tRnnlH7TfishRZ1aEySG\nWDeiPRMde4x33mghzPfVCSxWyBen6pCfpa+Mb+SsPwCnnXXArbJixKVDSWQDX8/h\nKFrrItdVk+Q7koECAwEAAaNnMGUwDgYDVR0PAQH/BAQDAgWgMBMGA1UdJQQMMAoG\nCCsGAQUFBwMBMAwGA1UdEwEB/wQCMAAwHwYDVR0jBBgwFoAUb8pH41c3ce7MEbAM\nKscwUsaVVx0wDwYDVR0RBAgwBocEfwAAATANBgkqhkiG9w0BAQsFAAOCAQEAmFlj\nFBzGaGseqivvTwgt9ZX/myDOFtKkcTPYKzceUo5/Fvy9C5JM206KG+LcqhA7voiV\nJWoofud6DYTTD+5gm7tnKNKwhpQOqZNaGiQpFNFC46i7d8neqMenG4ChVEiHdxqa\ncNpC9/93zTWOdCnwNZo9C1SBy6ZeiECqXSxJahk7pRpBsVSs9saQWxl6EIqk5kyy\n8VqiKo62GWrtkjE3fHucHtN1b/Ym5ax1+Zzsx/TUajyJ9gzHvMTMIjXtwPO+i5wV\nhw0plQtkBM0CJS3yMi0CmvflAuH9tD9rf+JGvlGAZZveaZwiozA70ZCfdITsDLHK\nfADX4MnTBjTcZpV/Ew==\n-----END CERTIFICATE-----\n">>},
   {pkey_passphrase_settings,[]},
   {certs_epoch,0},
   {hostname,"127.0.0.1"}]
[ns_server:debug,2023-08-29T18:21:15.214Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',erl_external_listeners} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     erl_external_listeners}:
  [{inet,false}] ->
  [{inet,false}]
[error_logger:info,2023-08-29T18:21:15.215Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13836.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2023-08-29T18:21:15.215Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792644.224216>,
                               inet_tcp_dist,<0.13836.0>,
                               #Ref<0.4274179209.2407792648.219784>}
[ns_server:debug,2023-08-29T18:21:15.215Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',node_encryption} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              node_encryption}:
  false ->
  false
[ns_server:debug,2023-08-29T18:21:15.215Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',address_family} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             address_family}:
  inet ->
  inet
[ns_server:debug,2023-08-29T18:21:15.218Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf quorum_nodes -> quorum_nodes:
  ['ns_1@cb.local'] ->
  ['ns_1@127.0.0.1']
[ns_server:debug,2023-08-29T18:21:15.220Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',audit} -> {node,'ns_1@127.0.0.1',
                                                    audit}:
  [] ->
  []
[ns_server:debug,2023-08-29T18:21:15.221Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',backup_grpc_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               backup_grpc_port}:
  9124 ->
  9124
[ns_server:debug,2023-08-29T18:21:15.221Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',backup_http_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               backup_http_port}:
  8097 ->
  8097
[ns_server:debug,2023-08-29T18:21:15.223Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',backup_https_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                backup_https_port}:
  18097 ->
  18097
[ns_server:debug,2023-08-29T18:21:15.223Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',capi_port} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        capi_port}:
  8092 ->
  8092
[ns_server:debug,2023-08-29T18:21:15.223Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_admin_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_admin_port}:
  9110 ->
  9110
[ns_server:debug,2023-08-29T18:21:15.224Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_cc_client_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_cc_client_port}:
  9113 ->
  9113
[ns_server:debug,2023-08-29T18:21:15.225Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_cc_cluster_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   cbas_cc_cluster_port}:
  9112 ->
  9112
[ns_server:debug,2023-08-29T18:21:15.226Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_cc_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cc_http_port}:
  9111 ->
  9111
[ns_server:debug,2023-08-29T18:21:15.226Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_cluster_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_cluster_port}:
  9115 ->
  9115
[ns_server:debug,2023-08-29T18:21:15.227Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_console_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                cbas_console_port}:
  9114 ->
  9114
[ns_server:debug,2023-08-29T18:21:15.228Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_data_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_data_port}:
  9116 ->
  9116
[ns_server:debug,2023-08-29T18:21:15.228Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_debug_port} -> {node,
                                                              'ns_1@127.0.0.1',
                                                              cbas_debug_port}:
  -1 ->
  -1
[ns_server:debug,2023-08-29T18:21:15.228Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_http_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             cbas_http_port}:
  8095 ->
  8095
[ns_server:debug,2023-08-29T18:21:15.229Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_messaging_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  cbas_messaging_port}:
  9118 ->
  9118
[ns_server:debug,2023-08-29T18:21:15.229Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_metadata_callback_port} -> {node,
                                                                          'ns_1@127.0.0.1',
                                                                          cbas_metadata_callback_port}:
  9119 ->
  9119
[ns_server:debug,2023-08-29T18:21:15.229Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_metadata_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 cbas_metadata_port}:
  9121 ->
  9121
[ns_server:debug,2023-08-29T18:21:15.230Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_parent_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_parent_port}:
  9122 ->
  9122
[ns_server:debug,2023-08-29T18:21:15.230Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_replication_port} -> {node,
                                                                    'ns_1@127.0.0.1',
                                                                    cbas_replication_port}:
  9120 ->
  9120
[ns_server:debug,2023-08-29T18:21:15.230Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_result_port} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               cbas_result_port}:
  9117 ->
  9117
[ns_server:debug,2023-08-29T18:21:15.230Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',cbas_ssl_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            cbas_ssl_port}:
  18095 ->
  18095
[ns_server:debug,2023-08-29T18:21:15.231Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',compaction_daemon} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                compaction_daemon}:
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}] ->
  [{check_interval,30},
   {min_db_file_size,131072},
   {min_view_file_size,20971520}]
[ns_server:debug,2023-08-29T18:21:15.231Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',config_version} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             config_version}:
  {7,2} ->
  {7,2}
[ns_server:debug,2023-08-29T18:21:15.231Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',database_dir} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           database_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2023-08-29T18:21:15.232Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',event_log} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        event_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2023-08-29T18:21:15.232Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',eventing_debug_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_debug_port}:
  9140 ->
  9140
[ns_server:debug,2023-08-29T18:21:15.233Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',eventing_http_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 eventing_http_port}:
  8096 ->
  8096
[ns_server:debug,2023-08-29T18:21:15.233Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',eventing_https_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  eventing_https_port}:
  18096 ->
  18096
[ns_server:debug,2023-08-29T18:21:15.233Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',fts_grpc_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_grpc_port}:
  9130 ->
  9130
[ns_server:debug,2023-08-29T18:21:15.234Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',fts_grpc_ssl_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                fts_grpc_ssl_port}:
  19130 ->
  19130
[ns_server:debug,2023-08-29T18:21:15.234Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',fts_http_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            fts_http_port}:
  8094 ->
  8094
[ns_server:debug,2023-08-29T18:21:15.234Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',fts_ssl_port} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           fts_ssl_port}:
  18094 ->
  18094
[ns_server:debug,2023-08-29T18:21:15.237Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',index_dir} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        index_dir}:
  "/opt/couchbase/var/lib/couchbase/data" ->
  "/opt/couchbase/var/lib/couchbase/data"
[ns_server:debug,2023-08-29T18:21:15.239Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',indexer_admin_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_admin_port}:
  9100 ->
  9100
[ns_server:debug,2023-08-29T18:21:15.241Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Accepted new connection from <0.13786.0> DistCtrl #Port<0.351>: {con,
                                                                          #Ref<0.4274179209.2407792645.223238>,
                                                                          inet_tcp_dist,
                                                                          undefined,
                                                                          undefined}
[ns_server:debug,2023-08-29T18:21:15.241Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',indexer_http_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_http_port}:
  9102 ->
  9102
[ns_server:debug,2023-08-29T18:21:15.241Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:cb_dist:info_msg:872]cb_dist: Accepting connection from <0.13786.0> using module inet_tcp_dist
[ns_server:debug,2023-08-29T18:21:15.242Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Updated connection: {con,#Ref<0.4274179209.2407792645.223238>,
                                  inet_tcp_dist,<0.13838.0>,
                                  #Ref<0.4274179209.2407792645.223241>}
[ns_server:debug,2023-08-29T18:21:15.241Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',indexer_https_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 indexer_https_port}:
  19102 ->
  19102
[ns_server:debug,2023-08-29T18:21:15.243Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',indexer_scan_port} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                indexer_scan_port}:
  9101 ->
  9101
[ns_server:debug,2023-08-29T18:21:15.243Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',indexer_stcatchup_port} -> {node,
                                                                     'ns_1@127.0.0.1',
                                                                     indexer_stcatchup_port}:
  9104 ->
  9104
[ns_server:debug,2023-08-29T18:21:15.244Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',indexer_stinit_port} -> {node,
                                                                  'ns_1@127.0.0.1',
                                                                  indexer_stinit_port}:
  9103 ->
  9103
[ns_server:debug,2023-08-29T18:21:15.244Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',indexer_stmaint_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   indexer_stmaint_port}:
  9105 ->
  9105
[ns_server:debug,2023-08-29T18:21:15.244Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',is_enterprise} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            is_enterprise}:
  true ->
  true
[ns_server:debug,2023-08-29T18:21:15.245Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',isasl} -> {node,'ns_1@127.0.0.1',
                                                    isasl}:
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}] ->
  [{path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[error_logger:info,2023-08-29T18:21:15.245Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13838.0>,{recv_challenge_reply_failed,{error,closed}}}}
[ns_server:debug,2023-08-29T18:21:15.245Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792645.223238>,
                               inet_tcp_dist,<0.13838.0>,
                               #Ref<0.4274179209.2407792645.223241>}
[ns_server:debug,2023-08-29T18:21:15.251Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',memcached} -> {node,
                                                        'ns_1@127.0.0.1',
                                                        memcached}:
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}] ->
  [{port,11210},
   {dedicated_port,11209},
   {dedicated_ssl_port,11206},
   {ssl_port,11207},
   {admin_user,"@ns_server"},
   {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
                 "@eventing","@cbas","@backup"]},
   {admin_pass,"*****"},
   {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                       {static_config_string,"failpartialwarmup=false"}]},
             {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                         {static_config_string,"vb0=true"}]}]},
   {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
   {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
   {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
   {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
   {log_prefix,"memcached.log"},
   {log_generations,20},
   {log_cyclesize,10485760},
   {log_rotation_period,39003}]
[ns_server:debug,2023-08-29T18:21:15.253Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',memcached_config} -> {node,
                                                               'ns_1@127.0.0.1',
                                                               memcached_config}:
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {enforce_tenant_limits_enabled,
        {memcached_config_mgr,should_enforce_limits,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {max_client_connection_details,max_client_connection_details}]} ->
  {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
    {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
    {connection_idle_time,connection_idle_time},
    {privilege_debug,privilege_debug},
    {breakpad,
        {[{enabled,breakpad_enabled},
          {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
    {admin,{"~s",[admin_user]}},
    {verbosity,verbosity},
    {audit_file,{"~s",[audit_file]}},
    {rbac_file,{"~s",[rbac_file]}},
    {dedupe_nmvb_maps,dedupe_nmvb_maps},
    {tracing_enabled,tracing_enabled},
    {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
    {xattr_enabled,true},
    {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
    {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
    {enforce_tenant_limits_enabled,
        {memcached_config_mgr,should_enforce_limits,[]}},
    {max_connections,max_connections},
    {system_connections,system_connections},
    {num_reader_threads,num_reader_threads},
    {num_writer_threads,num_writer_threads},
    {num_auxio_threads,num_auxio_threads},
    {num_nonio_threads,num_nonio_threads},
    {num_storage_threads,num_storage_threads},
    {logger,
        {[{filename,{"~s/~s",[log_path,log_prefix]}},
          {cyclesize,log_cyclesize}]}},
    {external_auth_service,
        {memcached_config_mgr,get_external_auth_service,[]}},
    {active_external_users_push_interval,
        {memcached_config_mgr,get_external_users_push_interval,[]}},
    {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
    {connection_limit_mode,connection_limit_mode},
    {free_connection_pool_size,free_connection_pool_size},
    {tcp_keepalive_idle,tcp_keepalive_idle},
    {tcp_keepalive_interval,tcp_keepalive_interval},
    {tcp_keepalive_probes,tcp_keepalive_probes},
    {max_client_connection_details,max_client_connection_details}]}
[ns_server:debug,2023-08-29T18:21:15.256Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',memcached_dedicated_ssl_port} -> {node,
                                                                           'ns_1@127.0.0.1',
                                                                           memcached_dedicated_ssl_port}:
  11206 ->
  11206
[ns_server:debug,2023-08-29T18:21:15.256Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',memcached_defaults} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 memcached_defaults}:
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {max_client_connection_details,0}] ->
  [{max_connections,65000},
   {system_connections,5000},
   {connection_idle_time,0},
   {verbosity,0},
   {privilege_debug,false},
   {breakpad_enabled,true},
   {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
   {dedupe_nmvb_maps,false},
   {je_malloc_conf,undefined},
   {tracing_enabled,true},
   {datatype_snappy,true},
   {num_reader_threads,<<"default">>},
   {num_writer_threads,<<"default">>},
   {num_auxio_threads,<<"default">>},
   {num_nonio_threads,<<"default">>},
   {num_storage_threads,<<"default">>},
   {connection_limit_mode,<<"disconnect">>},
   {free_connection_pool_size,0},
   {tcp_keepalive_idle,360},
   {tcp_keepalive_interval,10},
   {tcp_keepalive_probes,3},
   {max_client_connection_details,0}]
[ns_server:debug,2023-08-29T18:21:15.259Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',memcached_prometheus} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   memcached_prometheus}:
  11280 ->
  11280
[ns_server:debug,2023-08-29T18:21:15.261Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',ns_log} -> {node,'ns_1@127.0.0.1',
                                                     ns_log}:
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}] ->
  [{filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2023-08-29T18:21:15.261Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',port_servers} -> {node,
                                                           'ns_1@127.0.0.1',
                                                           port_servers}:
  [] ->
  []
[ns_server:debug,2023-08-29T18:21:15.262Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',projector_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             projector_port}:
  9999 ->
  9999
[ns_server:debug,2023-08-29T18:21:15.262Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',projector_ssl_port} -> {node,
                                                                 'ns_1@127.0.0.1',
                                                                 projector_ssl_port}:
  9999 ->
  9999
[ns_server:debug,2023-08-29T18:21:15.263Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',prometheus_http_port} -> {node,
                                                                   'ns_1@127.0.0.1',
                                                                   prometheus_http_port}:
  9123 ->
  9123
[ns_server:debug,2023-08-29T18:21:15.263Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',query_port} -> {node,
                                                         'ns_1@127.0.0.1',
                                                         query_port}:
  8093 ->
  8093
[ns_server:debug,2023-08-29T18:21:15.264Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',rest} -> {node,'ns_1@127.0.0.1',rest}:
  [{port,8091},{port_meta,global}] ->
  [{port,8091},{port_meta,global}]
[ns_server:debug,2023-08-29T18:21:15.265Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',saslauthd_enabled} -> {node,
                                                                'ns_1@127.0.0.1',
                                                                saslauthd_enabled}:
  true ->
  true
[ns_server:debug,2023-08-29T18:21:15.265Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',ssl_capi_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_capi_port}:
  18092 ->
  18092
[ns_server:debug,2023-08-29T18:21:15.266Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',ssl_query_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             ssl_query_port}:
  18093 ->
  18093
[ns_server:debug,2023-08-29T18:21:15.266Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',ssl_rest_port} -> {node,
                                                            'ns_1@127.0.0.1',
                                                            ssl_rest_port}:
  18091 ->
  18091
[ns_server:debug,2023-08-29T18:21:15.267Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',uuid} -> {node,'ns_1@127.0.0.1',uuid}:
  <<"a2acfe035c01f55c0d78fce935ad3d90">> ->
  <<"a2acfe035c01f55c0d78fce935ad3d90">>
[ns_server:debug,2023-08-29T18:21:15.268Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',xdcr_rest_port} -> {node,
                                                             'ns_1@127.0.0.1',
                                                             xdcr_rest_port}:
  9998 ->
  9998
[ns_server:debug,2023-08-29T18:21:15.269Z,ns_1@127.0.0.1:ns_config<0.260.0>:dist_manager:rename_config_kv:447]renaming node conf {node,'ns_1@cb.local',{project_intact,is_vulnerable}} -> {node,
                                                                             'ns_1@127.0.0.1',
                                                                             {project_intact,
                                                                              is_vulnerable}}:
  false ->
  false
[ns_server:debug,2023-08-29T18:21:15.270Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:wait_for_node:272]Waiting for connection to node 'couchdb_ns_1@cb.local' to be established
[error_logger:info,2023-08-29T18:21:15.271Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{connect,normal,'couchdb_ns_1@cb.local'}}
[ns_server:warn,2023-08-29T18:21:15.271Z,ns_1@127.0.0.1:ns_tick_agent<0.773.0>:ns_tick_agent:handle_tick:113]Ignoring tick from a non-master node 'ns_1@127.0.0.1'. Master: 'ns_1@cb.local'
[ns_server:debug,2023-08-29T18:21:15.271Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:wait_for_node:284]Observed node 'couchdb_ns_1@cb.local' to come up
[ns_server:warn,2023-08-29T18:21:15.272Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.696.0>:leader_quorum_nodes_manager:handle_quorum_nodes_updated:155]Somebody else updated the quorum nodes when we are the master node.
Our quorum nodes: ['ns_1@cb.local']
Their quorum nodes: ['ns_1@127.0.0.1']
[ns_server:debug,2023-08-29T18:21:15.272Z,ns_1@127.0.0.1:leader_activities<0.681.0>:leader_activities:handle_internal_process_down:433]Process {quorum_nodes_manager,<0.696.0>} terminated with reason {shutdown,
                                                                 {quorum_nodes_update_conflict,
                                                                  ['ns_1@cb.local'],
                                                                  ['ns_1@127.0.0.1']}}
[ns_server:debug,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_apply_new_settings:601]Settings didn't change, ignoring update
[ns_server:debug,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{12,63860552475}}]}]
[error_logger:error,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,mb_master_sup}
    errorContext: child_terminated
    reason: {shutdown,
                {quorum_nodes_update_conflict,
                    ['ns_1@cb.local'],
                    ['ns_1@127.0.0.1']}}
    offender: [{pid,<0.696.0>},
               {id,leader_quorum_nodes_manager},
               {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
               {restart_type,permanent},
               {significant,false},
               {shutdown,1000},
               {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.272Z,ns_1@127.0.0.1:<0.613.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":26,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:<0.700.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.696.0>} exited with reason {shutdown,
                                                                                {quorum_nodes_update_conflict,
                                                                                 ['ns_1@cb.local'],
                                                                                 ['ns_1@127.0.0.1']}}
[ns_server:debug,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.13841.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:info,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:ns_ssl_services_setup<0.288.0>:ns_ssl_services_setup:handle_info:656]cert_and_pkey changed
[ns_server:debug,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',{project_intact,is_vulnerable}} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|false]
[ns_server:debug,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',xdcr_rest_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9998]
[ns_server:debug,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',uuid} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|
 <<"a2acfe035c01f55c0d78fce935ad3d90">>]
[error_logger:info,2023-08-29T18:21:15.273Z,ns_1@127.0.0.1:mb_master_sup<0.693.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13841.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.274Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ssl_rest_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|18091]
[ns_server:debug,2023-08-29T18:21:15.274Z,ns_1@127.0.0.1:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_apply_new_settings:601]Settings didn't change, ignoring update
[ns_server:debug,2023-08-29T18:21:15.274Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ssl_query_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|18093]
[ns_server:debug,2023-08-29T18:21:15.274Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ssl_capi_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|18092]
[ns_server:debug,2023-08-29T18:21:15.275Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',saslauthd_enabled} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|true]
[ns_server:debug,2023-08-29T18:21:15.275Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',rest} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {port,8091},
 {port_meta,global}]
[ns_server:debug,2023-08-29T18:21:15.277Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',query_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|8093]
[ns_server:debug,2023-08-29T18:21:15.279Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',prometheus_http_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9123]
[ns_server:debug,2023-08-29T18:21:15.280Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',projector_ssl_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9999]
[ns_server:debug,2023-08-29T18:21:15.276Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([quorum_nodes,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {node,'ns_1@127.0.0.1',address_family},
                               {node,'ns_1@127.0.0.1',audit},
                               {node,'ns_1@127.0.0.1',backup_grpc_port},
                               {node,'ns_1@127.0.0.1',backup_http_port},
                               {node,'ns_1@127.0.0.1',backup_https_port},
                               {node,'ns_1@127.0.0.1',capi_port},
                               {node,'ns_1@127.0.0.1',cbas_admin_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_client_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_cc_http_port},
                               {node,'ns_1@127.0.0.1',cbas_cluster_port},
                               {node,'ns_1@127.0.0.1',cbas_console_port},
                               {node,'ns_1@127.0.0.1',cbas_data_port},
                               {node,'ns_1@127.0.0.1',cbas_debug_port},
                               {node,'ns_1@127.0.0.1',cbas_dirs},
                               {node,'ns_1@127.0.0.1',cbas_http_port},
                               {node,'ns_1@127.0.0.1',cbas_messaging_port},
                               {node,'ns_1@127.0.0.1',
                                   cbas_metadata_callback_port},
                               {node,'ns_1@127.0.0.1',cbas_metadata_port},
                               {node,'ns_1@127.0.0.1',cbas_parent_port},
                               {node,'ns_1@127.0.0.1',cbas_replication_port},
                               {node,'ns_1@127.0.0.1',cbas_result_port},
                               {node,'ns_1@127.0.0.1',cbas_ssl_port},
                               {node,'ns_1@127.0.0.1',compaction_daemon},
                               {node,'ns_1@127.0.0.1',config_version},
                               {node,'ns_1@127.0.0.1',database_dir},
                               {node,'ns_1@127.0.0.1',erl_external_listeners},
                               {node,'ns_1@127.0.0.1',event_log},
                               {node,'ns_1@127.0.0.1',eventing_debug_port},
                               {node,'ns_1@127.0.0.1',eventing_dir},
                               {node,'ns_1@127.0.0.1',eventing_http_port},
                               {node,'ns_1@127.0.0.1',eventing_https_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_port},
                               {node,'ns_1@127.0.0.1',fts_grpc_ssl_port},
                               {node,'ns_1@127.0.0.1',fts_http_port},
                               {node,'ns_1@127.0.0.1',fts_ssl_port},
                               {node,'ns_1@127.0.0.1',index_dir},
                               {node,'ns_1@127.0.0.1',indexer_admin_port},
                               {node,'ns_1@127.0.0.1',indexer_http_port},
                               {node,'ns_1@127.0.0.1',indexer_https_port},
                               {node,'ns_1@127.0.0.1',indexer_scan_port},
                               {node,'ns_1@127.0.0.1',indexer_stcatchup_port},
                               {node,'ns_1@127.0.0.1',indexer_stinit_port},
                               {node,'ns_1@127.0.0.1',indexer_stmaint_port},
                               {node,'ns_1@127.0.0.1',is_enterprise},
                               {node,'ns_1@127.0.0.1',isasl},
                               {node,'ns_1@127.0.0.1',memcached},
                               {node,'ns_1@127.0.0.1',memcached_config},
                               {node,'ns_1@127.0.0.1',
                                   memcached_dedicated_ssl_port},
                               {node,'ns_1@127.0.0.1',memcached_defaults},
                               {node,'ns_1@127.0.0.1',memcached_prometheus},
                               {node,'ns_1@127.0.0.1',node_cert},
                               {node,'ns_1@127.0.0.1',node_encryption},
                               {node,'ns_1@127.0.0.1',ns_log},
                               {node,'ns_1@127.0.0.1',port_servers},
                               {node,'ns_1@127.0.0.1',projector_port},
                               {node,'ns_1@127.0.0.1',projector_ssl_port},
                               {node,'ns_1@127.0.0.1',prometheus_auth_info},
                               {node,'ns_1@127.0.0.1',prometheus_http_port},
                               {node,'ns_1@127.0.0.1',query_port},
                               {node,'ns_1@127.0.0.1',rest},
                               {node,'ns_1@127.0.0.1',saslauthd_enabled}]..)
[ns_server:debug,2023-08-29T18:21:15.280Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',projector_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9999]
[ns_server:debug,2023-08-29T18:21:15.281Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',port_servers} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}]
[ns_server:debug,2023-08-29T18:21:15.284Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.13841.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:debug,2023-08-29T18:21:15.288Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',ns_log} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/ns_log"}]
[ns_server:debug,2023-08-29T18:21:15.289Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_prometheus} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|11280]
[ns_server:debug,2023-08-29T18:21:15.290Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_defaults} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {max_connections,65000},
 {system_connections,5000},
 {connection_idle_time,0},
 {verbosity,0},
 {privilege_debug,false},
 {breakpad_enabled,true},
 {breakpad_minidump_dir_path,"/opt/couchbase/var/lib/couchbase/crash"},
 {dedupe_nmvb_maps,false},
 {je_malloc_conf,undefined},
 {tracing_enabled,true},
 {datatype_snappy,true},
 {num_reader_threads,<<"default">>},
 {num_writer_threads,<<"default">>},
 {num_auxio_threads,<<"default">>},
 {num_nonio_threads,<<"default">>},
 {num_storage_threads,<<"default">>},
 {connection_limit_mode,<<"disconnect">>},
 {free_connection_pool_size,0},
 {tcp_keepalive_idle,360},
 {tcp_keepalive_interval,10},
 {tcp_keepalive_probes,3},
 {max_client_connection_details,0}]
[ns_server:debug,2023-08-29T18:21:15.291Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_dedicated_ssl_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|11206]
[ns_server:debug,2023-08-29T18:21:15.292Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached_config} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|
 {[{interfaces,{memcached_config_mgr,get_interfaces,[]}},
   {client_cert_auth,{memcached_config_mgr,client_cert_auth,[]}},
   {connection_idle_time,connection_idle_time},
   {privilege_debug,privilege_debug},
   {breakpad,
    {[{enabled,breakpad_enabled},
      {minidump_dir,{memcached_config_mgr,get_minidump_dir,[]}}]}},
   {admin,{"~s",[admin_user]}},
   {verbosity,verbosity},
   {audit_file,{"~s",[audit_file]}},
   {rbac_file,{"~s",[rbac_file]}},
   {dedupe_nmvb_maps,dedupe_nmvb_maps},
   {tracing_enabled,tracing_enabled},
   {datatype_snappy,{memcached_config_mgr,is_snappy_enabled,[]}},
   {xattr_enabled,true},
   {scramsha_fallback_salt,{memcached_config_mgr,get_fallback_salt,[]}},
   {collections_enabled,{memcached_config_mgr,collections_enabled,[]}},
   {enforce_tenant_limits_enabled,
    {memcached_config_mgr,should_enforce_limits,[]}},
   {max_connections,max_connections},
   {system_connections,system_connections},
   {num_reader_threads,num_reader_threads},
   {num_writer_threads,num_writer_threads},
   {num_auxio_threads,num_auxio_threads},
   {num_nonio_threads,num_nonio_threads},
   {num_storage_threads,num_storage_threads},
   {logger,
    {[{filename,{"~s/~s",[log_path,log_prefix]}},{cyclesize,log_cyclesize}]}},
   {external_auth_service,{memcached_config_mgr,get_external_auth_service,[]}},
   {active_external_users_push_interval,
    {memcached_config_mgr,get_external_users_push_interval,[]}},
   {prometheus,{memcached_config_mgr,prometheus_cfg,[]}},
   {connection_limit_mode,connection_limit_mode},
   {free_connection_pool_size,free_connection_pool_size},
   {tcp_keepalive_idle,tcp_keepalive_idle},
   {tcp_keepalive_interval,tcp_keepalive_interval},
   {tcp_keepalive_probes,tcp_keepalive_probes},
   {max_client_connection_details,max_client_connection_details}]}]
[ns_server:debug,2023-08-29T18:21:15.294Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',memcached} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {port,11210},
 {dedicated_port,11209},
 {dedicated_ssl_port,11206},
 {ssl_port,11207},
 {admin_user,"@ns_server"},
 {other_users,["@cbq-engine","@projector","@goxdcr","@index","@fts",
               "@eventing","@cbas","@backup"]},
 {admin_pass,"*****"},
 {engines,[{membase,[{engine,"/opt/couchbase/lib/memcached/ep.so"},
                     {static_config_string,"failpartialwarmup=false"}]},
           {memcached,[{engine,"/opt/couchbase/lib/memcached/default_engine.so"},
                       {static_config_string,"vb0=true"}]}]},
 {config_path,"/opt/couchbase/var/lib/couchbase/config/memcached.json"},
 {audit_file,"/opt/couchbase/var/lib/couchbase/config/audit.json"},
 {rbac_file,"/opt/couchbase/var/lib/couchbase/config/memcached.rbac"},
 {log_path,"/opt/couchbase/var/lib/couchbase/logs"},
 {log_prefix,"memcached.log"},
 {log_generations,20},
 {log_cyclesize,10485760},
 {log_rotation_period,39003}]
[ns_server:debug,2023-08-29T18:21:15.295Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',isasl} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {path,"/opt/couchbase/var/lib/couchbase/isasl.pw"}]
[ns_server:debug,2023-08-29T18:21:15.295Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',is_enterprise} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|true]
[ns_server:debug,2023-08-29T18:21:15.296Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_stmaint_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9105]
[ns_server:debug,2023-08-29T18:21:15.296Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_stinit_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9103]
[ns_server:debug,2023-08-29T18:21:15.297Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_stcatchup_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9104]
[ns_server:debug,2023-08-29T18:21:15.297Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_scan_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9101]
[ns_server:debug,2023-08-29T18:21:15.297Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_https_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|19102]
[ns_server:debug,2023-08-29T18:21:15.297Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_http_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9102]
[ns_server:debug,2023-08-29T18:21:15.298Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',indexer_admin_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9100]
[ns_server:debug,2023-08-29T18:21:15.298Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',index_dir} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2023-08-29T18:21:15.299Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_ssl_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|18094]
[ns_server:debug,2023-08-29T18:21:15.299Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_http_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|8094]
[ns_server:debug,2023-08-29T18:21:15.299Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_grpc_ssl_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|19130]
[ns_server:debug,2023-08-29T18:21:15.299Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',fts_grpc_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9130]
[ns_server:debug,2023-08-29T18:21:15.300Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_https_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|18096]
[ns_server:debug,2023-08-29T18:21:15.300Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_http_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|8096]
[ns_server:debug,2023-08-29T18:21:15.300Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_debug_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9140]
[ns_server:debug,2023-08-29T18:21:15.301Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',event_log} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {filename,"/opt/couchbase/var/lib/couchbase/event_log"}]
[ns_server:debug,2023-08-29T18:21:15.301Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',database_dir} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2023-08-29T18:21:15.301Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',config_version} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|{7,2}]
[ns_server:debug,2023-08-29T18:21:15.301Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',compaction_daemon} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {check_interval,30},
 {min_db_file_size,131072},
 {min_view_file_size,20971520}]
[ns_server:debug,2023-08-29T18:21:15.302Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_ssl_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|18095]
[ns_server:debug,2023-08-29T18:21:15.304Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_result_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9117]
[ns_server:debug,2023-08-29T18:21:15.304Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_replication_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9120]
[ns_server:debug,2023-08-29T18:21:15.305Z,ns_1@127.0.0.1:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_apply_new_settings:601]Settings didn't change, ignoring update
[ns_server:debug,2023-08-29T18:21:15.307Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_parent_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9122]
[ns_server:debug,2023-08-29T18:21:15.308Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9121]
[ns_server:debug,2023-08-29T18:21:15.309Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_metadata_callback_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9119]
[ns_server:debug,2023-08-29T18:21:15.309Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_messaging_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9118]
[ns_server:debug,2023-08-29T18:21:15.309Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_http_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|8095]
[ns_server:debug,2023-08-29T18:21:15.310Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_debug_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|-1]
[ns_server:debug,2023-08-29T18:21:15.310Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_data_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9116]
[ns_server:debug,2023-08-29T18:21:15.310Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_console_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9114]
[ns_server:debug,2023-08-29T18:21:15.310Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cluster_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9115]
[ns_server:debug,2023-08-29T18:21:15.313Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cc_http_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9111]
[ns_server:debug,2023-08-29T18:21:15.314Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cc_cluster_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9112]
[ns_server:debug,2023-08-29T18:21:15.314Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_cc_client_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9113]
[ns_server:debug,2023-08-29T18:21:15.316Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_admin_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9110]
[ns_server:debug,2023-08-29T18:21:15.316Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',capi_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|8092]
[ns_server:debug,2023-08-29T18:21:15.317Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',backup_https_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|18097]
[ns_server:debug,2023-08-29T18:21:15.317Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',backup_http_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|8097]
[ns_server:debug,2023-08-29T18:21:15.317Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:handle_cast:173]Synchronized with merger in 26 us
[ns_server:debug,2023-08-29T18:21:15.318Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',backup_grpc_port} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|9124]
[ns_server:debug,2023-08-29T18:21:15.318Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',audit} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}]
[ns_server:debug,2023-08-29T18:21:15.318Z,ns_1@127.0.0.1:<0.13768.0>:dist_manager:complete_rename:407]Node 'ns_1@cb.local' has been renamed to 'ns_1@127.0.0.1'.
[ns_server:debug,2023-08-29T18:21:15.318Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
quorum_nodes ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552475}}]},
 'ns_1@127.0.0.1']
[ns_server:debug,2023-08-29T18:21:15.319Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',address_family} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|inet]
[ns_server:debug,2023-08-29T18:21:15.319Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',node_encryption} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|false]
[ns_server:debug,2023-08-29T18:21:15.320Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',erl_external_listeners} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {inet,false}]
[ns_server:debug,2023-08-29T18:21:15.321Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',node_cert} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 {subject,<<"CN=Couchbase Server Node (127.0.0.1)">>},
 {not_after,64691827199},
 {verified_with,<<133,166,53,2,8,48,187,102,19,107,176,9,230,211,49,117>>},
 {type,generated},
 {load_timestamp,63860552090},
 {ca,<<"-----BEGIN CERTIFICATE-----\nMIIDITCCAgmgAwIBAgIIF3/tSsVH0d4wDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCQxIjAgBgNVBAMTGUNvdWNoYmFzZSBTZXJ2ZXIgYjUxZWI3\nNzUwggEiMA0GCSqGSIb3DQEBAQUAA4IBDwAwggEKAoIBAQCr9XJDH3HmSp0Vabu1\nQm1+A0/9t3DlYJP9BDjf5CmXOlsWpvUIAP24RQ9rQsu3gn8VvYYnkK2mFfF0SRo5\nxJX/EZd"...>>},
 {pem,<<"-----BEGIN CERTIFICATE-----\nMIIDOTCCAiGgAwIBAgIIF3/tSu5x35owDQYJKoZIhvcNAQELBQAwJDEiMCAGA1UE\nAxMZQ291Y2hiYXNlIFNlcnZlciBiNTFlYjc3NTAeFw0xMzAxMDEwMDAwMDBaFw00\nOTEyMzEyMzU5NTlaMCwxKjAoBgNVBAMTIUNvdWNoYmFzZSBTZXJ2ZXIgTm9kZSAo\nMTI3LjAuMC4xKTCCASIwDQYJKoZIhvcNAQEBBQADggEPADCCAQoCggEBAK7J80br\nemoHet3x/9SjInqRaDHbxjoA13fYRB6cqVMFucsd+STGbshOlFzbP1rVfQD7I/tr\nytn"...>>},
 {pkey_passphrase_settings,[]},
 {certs_epoch,0},
 {hostname,"127.0.0.1"}]
[ns_server:debug,2023-08-29T18:21:15.322Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',cbas_dirs} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 "/opt/couchbase/var/lib/couchbase/data"]
[ns_server:debug,2023-08-29T18:21:15.323Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',eventing_dir} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]},
 47,111,112,116,47,99,111,117,99,104,98,97,115,101,47,118,97,114,47,108,105,
 98,47,99,111,117,99,104,98,97,115,101,47,100,97,116,97]
[ns_server:debug,2023-08-29T18:21:15.323Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',prometheus_auth_info} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552475}}]}|
 {"@prometheus",
  {auth,
   [{<<"plain">>,
     {sanitized,<<"DxBp77BWEtDRnhVC01VIpYURs7Gn5zjftfzSQ4+vkRI=">>}},
    {<<"sha512">>,
     {[{<<"h">>,
        {sanitized,<<"cwWcSL12N4zWttNlTH4XtlOX2Mf/4sX4mzl99mnuuYk=">>}},
       {<<"s">>,
        <<"ZO4AsN0VtNAJ/HUwKHmxkQdPXcFYsiSQL5dJF13JYcY63pU/uLlEqkvwKoDIRcT6jdmUuzSJscSsaVB81udUOw==">>},
       {<<"i">>,4000}]}},
    {<<"sha256">>,
     {[{<<"h">>,
        {sanitized,<<"V58Yrqjy9WA2D9HoExukibtMDOTSf5pMNxy/+jhZKNo=">>}},
       {<<"s">>,<<"OoiOw4UgLXbJ8bO1hb3dwbpOtObbR8RTflQq9vde8pU=">>},
       {<<"i">>,4000}]}},
    {<<"sha1">>,
     {[{<<"h">>,
        {sanitized,<<"7N1ZV3XdMV7vuQtOnXnSnPCSq2dc/VxI1Ls4nQ7f+fI=">>}},
       {<<"s">>,<<"WwNr6OiWKpSPGlmapdb14zR+9Zw=">>},
       {<<"i">>,4000}]}}]}}]
[ns_server:debug,2023-08-29T18:21:15.360Z,ns_1@127.0.0.1:<0.613.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":26,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2023-08-29T18:21:15.373Z,ns_1@127.0.0.1:<0.490.0>:restartable:loop:65]Restarting child <0.492.0>
  MFA: {ns_doctor_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.13768.0>,#Ref<0.4274179209.2407792644.224275>}
[ns_server:debug,2023-08-29T18:21:15.374Z,ns_1@127.0.0.1:<0.490.0>:restartable:shutdown_child:114]Successfully terminated process <0.492.0>
[error_logger:info,2023-08-29T18:21:15.376Z,ns_1@127.0.0.1:ns_doctor_sup<0.13860.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.13861.0>},
              {id,ns_doctor_events},
              {mfargs,{gen_event,start_link,[{local,ns_doctor_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.374Z,ns_1@127.0.0.1:<0.499.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.498.0>} exited with reason shutdown
[error_logger:info,2023-08-29T18:21:15.378Z,ns_1@127.0.0.1:ns_doctor_sup<0.13860.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_doctor_sup}
    started: [{pid,<0.13862.0>},
              {id,ns_doctor},
              {mfargs,{ns_doctor,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.379Z,ns_1@127.0.0.1:<0.490.0>:restartable:start_child:92]Started child process <0.13860.0>
  MFA: {ns_doctor_sup,start_link,[]}
[ns_server:debug,2023-08-29T18:21:15.380Z,ns_1@127.0.0.1:<0.678.0>:restartable:loop:65]Restarting child <0.679.0>
  MFA: {leader_services_sup,start_link,[]}
  Shutdown policy: infinity
  Caller: {<0.13768.0>,#Ref<0.4274179209.2407792642.228018>}
[ns_server:info,2023-08-29T18:21:15.382Z,ns_1@127.0.0.1:mb_master<0.691.0>:mb_master:terminate:298]Synchronously shutting down child mb_master_sup
[ns_server:debug,2023-08-29T18:21:15.382Z,ns_1@127.0.0.1:<0.772.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.771.0>} exited with reason shutdown
[ns_server:info,2023-08-29T18:21:15.383Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.771.0> registered as 'license_reporting' terminated.
[ns_server:debug,2023-08-29T18:21:15.383Z,ns_1@127.0.0.1:<0.761.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {compat_mode_events,<0.760.0>} exited with reason shutdown
[ns_server:info,2023-08-29T18:21:15.383Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.768.0> registered as 'tombstone_purger' terminated.
[ns_server:debug,2023-08-29T18:21:15.383Z,ns_1@127.0.0.1:<0.769.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.768.0>} exited with reason shutdown
[ns_server:info,2023-08-29T18:21:15.384Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.760.0> registered as 'auto_failover' terminated.
[ns_server:info,2023-08-29T18:21:15.384Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.758.0> registered as 'ns_orchestrator' terminated.
[ns_server:info,2023-08-29T18:21:15.384Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.757.0> registered as 'auto_rebalance' terminated.
[ns_server:debug,2023-08-29T18:21:15.384Z,ns_1@127.0.0.1:<0.707.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {'kv-events',<0.706.0>} exited with reason shutdown
[ns_server:info,2023-08-29T18:21:15.384Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.756.0> registered as 'auto_reprovision' terminated.
[ns_server:debug,2023-08-29T18:21:15.384Z,ns_1@127.0.0.1:<0.708.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.706.0>} exited with reason shutdown
[ns_server:debug,2023-08-29T18:21:15.385Z,ns_1@127.0.0.1:<0.13852.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.13841.0>} exited with reason shutdown
[ns_server:info,2023-08-29T18:21:15.385Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.706.0> registered as 'chronicle_master' terminated.
[ns_server:warn,2023-08-29T18:21:15.385Z,ns_1@127.0.0.1:<0.13827.0>:async:maybe_log_down_message:478]Monitored process <0.701.0> terminated abnormally (reason = killed)
[ns_server:info,2023-08-29T18:21:15.385Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_down:286]Process <0.702.0> registered as 'ns_tick' terminated.
[ns_server:debug,2023-08-29T18:21:15.385Z,ns_1@127.0.0.1:leader_activities<0.681.0>:leader_activities:handle_internal_process_down:433]Process {quorum_nodes_manager,<0.13841.0>} terminated with reason shutdown
[ns_server:debug,2023-08-29T18:21:15.386Z,ns_1@127.0.0.1:leader_registry<0.689.0>:leader_registry:handle_new_leader:275]New leader is undefined. Invalidating name cache.
[ns_server:debug,2023-08-29T18:21:15.386Z,ns_1@127.0.0.1:<0.695.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_node_disco_events,<0.694.0>} exited with reason shutdown
[ns_server:debug,2023-08-29T18:21:15.386Z,ns_1@127.0.0.1:leader_activities<0.681.0>:leader_activities:handle_internal_process_down:433]Process {acquirer,<0.694.0>} terminated with reason shutdown
[ns_server:debug,2023-08-29T18:21:15.386Z,ns_1@127.0.0.1:<0.692.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {chronicle_compat_event_manager,<0.691.0>} exited with reason shutdown
[ns_server:debug,2023-08-29T18:21:15.386Z,ns_1@127.0.0.1:<0.690.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {leader_events,<0.689.0>} exited with reason shutdown
[ns_server:warn,2023-08-29T18:21:15.386Z,ns_1@127.0.0.1:leader_lease_agent<0.682.0>:leader_lease_agent:handle_terminate:302]Terminating with reason shutdown when we own an active lease:
{lease,
    {lease_holder,<<"c068c6b6b1dd5df104ba1bbe14a568e8">>,'ns_1@cb.local'},
    -576460362482667794,-576460347482667794,
    {timer,#Ref<0.4274179209.2407792642.227867>,
        {lease_expired,
            {lease_holder,<<"c068c6b6b1dd5df104ba1bbe14a568e8">>,
                'ns_1@cb.local'}}},
    active}
Persisting updated lease.
[ns_server:debug,2023-08-29T18:21:15.513Z,ns_1@127.0.0.1:leader_activities<0.681.0>:leader_activities:handle_internal_process_down:433]Process {agent,<0.682.0>} terminated with reason shutdown
[ns_server:debug,2023-08-29T18:21:15.514Z,ns_1@127.0.0.1:<0.678.0>:restartable:shutdown_child:114]Successfully terminated process <0.679.0>
[error_logger:info,2023-08-29T18:21:15.516Z,ns_1@127.0.0.1:leader_leases_sup<0.13867.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.13868.0>},
              {id,leader_activities},
              {mfargs,{leader_activities,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:warn,2023-08-29T18:21:15.527Z,ns_1@127.0.0.1:leader_lease_agent<0.13869.0>:leader_lease_agent:maybe_recover_persisted_lease:390]Found persisted lease [{node,'ns_1@cb.local'},
                       {uuid,<<"c068c6b6b1dd5df104ba1bbe14a568e8">>},
                       {time_left,12705},
                       {status,active}]
[error_logger:info,2023-08-29T18:21:15.528Z,ns_1@127.0.0.1:leader_leases_sup<0.13867.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_leases_sup}
    started: [{pid,<0.13869.0>},
              {id,leader_lease_agent},
              {mfargs,{leader_lease_agent,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:15.529Z,ns_1@127.0.0.1:leader_services_sup<0.13866.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.13867.0>},
              {id,leader_leases_sup},
              {mfargs,{leader_leases_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:15.530Z,ns_1@127.0.0.1:leader_registry_sup<0.13870.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.13871.0>},
              {id,leader_registry},
              {mfargs,{leader_registry,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.531Z,ns_1@127.0.0.1:leader_registry_sup<0.13870.0>:mb_master:check_master_takeover_needed:254]Sending master node question to the following nodes: []
[ns_server:debug,2023-08-29T18:21:15.531Z,ns_1@127.0.0.1:leader_registry_sup<0.13870.0>:mb_master:check_master_takeover_needed:256]Got replies: []
[ns_server:debug,2023-08-29T18:21:15.531Z,ns_1@127.0.0.1:leader_registry_sup<0.13870.0>:mb_master:check_master_takeover_needed:262]Was unable to discover master, not going to force mastership takeover
[ns_server:debug,2023-08-29T18:21:15.532Z,ns_1@127.0.0.1:mb_master<0.13873.0>:mb_master:init:80]Heartbeat interval is 2000
[user:info,2023-08-29T18:21:15.532Z,ns_1@127.0.0.1:mb_master<0.13873.0>:mb_master:init:85]I'm the only node, so I'm the master.
[ns_server:debug,2023-08-29T18:21:15.533Z,ns_1@127.0.0.1:leader_registry<0.13871.0>:leader_registry:handle_new_leader:275]New leader is 'ns_1@127.0.0.1'. Invalidating name cache.
[error_logger:info,2023-08-29T18:21:15.533Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13876.0>},
              {id,leader_lease_acquirer},
              {mfargs,{leader_lease_acquirer,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.534Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.13879.0>:leader_quorum_nodes_manager:pull_config:99]Attempting to pull config from nodes:
[]
[ns_server:debug,2023-08-29T18:21:15.534Z,ns_1@127.0.0.1:leader_quorum_nodes_manager<0.13879.0>:leader_quorum_nodes_manager:pull_config:104]Pulled config successfully.
[ns_server:warn,2023-08-29T18:21:15.534Z,ns_1@127.0.0.1:<0.13878.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@cb.local',
                                                                            <<"c068c6b6b1dd5df104ba1bbe14a568e8">>} (valid for 12698ms)
[error_logger:info,2023-08-29T18:21:15.534Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13879.0>},
              {id,leader_quorum_nodes_manager},
              {mfargs,{leader_quorum_nodes_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:21:15.535Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,ns_tick},
                                         ns_tick,[],[]]): started as <0.13885.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.535Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13885.0>},
              {id,ns_tick},
              {mfargs,{ns_tick,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,10},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.535Z,ns_1@127.0.0.1:<0.13886.0>:chronicle_master:do_init:141]Starting with SelfRef = #Ref<0.4274179209.2407792643.227527>
[ns_server:info,2023-08-29T18:21:15.536Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:misc:start_singleton:901]start_singleton(gen_server2, start_link, [{via,leader_registry,
                                           chronicle_master},
                                          chronicle_master,[],[]]): started as <0.13886.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.536Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13886.0>},
              {id,chronicle_master},
              {mfargs,{chronicle_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:15.536Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.13889.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.13890.0>},
              {id,compat_mode_events},
              {mfargs,{gen_event,start_link,[{local,compat_mode_events}]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:15.538Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.13889.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.13891.0>},
              {id,compat_mode_manager},
              {mfargs,{compat_mode_manager,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:15.539Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.13892.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.13893.0>},
              {id,ns_janitor_server},
              {mfargs,{ns_janitor_server,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:21:15.539Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.13892.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          auto_reprovision},
                                         auto_reprovision,[],[]]): started as <0.13894.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.540Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.13892.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.13894.0>},
              {id,auto_reprovision},
              {mfargs,{auto_reprovision,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:21:15.541Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.13892.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_rebalance},
                                         auto_rebalance,[],[]]): started as <0.13895.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.542Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.13892.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.13895.0>},
              {id,auto_rebalance},
              {mfargs,{auto_rebalance,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:info,2023-08-29T18:21:15.543Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.13892.0>:misc:start_singleton:901]start_singleton(gen_statem, start_link, [{via,leader_registry,ns_orchestrator},
                                         ns_orchestrator,[],[]]): started as <0.13896.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.544Z,ns_1@127.0.0.1:ns_orchestrator_child_sup<0.13892.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_child_sup}
    started: [{pid,<0.13896.0>},
              {id,ns_orchestrator},
              {mfargs,{ns_orchestrator,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:15.544Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.13889.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.13892.0>},
              {id,ns_orchestrator_child_sup},
              {mfargs,{ns_orchestrator_child_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:21:15.545Z,ns_1@127.0.0.1:<0.13898.0>:auto_failover:init:198]init auto_failover.
[user:info,2023-08-29T18:21:15.545Z,ns_1@127.0.0.1:<0.13898.0>:auto_failover:handle_call:229]Enabled auto-failover with timeout 120 and max count 1
[ns_server:info,2023-08-29T18:21:15.546Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.13889.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,auto_failover},
                                         auto_failover,[],[]]): started as <0.13898.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.546Z,ns_1@127.0.0.1:ns_orchestrator_sup<0.13889.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_orchestrator_sup}
    started: [{pid,<0.13898.0>},
              {id,auto_failover},
              {mfargs,{auto_failover,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:15.547Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13889.0>},
              {id,ns_orchestrator_sup},
              {mfargs,{ns_orchestrator_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:info,2023-08-29T18:21:15.548Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          tombstone_purger},
                                         tombstone_purger,[],[]]): started as <0.13900.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.549Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13900.0>},
              {id,tombstone_purger},
              {mfargs,{tombstone_purger,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:21:15.549Z,ns_1@127.0.0.1:<0.13902.0>:license_reporting:init:60]Starting license_reporting server
[ns_server:info,2023-08-29T18:21:15.550Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:misc:start_singleton:901]start_singleton(gen_server, start_link, [{via,leader_registry,
                                          license_reporting},
                                         license_reporting,[],[]]): started as <0.13902.0> on 'ns_1@127.0.0.1'

[error_logger:info,2023-08-29T18:21:15.550Z,ns_1@127.0.0.1:mb_master_sup<0.13875.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,mb_master_sup}
    started: [{pid,<0.13902.0>},
              {id,license_reporting},
              {mfargs,{license_reporting,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,1000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:21:15.551Z,ns_1@127.0.0.1:leader_registry_sup<0.13870.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_registry_sup}
    started: [{pid,<0.13873.0>},
              {id,mb_master},
              {mfargs,{mb_master,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[error_logger:info,2023-08-29T18:21:15.552Z,ns_1@127.0.0.1:leader_services_sup<0.13866.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,leader_services_sup}
    started: [{pid,<0.13870.0>},
              {id,leader_registry_sup},
              {mfargs,{leader_registry_sup,start_link,[]}},
              {restart_type,permanent},
              {significant,false},
              {shutdown,infinity},
              {child_type,supervisor}]

[ns_server:debug,2023-08-29T18:21:15.552Z,ns_1@127.0.0.1:<0.678.0>:restartable:start_child:92]Started child process <0.13866.0>
  MFA: {leader_services_sup,start_link,[]}
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:remote_monitors<0.277.0>:remote_monitors:handle_info:86]Node renaming transaction ended. MRef = #Ref<0.4274179209.2407792642.227918>
[cluster:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:765]Renamed node from 'ns_1@cb.local' to 'ns_1@127.0.0.1'.
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:ns_node_disco<0.450.0>:ns_node_disco:handle_info:174]Node renaming transaction ended. MRef = #Ref<0.4274179209.2407792641.232339>
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:ns_cookie_manager<0.221.0>:ns_cookie_manager:do_cookie_sync:101]ns_cookie_manager do_cookie_sync
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.377.0>:ns_server_nodes_sup:wait_link_to_couchdb_node_loop:197]Link to couchdb node was unpaused.
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:ns_ports_setup<0.581.0>:ns_ports_setup:children_loop_continue:93]Remote monitor <16118.129.0> was unpaused after node name change. Restart loop.
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:memcached_config_mgr<0.801.0>:memcached_config_mgr:handle_info:195]Got DOWN with reason: unpaused from memcached port server: <16118.135.0>. Shutting down
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:<0.613.0>:terse_cluster_info_uploader:handle_info:64]Got DOWN with reason: unpaused from memcached port server: <16118.135.0>. Shutting down
[cluster:info,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:734]Renamed node. New name is 'ns_1@127.0.0.1'.
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:<0.13904.0>:ns_node_disco:do_nodes_wanted_updated_fun:224]ns_node_disco: nodes_wanted updated: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                       <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:21:15.561Z,ns_1@127.0.0.1:<0.614.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {bucket_info_cache_invalidations,<0.613.0>} exited with reason {shutdown,
                                                                                               {memcached_port_server_down,
                                                                                                <16118.135.0>,
                                                                                                unpaused}}
[ns_server:debug,2023-08-29T18:21:15.562Z,ns_1@127.0.0.1:<0.13904.0>:ns_node_disco:do_nodes_wanted_updated_fun:227]ns_node_disco: nodes_wanted pong: ['ns_1@127.0.0.1'], with cookie: {sanitized,
                                                                    <<"b1l/2DcbTAH/J5q7aHKAl/NNbXYJIozmg1GT/frwzww=">>}
[ns_server:debug,2023-08-29T18:21:15.561Z,ns_1@127.0.0.1:<0.803.0>:ns_pubsub:do_subscribe_link_continue:150]Parent process of subscription {ns_config_events,<0.801.0>} exited with reason {shutdown,
                                                                                {memcached_port_server_down,
                                                                                 <16118.135.0>,
                                                                                 unpaused}}
[ns_server:debug,2023-08-29T18:21:15.561Z,ns_1@127.0.0.1:wait_link_to_couchdb_node<0.377.0>:ns_server_nodes_sup:do_wait_link_to_couchdb_node:153]Waiting for ns_couchdb node to start
[ns_server:debug,2023-08-29T18:21:15.560Z,ns_1@127.0.0.1:ns_node_disco_events<0.448.0>:ns_config_rep:handle_node_disco_event:512]Detected new nodes (['ns_1@127.0.0.1']).  Moving config around.
[ns_server:info,2023-08-29T18:21:15.562Z,ns_1@127.0.0.1:ns_node_disco_events<0.448.0>:ns_node_disco_log:handle_event:40]ns_node_disco_log: nodes changed: ['ns_1@127.0.0.1']
[error_logger:error,2023-08-29T18:21:15.562Z,ns_1@127.0.0.1:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16118.135.0>,unpaused}}
    offender: [{pid,<0.801.0>},
               {name,memcached_config_mgr},
               {mfargs,{memcached_config_mgr,start_link,[]}},
               {restart_type,{permanent,4}},
               {shutdown,1000},
               {child_type,worker}]
[ns_server:debug,2023-08-29T18:21:15.563Z,ns_1@127.0.0.1:memcached_config_mgr<0.13906.0>:memcached_config_mgr:memcached_port_pid:151]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-08-29T18:21:15.563Z,ns_1@127.0.0.1:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.13906.0>},
              {name,memcached_config_mgr},
              {mfargs,{memcached_config_mgr,start_link,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[error_logger:error,2023-08-29T18:21:15.564Z,ns_1@127.0.0.1:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {shutdown,{memcached_port_server_down,<16118.135.0>,unpaused}}
    offender: [{pid,<0.613.0>},
               {name,terse_cluster_info_uploader},
               {mfargs,{terse_cluster_info_uploader,start_link,[]}},
               {restart_type,permanent},
               {shutdown,1000},
               {child_type,worker}]
[ns_server:debug,2023-08-29T18:21:15.566Z,ns_1@127.0.0.1:<0.13907.0>:memcached_config_mgr:memcached_port_pid:151]waiting for completion of initial ns_ports_setup round
[error_logger:info,2023-08-29T18:21:15.566Z,ns_1@127.0.0.1:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.13907.0>},
              {name,terse_cluster_info_uploader},
              {mfargs,{terse_cluster_info_uploader,start_link,[]}},
              {restart_type,permanent},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:21:15.574Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit rename_node: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                    {remote,{[{ip,<<"172.17.0.1">>},{port,48056}]}},
                    {real_userid,{[{domain,wrong_token},
                                   {user,<<"<ud></ud>">>}]}},
                    {timestamp,<<"2023-08-29T18:21:15.573Z">>},
                    {hostname,<<"127.0.0.1">>},
                    {node,'ns_1@127.0.0.1'}]
[ns_server:debug,2023-08-29T18:21:15.575Z,ns_1@127.0.0.1:ns_ports_setup<0.581.0>:ns_ports_manager:set_dynamic_children:48]Setting children [memcached,saslauthd_port,goxdcr]
[ns_server:debug,2023-08-29T18:21:15.602Z,ns_1@127.0.0.1:ns_ports_setup<0.581.0>:ns_ports_setup:set_children:60]Monitor ns_child_ports_sup <16118.129.0>
[ns_server:debug,2023-08-29T18:21:15.603Z,ns_1@127.0.0.1:memcached_config_mgr<0.13906.0>:memcached_config_mgr:memcached_port_pid:153]ns_ports_setup seems to be ready
[ns_server:debug,2023-08-29T18:21:15.603Z,ns_1@127.0.0.1:<0.13907.0>:memcached_config_mgr:memcached_port_pid:153]ns_ports_setup seems to be ready
[ns_server:debug,2023-08-29T18:21:15.616Z,ns_1@127.0.0.1:memcached_config_mgr<0.13906.0>:memcached_config_mgr:find_port_pid_loop:161]Found memcached port <16118.135.0>
[ns_server:debug,2023-08-29T18:21:15.621Z,ns_1@127.0.0.1:<0.13907.0>:memcached_config_mgr:find_port_pid_loop:161]Found memcached port <16118.135.0>
[ns_server:debug,2023-08-29T18:21:15.622Z,ns_1@127.0.0.1:<0.13907.0>:terse_cluster_info_uploader:handle_info:53]Refreshing terse cluster info with <<"{\"rev\":26,\"nodesExt\":[{\"services\":{\"capi\":8092,\"capiSSL\":18092,\"kv\":11210,\"kvSSL\":11207,\"mgmt\":8091,\"mgmtSSL\":18091,\"projector\":9999},\"thisNode\":true}],\"clusterCapabilitiesVer\":[1,0],\"clusterCapabilities\":{\"n1ql\":[\"enhancedPreparedStatements\"]},\"revEpoch\":1}">>
[ns_server:debug,2023-08-29T18:21:15.623Z,ns_1@127.0.0.1:memcached_config_mgr<0.13906.0>:memcached_config_mgr:do_read_current_memcached_config:368]Got enoent while trying to read active memcached config from /opt/couchbase/var/lib/couchbase/config/memcached.json.prev
[ns_server:debug,2023-08-29T18:21:15.632Z,ns_1@127.0.0.1:memcached_config_mgr<0.13906.0>:memcached_config_mgr:init:99]found memcached port to be already active
[ns_server:info,2023-08-29T18:21:15.637Z,ns_1@127.0.0.1:memcached_config_mgr<0.13906.0>:memcached_config_mgr:push_tls_config:230]Pushing TLS config to memcached:
{[{<<"private key">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/pkey.pem">>},
  {<<"certificate chain">>,
   <<"/opt/couchbase/var/lib/couchbase/config/certs/chain.pem">>},
  {<<"CA file">>,<<"/opt/couchbase/var/lib/couchbase/config/certs/ca.pem">>},
  {<<"minimum version">>,<<"TLS 1.2">>},
  {<<"cipher list">>,
   {[{<<"TLS 1.2">>,<<"HIGH">>},
     {<<"TLS 1.3">>,
      <<"TLS_AES_256_GCM_SHA384:TLS_AES_128_GCM_SHA256:TLS_CHACHA20_POLY1305_SHA256">>}]}},
  {<<"cipher order">>,true},
  {<<"client cert auth">>,<<"disabled">>}]}
[ns_server:info,2023-08-29T18:21:15.669Z,ns_1@127.0.0.1:memcached_config_mgr<0.13906.0>:memcached_config_mgr:push_tls_config:234]Successfully pushed TLS config to memcached
[error_logger:error,2023-08-29T18:21:15.778Z,ns_1@127.0.0.1:<0.421.0>:ale_error_logger_handler:do_log:101]
=========================CRASH REPORT=========================
  crasher:
    initial call: ns_log:'-fun.babysitter_log_consumption_loop_tramp/0-'/0
    pid: <0.421.0>
    registered_name: []
    exception exit: {{nodedown,'babysitter_of_ns_1@cb.local'},
                     {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}}
      in function  gen_server:call/3 (gen_server.erl, line 247)
      in call from ns_log:babysitter_log_consumption_loop/0 (src/ns_log.erl, line 66)
      in call from misc:delaying_crash/2 (src/misc.erl, line 1734)
    ancestors: [ns_server_sup,ns_server_nodes_sup,<0.270.0>,
                  ns_server_cluster_sup,root_sup,<0.146.0>]
    message_queue_len: 3
    messages: [{[alias|#Ref<0.4274179209.2407858177.219004>],superseded},
                  {[alias|#Ref<15442.4274179209.2407858177.219008>],
                   superseded},
                  {[alias|#Ref<15442.4274179209.2407858177.219392>],
                   superseded}]
    links: [<0.407.0>]
    dictionary: []
    trap_exit: false
    status: running
    heap_size: 4185
    stack_size: 29
    reductions: 7525
  neighbours:

[error_logger:error,2023-08-29T18:21:15.779Z,ns_1@127.0.0.1:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================SUPERVISOR REPORT=========================
    supervisor: {local,ns_server_sup}
    errorContext: child_terminated
    reason: {{nodedown,'babysitter_of_ns_1@cb.local'},
             {gen_server,call,
                         [{ns_babysitter_log,'babysitter_of_ns_1@cb.local'},
                          consume,infinity]}}
    offender: [{pid,<0.421.0>},
               {name,ns_babysitter_log_consumer},
               {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
               {restart_type,{permanent,4}},
               {shutdown,1000},
               {child_type,worker}]
[error_logger:info,2023-08-29T18:21:15.780Z,ns_1@127.0.0.1:ns_server_sup<0.407.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {local,ns_server_sup}
    started: [{pid,<0.13922.0>},
              {name,ns_babysitter_log_consumer},
              {mfargs,{ns_log,start_link_babysitter_log_consumer,[]}},
              {restart_type,{permanent,4}},
              {shutdown,1000},
              {child_type,worker}]
[ns_server:debug,2023-08-29T18:21:20.043Z,ns_1@127.0.0.1:<0.13898.0>:auto_failover_logic:log_master_activity:141]Transitioned node {'ns_1@127.0.0.1',<<"a2acfe035c01f55c0d78fce935ad3d90">>} state new -> up
[cluster:info,2023-08-29T18:21:20.149Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:21:20.149Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:21:20.149Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 1 times in the past 5.382034 secs (last seen 5.382034 secs ago
[ns_server:info,2023-08-29T18:21:20.150Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 1 times in the past 5.381846 secs (last seen 5.381846 secs ago
[cluster:debug,2023-08-29T18:21:20.150Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:21:20.152Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{3,63860552480}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:21:20.153Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{13,63860552480}}]}]
[ns_server:debug,2023-08-29T18:21:20.153Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,48056}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:21:20.152Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:21:20.153Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{14,63860552480}}]}]
[ns_server:debug,2023-08-29T18:21:20.153Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:21:20.153Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:21:20.154Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{15,63860552480}}]}]
[ns_server:debug,2023-08-29T18:21:20.155Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:21:20.156Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:21:21.840Z,ns_1@127.0.0.1:cb_dist<0.13782.0>:cb_dist:info_msg:872]cb_dist: Connection down: {con,#Ref<0.4274179209.2407792643.227297>,
                               inet_tcp_dist,<0.13790.0>,
                               #Ref<0.4274179209.2407792642.227931>}
[error_logger:info,2023-08-29T18:21:21.840Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{'EXIT',<0.13790.0>,setup_timer_timeout}}
[error_logger:info,2023-08-29T18:21:21.840Z,ns_1@127.0.0.1:net_kernel<0.13785.0>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
{net_kernel,{net_kernel,1310,nodedown,'ns_1@cb.local'}}
[ns_server:debug,2023-08-29T18:21:26.765Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:21:26.766Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:21:26.767Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:21:26.767Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:21:28.234Z,ns_1@127.0.0.1:leader_lease_agent<0.13869.0>:leader_lease_agent:handle_lease_expired:277]Lease held by {lease_holder,<<"c068c6b6b1dd5df104ba1bbe14a568e8">>,
                            'ns_1@cb.local'} expired. Starting expirer.
[ns_server:warn,2023-08-29T18:21:28.235Z,ns_1@127.0.0.1:<0.13878.0>:leader_lease_acquire_worker:handle_lease_already_acquired:226]Failed to acquire lease from 'ns_1@127.0.0.1' because its already taken by {'ns_1@cb.local',
                                                                            <<"c068c6b6b1dd5df104ba1bbe14a568e8">>} (valid for 0ms)
[ns_server:debug,2023-08-29T18:21:28.242Z,ns_1@127.0.0.1:leader_lease_agent<0.13869.0>:leader_lease_agent:do_handle_acquire_lease:140]Granting lease to {lease_holder,<<"b910d8771c82727c5cabe9d03885ecb2">>,
                                'ns_1@127.0.0.1'} for 15000ms
[ns_server:info,2023-08-29T18:21:28.316Z,ns_1@127.0.0.1:<0.13878.0>:leader_lease_acquire_worker:handle_fresh_lease_acquired:296]Acquired lease from node 'ns_1@127.0.0.1' (lease uuid: <<"b910d8771c82727c5cabe9d03885ecb2">>)
[cluster:info,2023-08-29T18:21:28.552Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:21:28.553Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:21:28.553Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 2 times in the past 13.785247 secs (last seen 8.403213 secs ago
[ns_server:info,2023-08-29T18:21:28.553Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 2 times in the past 13.785298 secs (last seen 8.403452 secs ago
[cluster:debug,2023-08-29T18:21:28.553Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:21:28.557Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{4,63860552488}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:21:28.557Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{16,63860552488}}]}]
[ns_server:debug,2023-08-29T18:21:28.558Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{17,63860552488}}]}]
[ns_server:debug,2023-08-29T18:21:28.558Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,35704}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:21:28.557Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:21:28.559Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:21:28.559Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:21:28.560Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{18,63860552488}}]}]
[ns_server:debug,2023-08-29T18:21:28.561Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:21:28.563Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:21:29.813Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:21:29.814Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 3 times in the past 15.046104 secs (last seen 1.260857 secs ago
[cluster:info,2023-08-29T18:21:29.814Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:21:29.815Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:21:29.814Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 3 times in the past 15.046517 secs (last seen 1.261219 secs ago
[ns_server:debug,2023-08-29T18:21:29.819Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{5,63860552489}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:21:29.819Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:21:29.819Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{19,63860552489}}]}]
[ns_server:debug,2023-08-29T18:21:29.820Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{20,63860552489}}]}]
[ns_server:debug,2023-08-29T18:21:29.819Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,35714}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:21:29.819Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:21:29.820Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:21:29.821Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{21,63860552489}}]}]
[ns_server:debug,2023-08-29T18:21:29.821Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:21:29.821Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[cluster:info,2023-08-29T18:21:30.627Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:21:30.628Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 4 times in the past 15.860234 secs (last seen 0.81413 secs ago
[cluster:info,2023-08-29T18:21:30.628Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:21:30.629Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 4 times in the past 15.860559 secs (last seen 0.814042 secs ago
[cluster:debug,2023-08-29T18:21:30.629Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:21:30.633Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{6,63860552490}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:21:30.634Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{22,63860552490}}]}]
[ns_server:debug,2023-08-29T18:21:30.635Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{23,63860552490}}]}]
[ns_server:debug,2023-08-29T18:21:30.635Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,35720}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:21:30.634Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:21:30.634Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:21:30.636Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:21:30.637Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{24,63860552490}}]}]
[ns_server:debug,2023-08-29T18:21:30.638Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:21:30.642Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:21:31.235Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:21:31.235Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 5 times in the past 16.467973 secs (last seen 0.607739 secs ago
[cluster:info,2023-08-29T18:21:31.235Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:21:31.236Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:21:31.236Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 5 times in the past 16.467941 secs (last seen 0.607382 secs ago
[ns_server:debug,2023-08-29T18:21:31.238Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{7,63860552491}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:21:31.239Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:21:31.239Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{25,63860552491}}]}]
[ns_server:debug,2023-08-29T18:21:31.240Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{26,63860552491}}]}]
[ns_server:debug,2023-08-29T18:21:31.240Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,35730}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:21:31.239Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:21:31.240Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:21:31.241Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{27,63860552491}}]}]
[ns_server:debug,2023-08-29T18:21:31.242Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:21:31.244Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:21:55.034Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:21:55.035Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:21:55.035Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 6 times in the past 40.267469 secs (last seen 23.799496 secs ago
[cluster:debug,2023-08-29T18:21:55.035Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:21:55.035Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 6 times in the past 40.267159 secs (last seen 23.799218 secs ago
[ns_server:debug,2023-08-29T18:21:56.768Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:21:56.769Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:21:56.769Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:21:56.769Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:22:21.487Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:22:21.594Z,ns_1@127.0.0.1:roles_cache<0.366.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2023-08-29T18:22:21.594Z,ns_1@127.0.0.1:roles_cache<0.366.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-08-29T18:22:26.770Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:22:26.771Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:22:26.771Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:22:26.772Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:22:40.094Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:22:40.095Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:22:40.095Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:22:56.772Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:22:56.772Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:22:56.773Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:22:56.773Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:23:26.774Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:23:26.774Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:23:26.775Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:23:26.775Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:23:34.942Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:23:34.943Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:23:34.944Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:23:34.946Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{8,63860552614}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:23:34.947Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:23:34.947Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{28,63860552614}}]}]
[ns_server:debug,2023-08-29T18:23:34.948Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{29,63860552614}}]}]
[ns_server:debug,2023-08-29T18:23:34.948Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,43452}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:23:34.947Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:23:34.949Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:23:34.950Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{30,63860552614}}]}]
[ns_server:debug,2023-08-29T18:23:34.950Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:23:34.950Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:23:36.488Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:23:56.775Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:23:56.775Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:23:56.776Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:23:56.776Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:24:13.933Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:24:13.934Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:24:13.936Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:24:13.939Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{9,63860552653}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:13.939Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
memory_quota ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552653}}]}|300]
[ns_server:debug,2023-08-29T18:24:13.940Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{31,63860552653}}]}]
[ns_server:debug,2023-08-29T18:24:13.940Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,33466}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:13.939Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:13.941Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{32,63860552653}}]}]
[ns_server:debug,2023-08-29T18:24:13.942Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:13.942Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{33,63860552653}}]}]
[ns_server:debug,2023-08-29T18:24:13.941Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,memory_quota,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:13.943Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:13.945Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:18.564Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:24:18.566Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:18.565Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 1 times in the past 4.631688 secs (last seen 4.631688 secs ago
[cluster:debug,2023-08-29T18:24:18.566Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:24:18.566Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 1 times in the past 4.632032 secs (last seen 4.632032 secs ago
[ns_server:debug,2023-08-29T18:24:18.569Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:18.569Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{10,63860552658}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:18.570Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{34,63860552658}}]}]
[ns_server:debug,2023-08-29T18:24:18.570Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54850}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:18.569Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:18.571Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{35,63860552658}}]}]
[ns_server:debug,2023-08-29T18:24:18.571Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:18.571Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{36,63860552658}}]}]
[ns_server:debug,2023-08-29T18:24:18.572Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:18.573Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:19.326Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:24:19.327Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:19.327Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 2 times in the past 5.393477 secs (last seen 0.761789 secs ago
[ns_server:info,2023-08-29T18:24:19.327Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 2 times in the past 5.393195 secs (last seen 0.761163 secs ago
[cluster:debug,2023-08-29T18:24:19.327Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:24:19.331Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{11,63860552659}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:19.331Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54862}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:19.330Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:19.331Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:19.331Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{37,63860552659}}]}]
[ns_server:debug,2023-08-29T18:24:19.332Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{38,63860552659}}]}]
[ns_server:debug,2023-08-29T18:24:19.332Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:19.332Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{39,63860552659}}]}]
[ns_server:debug,2023-08-29T18:24:19.332Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:19.334Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:20.006Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:24:20.007Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 3 times in the past 6.07369 secs (last seen 0.680213 secs ago
[cluster:info,2023-08-29T18:24:20.007Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:20.007Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 3 times in the past 6.073575 secs (last seen 0.68038 secs ago
[cluster:debug,2023-08-29T18:24:20.008Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:24:20.011Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{12,63860552659}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:20.012Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{40,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.012Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{41,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.012Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54876}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:20.012Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:20.013Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:20.013Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:20.014Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{42,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.015Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:20.015Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:20.389Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:24:20.390Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 4 times in the past 6.45669 secs (last seen 0.383 secs ago
[cluster:info,2023-08-29T18:24:20.390Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:20.391Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 4 times in the past 6.456891 secs (last seen 0.383316 secs ago
[cluster:debug,2023-08-29T18:24:20.391Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:24:20.393Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{13,63860552660}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:20.393Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:20.394Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{43,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.394Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54890}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:20.394Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:20.394Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{44,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.394Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:20.395Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{45,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.395Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:20.396Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:20.867Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:24:20.867Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 5 times in the past 6.934053 secs (last seen 0.477363 secs ago
[cluster:info,2023-08-29T18:24:20.867Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:24:20.868Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:24:20.868Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 5 times in the past 6.933804 secs (last seen 0.476913 secs ago
[ns_server:debug,2023-08-29T18:24:20.870Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{14,63860552660}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:20.870Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{46,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.870Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54904}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:20.870Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:20.871Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{47,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.871Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:20.871Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:20.872Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{48,63860552660}}]}]
[ns_server:debug,2023-08-29T18:24:20.872Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:20.874Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:21.335Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:24:21.336Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 6 times in the past 7.402341 secs (last seen 0.468288 secs ago
[cluster:info,2023-08-29T18:24:21.336Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:24:21.336Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:24:21.336Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 6 times in the past 7.402363 secs (last seen 0.468559 secs ago
[ns_server:debug,2023-08-29T18:24:21.339Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{15,63860552661}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:21.340Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{49,63860552661}}]}]
[ns_server:debug,2023-08-29T18:24:21.340Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{50,63860552661}}]}]
[ns_server:debug,2023-08-29T18:24:21.340Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:21.340Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:21.340Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54910}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:21.340Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:21.342Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{51,63860552661}}]}]
[ns_server:debug,2023-08-29T18:24:21.342Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:21.342Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:21.841Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:24:21.842Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:21.841Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 7 times in the past 7.908129 secs (last seen 0.505788 secs ago
[cluster:debug,2023-08-29T18:24:21.842Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:24:21.842Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 7 times in the past 7.908053 secs (last seen 0.50569 secs ago
[ns_server:debug,2023-08-29T18:24:21.845Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{16,63860552661}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:21.845Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:21.845Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{52,63860552661}}]}]
[ns_server:debug,2023-08-29T18:24:21.846Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54914}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:21.845Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:21.846Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{53,63860552661}}]}]
[ns_server:debug,2023-08-29T18:24:21.846Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:21.847Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{54,63860552661}}]}]
[ns_server:debug,2023-08-29T18:24:21.847Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:21.848Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:24:25.180Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:24:25.181Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 8 times in the past 11.24748 secs (last seen 3.339351 secs ago
[cluster:info,2023-08-29T18:24:25.181Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:25.181Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 8 times in the past 11.24753 secs (last seen 3.339477 secs ago
[cluster:debug,2023-08-29T18:24:25.182Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:24:25.184Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:25.184Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{17,63860552665}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:25.185Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{55,63860552665}}]}]
[ns_server:debug,2023-08-29T18:24:25.185Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{56,63860552665}}]}]
[ns_server:debug,2023-08-29T18:24:25.185Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54930}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:25.185Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:25.185Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:25.186Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{57,63860552665}}]}]
[ns_server:debug,2023-08-29T18:24:25.186Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:24:25.186Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[cluster:info,2023-08-29T18:24:26.327Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:24:26.327Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:26.327Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 9 times in the past 12.394306 secs (last seen 1.146826 secs ago
[cluster:debug,2023-08-29T18:24:26.328Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:24:26.328Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 9 times in the past 12.393887 secs (last seen 1.146357 secs ago
[ns_server:debug,2023-08-29T18:24:26.330Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{18,63860552666}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:26.330Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,54932}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:26.330Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:26.331Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:26.331Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{58,63860552666}}]}]
[ns_server:debug,2023-08-29T18:24:26.331Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{59,63860552666}}]}]
[ns_server:debug,2023-08-29T18:24:26.331Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:26.331Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{60,63860552666}}]}]
[ns_server:debug,2023-08-29T18:24:26.331Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:26.332Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:24:26.776Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:24:26.776Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:24:26.777Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:24:26.777Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:24:47.357Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:24:47.358Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:24:47.358Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 10 times in the past 33.424502 secs (last seen 21.030196 secs ago
[ns_server:info,2023-08-29T18:24:47.358Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 10 times in the past 33.424179 secs (last seen 21.030292 secs ago
[cluster:debug,2023-08-29T18:24:47.358Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:24:47.360Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{19,63860552687}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:24:47.360Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:24:47.361Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{61,63860552687}}]}]
[ns_server:debug,2023-08-29T18:24:47.361Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,48072}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:24:47.361Z">>},
                         {cluster_name,<<"CACHE">>},
                         {quotas,{[{kv,300},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:24:47.361Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{62,63860552687}}]}]
[ns_server:debug,2023-08-29T18:24:47.361Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552614}}]},
 67,65,67,72,69]
[ns_server:debug,2023-08-29T18:24:47.362Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{63,63860552687}}]}]
[ns_server:debug,2023-08-29T18:24:47.362Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:24:47.363Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:24:51.490Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:24:51.523Z,ns_1@127.0.0.1:roles_cache<0.366.0>:active_cache:cleanup:258]Cache roles_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:24:55.920Z,ns_1@127.0.0.1:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:934]Recalculating prometheus scrape intervals for high cardinality metrics
[ns_server:debug,2023-08-29T18:24:55.953Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{64,63860552695}}]}]
[ns_server:debug,2023-08-29T18:24:55.953Z,ns_1@127.0.0.1:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_update_scrape_dynamic_intervals:962]New scrape intervals:
[]
Previous scrape intervals: 
undefined
CBCollect stats dir size estimation: 61084800
Calculated based on scrapes info:
[{kv,low_cardinality,18},
 {kv,high_cardinality,7},
 {ns_server,low_cardinality,112},
 {ns_server,high_cardinality,188},
 {xdcr,low_cardinality,0}]
Raw intervals:
[]
[ns_server:debug,2023-08-29T18:24:55.953Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {node,'ns_1@127.0.0.1',
                                   stats_scrape_dynamic_intervals}]..)
[ns_server:debug,2023-08-29T18:24:55.954Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{node,'ns_1@127.0.0.1',stats_scrape_dynamic_intervals} ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552695}}]}]
[ns_server:debug,2023-08-29T18:24:55.954Z,ns_1@127.0.0.1:prometheus_cfg<0.422.0>:prometheus_cfg:maybe_apply_new_settings:601]Settings didn't change, ignoring update
[ns_server:debug,2023-08-29T18:24:56.777Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:24:56.777Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:24:56.778Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:24:56.778Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:25:25.378Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:25:25.379Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:25:25.379Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:25:26.779Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:25:26.779Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:25:26.780Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:25:26.780Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:25:56.781Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:25:56.781Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:25:56.782Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:25:56.782Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:26:06.492Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:26:26.782Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:26:26.782Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:26:26.783Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:26:26.783Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:26:56.784Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:26:56.784Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:26:56.785Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:26:56.785Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:27:21.482Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:27:26.786Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:27:26.787Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:27:26.787Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:27:26.787Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:27:39.530Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:27:39.531Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:27:39.532Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:27:39.534Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{20,63860552859}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:27:39.534Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
memory_quota ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{2,63860552859}}]}|999]
[ns_server:debug,2023-08-29T18:27:39.535Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{65,63860552859}}]}]
[ns_server:debug,2023-08-29T18:27:39.535Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,memory_quota,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:27:39.535Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,52950}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:27:39.534Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:27:39.535Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{66,63860552859}}]}]
[ns_server:debug,2023-08-29T18:27:39.535Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{3,63860552859}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:27:39.536Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{67,63860552859}}]}]
[ns_server:debug,2023-08-29T18:27:39.536Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:27:39.536Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[cluster:info,2023-08-29T18:27:41.111Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:27:41.112Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:27:41.112Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 1 times in the past 1.581616 secs (last seen 1.581616 secs ago
[cluster:debug,2023-08-29T18:27:41.113Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:27:41.113Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 1 times in the past 1.581602 secs (last seen 1.581602 secs ago
[ns_server:debug,2023-08-29T18:27:41.116Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{21,63860552861}}]}|
 <<"{\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.log_level\":\"info\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:27:41.117Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{68,63860552861}}]}]
[ns_server:debug,2023-08-29T18:27:41.118Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{69,63860552861}}]}]
[ns_server:debug,2023-08-29T18:27:41.118Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,52966}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:27:41.118Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:27:41.118Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{3,63860552859}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:27:41.118Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:27:41.119Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{70,63860552861}}]}]
[ns_server:debug,2023-08-29T18:27:41.120Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:27:41.122Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:27:56.788Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:27:56.789Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:27:56.789Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:27:56.789Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:28:23.655Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:28:23.655Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:28:23.656Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:28:23.658Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{metakv,<<"/indexing/settings/config">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{22,63860552903}}]}|
 <<"{\"indexer.settings.compaction.days_of_week\":\"Sunday,Monday,Tuesday,Wednesday,Thursday,Friday,Saturday\",\"indexer.settings.rebalance.redistribute_indexes\":false,\"indexer.settings.compaction.interval\":\"00:00,00:00\",\"indexer.settings.compaction.compaction_mode\":\"circular\",\"indexer.settings.log_level\":\"info\",\"indexer.settings.persisted_snapshot.interval\":5000,\"indexer.settings.compaction.mi"...>>]
[ns_server:debug,2023-08-29T18:28:23.658Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([{local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>},
                               {metakv,<<"/indexing/settings/config">>}]..)
[ns_server:debug,2023-08-29T18:28:23.659Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{71,63860552903}}]}]
[ns_server:debug,2023-08-29T18:28:23.659Z,ns_1@127.0.0.1:ns_audit<0.603.0>:ns_audit:handle_call:149]Audit cluster_settings: [{local,{[{ip,<<"172.17.0.4">>},{port,8091}]}},
                         {remote,{[{ip,<<"172.17.0.1">>},{port,42912}]}},
                         {real_userid,{[{domain,wrong_token},
                                        {user,<<"<ud></ud>">>}]}},
                         {timestamp,<<"2023-08-29T18:28:23.659Z">>},
                         {cluster_name,<<"cache">>},
                         {quotas,{[{kv,999},
                                   {index,512},
                                   {fts,256},
                                   {cbas,1024},
                                   {eventing,256}]}}]
[ns_server:debug,2023-08-29T18:28:23.660Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{72,63860552903}}]}]
[ns_server:debug,2023-08-29T18:28:23.660Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
cluster_name ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{3,63860552859}}]},
 99,97,99,104,101]
[ns_server:debug,2023-08-29T18:28:23.660Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
{local_changes_count,<<"a2acfe035c01f55c0d78fce935ad3d90">>} ->
[{'_vclock',[{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{73,63860552903}}]}]
[ns_server:debug,2023-08-29T18:28:23.661Z,ns_1@127.0.0.1:ns_config_log<0.264.0>:ns_config_log:log_common:275]config change:
settings ->
[{'_vclock',63860552157,
            [{<<"a2acfe035c01f55c0d78fce935ad3d90">>,{1,63860552480}}]},
 {stats,[{send_stats,true}]}]
[ns_server:debug,2023-08-29T18:28:23.661Z,ns_1@127.0.0.1:ns_config_rep<0.458.0>:ns_config_rep:do_push_keys:383]Replicating some config keys ([cluster_name,settings,
                               {local_changes_count,
                                   <<"a2acfe035c01f55c0d78fce935ad3d90">>}]..)
[ns_server:debug,2023-08-29T18:28:26.790Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:28:26.790Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:28:26.791Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:28:26.791Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:28:36.495Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:28:56.792Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:28:56.792Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:28:56.793Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:28:56.793Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:29:03.676Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:29:03.677Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:29:03.677Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:29:03.703Z,ns_1@127.0.0.1:<0.30371.0>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:18091
[error_logger:warn,2023-08-29T18:29:03.713Z,ns_1@127.0.0.1:<0.30420.0>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:29:03.783Z,ns_1@127.0.0.1:<0.30422.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.30422.0>,tls_dyn_connection_sup}
    started: [{pid,<0.30425.0>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:29:03.784Z,ns_1@127.0.0.1:<0.30421.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.30421.0>,tls_dyn_connection_sup}
    started: [{pid,<0.30426.0>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:29:03.785Z,ns_1@127.0.0.1:<0.30422.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.30422.0>,tls_dyn_connection_sup}
    started: [{pid,<0.30427.0>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.30425.0>,"localhost",18091,#Port<0.771>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.30334.0>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:29:03.786Z,ns_1@127.0.0.1:<0.30421.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.30421.0>,tls_dyn_connection_sup}
    started: [{pid,<0.30428.0>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.30426.0>,"localhost",18091,#Port<0.769>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.30332.0>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:29:03.805Z,ns_1@127.0.0.1:<0.30433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.30433.0>,tls_dyn_connection_sup}
    started: [{pid,<0.30434.0>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:29:03.806Z,ns_1@127.0.0.1:<0.30433.0>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.30433.0>,tls_dyn_connection_sup}
    started: [{pid,<0.30435.0>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.30434.0>,
                       {127,0,0,1},
                       18091,#Port<0.770>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.30420.0>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[ns_server:debug,2023-08-29T18:29:04.105Z,ns_1@127.0.0.1:compiled_roles_cache<0.363.0>:menelaus_roles:build_compiled_roles:1062]Compile roles for user {"<ud>Administrator</ud>",admin}
[cluster:debug,2023-08-29T18:29:04.107Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:413]handling add_node(https, "127.0.0.1", 18091, undefined, ..)
[cluster:debug,2023-08-29T18:29:04.107Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:416]add_node(https, "127.0.0.1", 18091, undefined, ..) -> {error,
                                                       system_not_provisioned,
                                                       <<"Adding nodes to not provisioned nodes is not allowed.">>}
[user:info,2023-08-29T18:29:04.108Z,ns_1@127.0.0.1:<0.30334.0>:ns_cluster:add_node_to_group:89]Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.
[ns_server:debug,2023-08-29T18:29:26.793Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:29:26.794Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:29:26.794Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:29:26.795Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:29:51.495Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
[ns_server:debug,2023-08-29T18:29:51.594Z,ns_1@127.0.0.1:roles_cache<0.366.0>:active_cache:renew:238]Starting roles_cache cache renewal
[ns_server:debug,2023-08-29T18:29:51.594Z,ns_1@127.0.0.1:roles_cache<0.366.0>:active_cache:renew:244]roles_cache cache renewal process originated 0 queries
[ns_server:debug,2023-08-29T18:29:56.795Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:29:56.796Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:29:56.796Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:29:56.796Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:30:21.724Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:30:21.725Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:30:21.725Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:30:21.750Z,ns_1@127.0.0.1:<0.275.1>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:18091
[error_logger:info,2023-08-29T18:30:21.751Z,ns_1@127.0.0.1:<0.356.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.356.1>,tls_dyn_connection_sup}
    started: [{pid,<0.357.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:warn,2023-08-29T18:30:21.753Z,ns_1@127.0.0.1:<0.359.1>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:30:21.752Z,ns_1@127.0.0.1:<0.356.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.356.1>,tls_dyn_connection_sup}
    started: [{pid,<0.358.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.357.1>,"localhost",18091,#Port<0.827>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.69.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:21.754Z,ns_1@127.0.0.1:<0.360.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.360.1>,tls_dyn_connection_sup}
    started: [{pid,<0.362.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:21.755Z,ns_1@127.0.0.1:<0.363.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.363.1>,tls_dyn_connection_sup}
    started: [{pid,<0.364.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:21.756Z,ns_1@127.0.0.1:<0.360.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.360.1>,tls_dyn_connection_sup}
    started: [{pid,<0.365.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.362.1>,
                       {127,0,0,1},
                       18091,#Port<0.828>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.359.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:21.756Z,ns_1@127.0.0.1:<0.363.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.363.1>,tls_dyn_connection_sup}
    started: [{pid,<0.366.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.364.1>,"localhost",18091,#Port<0.829>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.70.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[cluster:debug,2023-08-29T18:30:21.819Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:413]handling add_node(https, "127.0.0.1", 18091, undefined, ..)
[cluster:debug,2023-08-29T18:30:21.819Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:416]add_node(https, "127.0.0.1", 18091, undefined, ..) -> {error,
                                                       system_not_provisioned,
                                                       <<"Adding nodes to not provisioned nodes is not allowed.">>}
[user:info,2023-08-29T18:30:21.820Z,ns_1@127.0.0.1:<0.70.1>:ns_cluster:add_node_to_group:89]Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.
[cluster:info,2023-08-29T18:30:22.740Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:30:22.740Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:30:22.740Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 1 times in the past 1.015594 secs (last seen 1.015594 secs ago
[cluster:debug,2023-08-29T18:30:22.741Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:30:22.741Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 1 times in the past 1.015558 secs (last seen 1.015558 secs ago
[ns_server:debug,2023-08-29T18:30:22.768Z,ns_1@127.0.0.1:<0.297.1>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:18091
[error_logger:info,2023-08-29T18:30:22.768Z,ns_1@127.0.0.1:<0.384.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.384.1>,tls_dyn_connection_sup}
    started: [{pid,<0.385.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:22.770Z,ns_1@127.0.0.1:<0.384.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.384.1>,tls_dyn_connection_sup}
    started: [{pid,<0.386.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.385.1>,"localhost",18091,#Port<0.835>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.326.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:warn,2023-08-29T18:30:22.772Z,ns_1@127.0.0.1:<0.387.1>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:30:22.774Z,ns_1@127.0.0.1:<0.391.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.391.1>,tls_dyn_connection_sup}
    started: [{pid,<0.392.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:22.774Z,ns_1@127.0.0.1:<0.389.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.389.1>,tls_dyn_connection_sup}
    started: [{pid,<0.390.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:22.775Z,ns_1@127.0.0.1:<0.389.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.389.1>,tls_dyn_connection_sup}
    started: [{pid,<0.394.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.390.1>,
                       {127,0,0,1},
                       18091,#Port<0.836>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.387.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:22.775Z,ns_1@127.0.0.1:<0.391.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.391.1>,tls_dyn_connection_sup}
    started: [{pid,<0.393.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.392.1>,"localhost",18091,#Port<0.837>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.325.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[cluster:debug,2023-08-29T18:30:22.840Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:413]handling add_node(https, "127.0.0.1", 18091, undefined, ..)
[cluster:debug,2023-08-29T18:30:22.841Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:416]add_node(https, "127.0.0.1", 18091, undefined, ..) -> {error,
                                                       system_not_provisioned,
                                                       <<"Adding nodes to not provisioned nodes is not allowed.">>}
[user:info,2023-08-29T18:30:22.841Z,ns_1@127.0.0.1:<0.325.1>:ns_cluster:add_node_to_group:89]Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.
[ns_server:info,2023-08-29T18:30:22.841Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:5([<<"Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.">>]) because it's been seen 1 times in the past 1.021104 secs (last seen 1.021104 secs ago
[cluster:info,2023-08-29T18:30:23.522Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:30:23.523Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:30:23.523Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 2 times in the past 1.798035 secs (last seen 0.782441 secs ago
[cluster:debug,2023-08-29T18:30:23.523Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:info,2023-08-29T18:30:23.523Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 2 times in the past 1.797846 secs (last seen 0.782288 secs ago
[ns_server:debug,2023-08-29T18:30:23.545Z,ns_1@127.0.0.1:<0.321.1>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:18091
[error_logger:info,2023-08-29T18:30:23.546Z,ns_1@127.0.0.1:<0.430.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.430.1>,tls_dyn_connection_sup}
    started: [{pid,<0.431.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:23.547Z,ns_1@127.0.0.1:<0.430.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.430.1>,tls_dyn_connection_sup}
    started: [{pid,<0.432.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.431.1>,"localhost",18091,#Port<0.843>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.327.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:warn,2023-08-29T18:30:23.553Z,ns_1@127.0.0.1:<0.433.1>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:30:23.554Z,ns_1@127.0.0.1:<0.437.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.437.1>,tls_dyn_connection_sup}
    started: [{pid,<0.438.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:23.554Z,ns_1@127.0.0.1:<0.435.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.435.1>,tls_dyn_connection_sup}
    started: [{pid,<0.436.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:23.555Z,ns_1@127.0.0.1:<0.435.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.435.1>,tls_dyn_connection_sup}
    started: [{pid,<0.440.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.436.1>,
                       {127,0,0,1},
                       18091,#Port<0.844>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.433.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:23.555Z,ns_1@127.0.0.1:<0.437.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.437.1>,tls_dyn_connection_sup}
    started: [{pid,<0.439.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.438.1>,"localhost",18091,#Port<0.845>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.328.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[cluster:debug,2023-08-29T18:30:23.610Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:413]handling add_node(https, "127.0.0.1", 18091, undefined, ..)
[cluster:debug,2023-08-29T18:30:23.610Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:416]add_node(https, "127.0.0.1", 18091, undefined, ..) -> {error,
                                                       system_not_provisioned,
                                                       <<"Adding nodes to not provisioned nodes is not allowed.">>}
[user:info,2023-08-29T18:30:23.611Z,ns_1@127.0.0.1:<0.328.1>:ns_cluster:add_node_to_group:89]Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.
[ns_server:info,2023-08-29T18:30:23.611Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:5([<<"Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.">>]) because it's been seen 2 times in the past 1.790828 secs (last seen 0.769724 secs ago
[cluster:info,2023-08-29T18:30:23.971Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:30:23.971Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 3 times in the past 2.246461 secs (last seen 0.448426 secs ago
[cluster:info,2023-08-29T18:30:23.971Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:30:23.972Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 3 times in the past 2.246464 secs (last seen 0.448618 secs ago
[cluster:debug,2023-08-29T18:30:23.972Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:30:23.993Z,ns_1@127.0.0.1:<0.383.1>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:18091
[error_logger:info,2023-08-29T18:30:23.994Z,ns_1@127.0.0.1:<0.454.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.454.1>,tls_dyn_connection_sup}
    started: [{pid,<0.455.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:23.995Z,ns_1@127.0.0.1:<0.454.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.454.1>,tls_dyn_connection_sup}
    started: [{pid,<0.456.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.455.1>,"localhost",18091,#Port<0.851>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.329.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:warn,2023-08-29T18:30:23.997Z,ns_1@127.0.0.1:<0.457.1>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:30:23.999Z,ns_1@127.0.0.1:<0.459.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.459.1>,tls_dyn_connection_sup}
    started: [{pid,<0.460.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:23.999Z,ns_1@127.0.0.1:<0.461.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.461.1>,tls_dyn_connection_sup}
    started: [{pid,<0.462.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:24.000Z,ns_1@127.0.0.1:<0.459.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.459.1>,tls_dyn_connection_sup}
    started: [{pid,<0.463.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.460.1>,
                       {127,0,0,1},
                       18091,#Port<0.852>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.457.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:24.001Z,ns_1@127.0.0.1:<0.461.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.461.1>,tls_dyn_connection_sup}
    started: [{pid,<0.464.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.462.1>,"localhost",18091,#Port<0.853>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.330.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[cluster:debug,2023-08-29T18:30:24.059Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:413]handling add_node(https, "127.0.0.1", 18091, undefined, ..)
[cluster:debug,2023-08-29T18:30:24.060Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:416]add_node(https, "127.0.0.1", 18091, undefined, ..) -> {error,
                                                       system_not_provisioned,
                                                       <<"Adding nodes to not provisioned nodes is not allowed.">>}
[user:info,2023-08-29T18:30:24.060Z,ns_1@127.0.0.1:<0.330.1>:ns_cluster:add_node_to_group:89]Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.
[ns_server:info,2023-08-29T18:30:24.060Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:5([<<"Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.">>]) because it's been seen 3 times in the past 2.240084 secs (last seen 0.449256 secs ago
[cluster:info,2023-08-29T18:30:24.391Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:30:24.391Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 4 times in the past 2.66671 secs (last seen 0.420249 secs ago
[cluster:info,2023-08-29T18:30:24.392Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:30:24.392Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 4 times in the past 2.666723 secs (last seen 0.420259 secs ago
[cluster:debug,2023-08-29T18:30:24.392Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:30:24.415Z,ns_1@127.0.0.1:<0.417.1>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:18091
[error_logger:info,2023-08-29T18:30:24.416Z,ns_1@127.0.0.1:<0.468.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.468.1>,tls_dyn_connection_sup}
    started: [{pid,<0.469.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:24.417Z,ns_1@127.0.0.1:<0.468.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.468.1>,tls_dyn_connection_sup}
    started: [{pid,<0.470.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.469.1>,"localhost",18091,#Port<0.859>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.331.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:warn,2023-08-29T18:30:24.419Z,ns_1@127.0.0.1:<0.471.1>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:30:24.420Z,ns_1@127.0.0.1:<0.473.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.473.1>,tls_dyn_connection_sup}
    started: [{pid,<0.474.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:24.421Z,ns_1@127.0.0.1:<0.475.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.475.1>,tls_dyn_connection_sup}
    started: [{pid,<0.476.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:24.421Z,ns_1@127.0.0.1:<0.473.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.473.1>,tls_dyn_connection_sup}
    started: [{pid,<0.477.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.474.1>,
                       {127,0,0,1},
                       18091,#Port<0.860>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.471.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:24.422Z,ns_1@127.0.0.1:<0.475.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.475.1>,tls_dyn_connection_sup}
    started: [{pid,<0.478.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.476.1>,"localhost",18091,#Port<0.861>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.332.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[cluster:debug,2023-08-29T18:30:24.480Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:413]handling add_node(https, "127.0.0.1", 18091, undefined, ..)
[cluster:debug,2023-08-29T18:30:24.480Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:416]add_node(https, "127.0.0.1", 18091, undefined, ..) -> {error,
                                                       system_not_provisioned,
                                                       <<"Adding nodes to not provisioned nodes is not allowed.">>}
[user:info,2023-08-29T18:30:24.481Z,ns_1@127.0.0.1:<0.332.1>:ns_cluster:add_node_to_group:89]Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.
[ns_server:info,2023-08-29T18:30:24.481Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:5([<<"Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.">>]) because it's been seen 4 times in the past 2.660739 secs (last seen 0.420655 secs ago
[ns_server:debug,2023-08-29T18:30:26.797Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:30:26.798Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:30:26.798Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:30:26.799Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[cluster:info,2023-08-29T18:30:48.978Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[ns_server:info,2023-08-29T18:30:48.978Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Changing address to \"127.0.0.1\" due to client request">>]) because it's been seen 5 times in the past 27.253522 secs (last seen 24.586812 secs ago
[cluster:info,2023-08-29T18:30:48.978Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[ns_server:info,2023-08-29T18:30:48.979Z,ns_1@127.0.0.1:ns_log<0.414.0>:ns_log:is_duplicate_log:156]suppressing duplicate log ns_cluster:0([<<"Change of address to \"127.0.0.1\" is requested.">>]) because it's been seen 5 times in the past 27.25358 secs (last seen 24.586857 secs ago
[cluster:debug,2023-08-29T18:30:48.979Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:30:49.003Z,ns_1@127.0.0.1:<0.1266.1>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:8091
[error_logger:warn,2023-08-29T18:30:49.007Z,ns_1@127.0.0.1:<0.1322.1>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:30:49.009Z,ns_1@127.0.0.1:<0.1323.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1323.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1325.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:49.010Z,ns_1@127.0.0.1:<0.1323.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1323.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1326.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.1325.1>,
                       {127,0,0,1},
                       8091,#Port<0.883>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.1322.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:49.013Z,ns_1@127.0.0.1:<0.1326.1>:ale_error_logger_handler:do_log:101]
=========================NOTICE REPORT=========================
TLS client: In state hello at tls_record.erl:564 generated CLIENT ALERT: Fatal - Unexpected Message
 - {unsupported_record_type,72}
[cluster:info,2023-08-29T18:30:55.926Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:446]Changing address to "127.0.0.1" due to client request
[cluster:info,2023-08-29T18:30:55.927Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:do_change_address:729]Change of address to "127.0.0.1" is requested.
[cluster:debug,2023-08-29T18:30:55.928Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:maybe_rename:756]Not renaming node.
[ns_server:debug,2023-08-29T18:30:55.954Z,ns_1@127.0.0.1:<0.1505.1>:ns_cluster:check_host_port_connectivity:670]Successfully checked TCP connectivity to {127,0,0,1}:18091
[error_logger:info,2023-08-29T18:30:55.955Z,ns_1@127.0.0.1:<0.1562.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1562.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1563.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:55.956Z,ns_1@127.0.0.1:<0.1562.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1562.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1564.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.1563.1>,"localhost",18091,#Port<0.893>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.1414.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:warn,2023-08-29T18:30:55.959Z,ns_1@127.0.0.1:<0.1565.1>:ale_error_logger_handler:do_log:101]
=========================WARNING REPORT=========================
Description: "Authenticity is not established by certificate path validation"
     Reason: "Option {verify, verify_peer} and cacertfile/cacerts is missing"

[error_logger:info,2023-08-29T18:30:55.961Z,ns_1@127.0.0.1:<0.1569.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1569.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1571.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:55.961Z,ns_1@127.0.0.1:<0.1568.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1568.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1570.1>},
              {id,sender},
              {mfargs,{tls_sender,start_link,[]}},
              {restart_type,temporary},
              {significant,false},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:55.964Z,ns_1@127.0.0.1:<0.1568.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1568.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1573.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [client,<0.1570.1>,
                       {127,0,0,1},
                       18091,#Port<0.894>,
                       {#{renegotiate_at => 268435456,fallback => false,
                          crl_check => false,log_level => notice,
                          password => [],erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => undefined,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => undefined,
                          certificate_authorities => false,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => undefined,reuse_sessions => true,
                          depth => 10,sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => undefined,...},
                        {socket_options,binary,http,0,0,false},
                        undefined},
                       <0.1565.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[error_logger:info,2023-08-29T18:30:55.963Z,ns_1@127.0.0.1:<0.1569.1>:ale_error_logger_handler:do_log:101]
=========================PROGRESS REPORT=========================
    supervisor: {<0.1569.1>,tls_dyn_connection_sup}
    started: [{pid,<0.1572.1>},
              {id,receiver},
              {mfargs,
                  {ssl_gen_statem,start_link,
                      [server,<0.1571.1>,"localhost",18091,#Port<0.895>,
                       {#{renegotiate_at => 268435456,fallback => undefined,
                          crl_check => false,log_level => notice,
                          password => undefined,erl_dist => false,
                          crl_cache => {ssl_crl_cache,{internal,[]}},
                          ocsp_nonce => true,honor_ecc_order => false,
                          handshake => full,
                          supported_groups =>
                              {supported_groups,
                                  [x25519,x448,secp256r1,secp384r1]},
                          signature_algs_cert => undefined,
                          partial_chain => #Fun<ssl.11.54588823>,
                          user_lookup_fun => undefined,
                          alpn_advertised_protocols => undefined,
                          alpn_preferred_protocols => undefined,
                          session_tickets => disabled,
                          eccs =>
                              {elliptic_curves,
                                  [{1,3,132,0,39},
                                   {1,3,132,0,38},
                                   {1,3,132,0,35},
                                   {1,3,36,3,3,2,8,1,1,13},
                                   {1,3,132,0,36},
                                   {1,3,132,0,37},
                                   {1,3,36,3,3,2,8,1,1,11},
                                   {1,3,132,0,34},
                                   {1,3,132,0,16},
                                   {1,3,132,0,17},
                                   {1,3,36,3,3,2,8,1,1,7},
                                   {1,3,132,0,10},
                                   {1,2,840,10045,3,1,7},
                                   {1,3,132,0,3},
                                   {1,3,132,0,26},
                                   {1,3,132,0,27},
                                   {1,3,132,0,32},
                                   {1,3,132,0,33},
                                   {1,3,132,0,24},
                                   {1,3,132,0,25},
                                   {1,3,132,0,31},
                                   {1,2,840,10045,3,1,1},
                                   {1,3,132,0,1},
                                   {1,3,132,0,2},
                                   {1,3,132,0,15},
                                   {1,3,132,0,9},
                                   {1,3,132,0,...},
                                   {1,3,132,...}]},
                          reuse_session => #Fun<ssl.13.54588823>,
                          certificate_authorities => undefined,
                          ocsp_responder_certs => [],key => undefined,
                          versions => [{3,4},{3,3}],
                          customize_hostname_check => [],
                          ocsp_stapling => false,
                          signature_algs =>
                              [ecdsa_secp521r1_sha512,ecdsa_secp384r1_sha384,
                               ecdsa_secp256r1_sha256,rsa_pss_pss_sha512,
                               rsa_pss_pss_sha384,rsa_pss_pss_sha256,
                               rsa_pss_rsae_sha512,rsa_pss_rsae_sha384,
                               rsa_pss_rsae_sha256,eddsa_ed25519,eddsa_ed448,
                               {sha512,ecdsa},
                               {sha512,rsa},
                               {sha384,ecdsa},
                               {sha384,rsa},
                               {sha256,ecdsa},
                               {sha256,rsa},
                               {sha224,ecdsa},
                               {sha224,rsa},
                               {sha,ecdsa},
                               {sha,rsa},
                               {sha,dsa}],
                          cookie => true,reuse_sessions => true,depth => 10,
                          sni_hosts => [],padding_check => true,
                          keep_secrets => false,
                          verify_fun => {#Fun<ssl.12.54588823>,[]},
                          honor_cipher_order => true,...},
                        {socket_options,binary,0,0,0,false},
                        [{option_tracker,<0.318.0>},
                         {session_tickets_tracker,disabled},
                         {session_id_tracker,<0.319.0>}]},
                       <0.1415.1>,
                       {gen_tcp,tcp,tcp_closed,tcp_error,tcp_passive}]}},
              {restart_type,temporary},
              {significant,true},
              {shutdown,5000},
              {child_type,worker}]

[cluster:debug,2023-08-29T18:30:56.029Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:413]handling add_node(https, "127.0.0.1", 18091, undefined, ..)
[cluster:debug,2023-08-29T18:30:56.029Z,ns_1@127.0.0.1:ns_cluster<0.253.0>:ns_cluster:handle_call:416]add_node(https, "127.0.0.1", 18091, undefined, ..) -> {error,
                                                       system_not_provisioned,
                                                       <<"Adding nodes to not provisioned nodes is not allowed.">>}
[user:info,2023-08-29T18:30:56.029Z,ns_1@127.0.0.1:<0.1415.1>:ns_cluster:add_node_to_group:89]Failed to add node 127.0.0.1:18091 to cluster. Adding nodes to not provisioned nodes is not allowed.
[ns_server:debug,2023-08-29T18:30:56.799Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_views. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:30:56.800Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_views too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:30:56.800Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_daemon:process_scheduler_message:1359]No buckets to compact for compact_kv. Rescheduling compaction.
[ns_server:debug,2023-08-29T18:30:56.801Z,ns_1@127.0.0.1:compaction_daemon<0.666.0>:compaction_scheduler:schedule_next:51]Finished compaction for compact_kv too soon. Next run will be in 30s
[ns_server:debug,2023-08-29T18:31:06.497Z,ns_1@127.0.0.1:ldap_auth_cache<0.355.0>:active_cache:cleanup:258]Cache ldap_auth_cache cleanup: 0/0 records deleted
